{"searchDocs":[{"title":"intro","type":0,"sectionRef":"#","url":"/personal-blog/docs/intro","content":"intro","keywords":"","version":"Next"},{"title":"Welcome","type":0,"sectionRef":"#","url":"/personal-blog/blog/welcome","content":"","keywords":"","version":null},{"title":"🌟 Welcome!​","type":1,"pageTitle":"Welcome","url":"/personal-blog/blog/welcome#-welcome","content":" Hello and welcome to my blog! 🚀  Here, you will discover everything you need to know about open source tools, DevOps, and GitOps practices.  Dive in and let's explore together! 💡🔧 ","version":null,"tagName":"h2"},{"title":"Cilium on EKS with Sveltos","type":0,"sectionRef":"#","url":"/personal-blog/blog/cilium-eks-sveltos","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#introduction","content":" In today's blog post, we will demonstrate an easy way of deploying and controlling Cilium on an EKS cluster with Sveltos.  As the majority of the documentation out there provides a step-by-step installation directly with the Helm chart commands, we decided to demonstrate a different approach, the GitOps approach, with the use of Sveltos ClusterProfile CRD (Custom Resource Definition).  We will utilise the Terraform AWS EKS module to create an EKS cluster. Once the cluster is up and running, we will register it with Sveltos. Then, we will update the aws-core daemonset to support ENI mode and remove the kube-proxy Kubernetes resources as Cilium will take over.  ","version":null,"tagName":"h2"},{"title":"Diagram​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#diagram","content":"   ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#lab-setup","content":" +-----------------+-------------------+--------------------------+ | Cluster Name | Type | Version | +-----------------+-------------------+--------------------------+ | mgmt | Management Cluster| RKE2 v1.28.9+rke2r1 | | eks-test01 | Managed Cluster | EKS v1.28.10-eks-49c6de4 | +-----------------+-------------------+--------------------------+ +-------------+----------+ | Deployment | Version | +-------------+----------+ | Cilium | v1.14.8 | | sveltosctl | v0.27.0 | +-------------+----------+   ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#prerequisites","content":" To follow along with the blog post, ensure the below are satisfied.  AWS Service AccountAWS CLI installedTerraform installedkubectl installedsveltosctl installed  ","version":null,"tagName":"h2"},{"title":"Step 1: Create EKS Cluster with Terraform​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-1-create-eks-cluster-with-terraform","content":" The easiest way to spin up an EKS cluster is by following the recommended training and resources from the Hashicorp website. Find the training material and the Git repository further below.  Training: https://developer.hashicorp.com/terraform/tutorials/kubernetes/eks GitHub Repository: https://github.com/hashicorp/learn-terraform-provision-eks-cluster  To execute the Terraform plan, a valid AWS Service Account should be available with the right permissions to create the required resources. For more information about the AWS Service Accounts, have a look here.  To get the cluster kubeconfig and start interacting with the cluster, the AWS CLI is used. Modify and execute the command below.  $ aws eks update-kubeconfig --region &lt;the region the cluster created&gt; --name &lt;the name of the cluster&gt;   tip The command will save the kubeconfig in the default directory ~/.kube/config. If the file should be stored elsewhere, pass the argument --kubeconfig and specify the output directory. For more details, check out the link.  ","version":null,"tagName":"h2"},{"title":"Step 2: Register Cluster with Sveltos​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-2-register-cluster-withsveltos","content":" Once we have access to the cluster, it is time to proceed with the Sveltos cluster registration. As this is a cloud Kubernetes cluster, we need to ensure Sveltos has the right set of permissions to perform the Kubernetes deployments and add-ons. To do that, we will utilise sveltosctl and generate a new kubeconfig file.  ","version":null,"tagName":"h2"},{"title":"Generate Sveltos kubeconfig​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#generate-sveltos-kubeconfig","content":" $ export KUBECONFIG=&lt;directory of the EKS kubeconfig file&gt; $ sveltosctl generate kubeconfig --create --expirationSeconds=86400   The sveltosctl command will create a kubeconfig file. The file will be used for the Sveltos cluster registration.  ","version":null,"tagName":"h3"},{"title":"Register EKS Cluster​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#register-ekscluster","content":" $ sveltosctl register cluster --namespace=&lt;namespace&gt; --cluster=&lt;cluster name&gt; \\ --kubeconfig=&lt;path to Sveltos file with Kubeconfig&gt; \\ --labels=env=test   The command above will register the EKS cluster with Sveltos on the mentioned namespace, and name and will attach the cluster label env=test defined.  note If the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.  $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl get sveltosclusters -A --show-labels NAMESPACE NAME READY VERSION LABELS mgmt mgmt true v1.28.9+rke2r1 sveltos-agent=present test eks-test01 true v1.28.10-eks-49c6de4 env=test,sveltos-agent=present   ","version":null,"tagName":"h3"},{"title":"Step 3: Update the EKS cluster​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-3-update-the-ekscluster","content":" As we would like to use Cilium with the Kube Proxy replacement and the ENI mode enabled, we need to perform additional actions. As the kube-proxy daemonset is already installed, we have to remove all related resources and update the aws-node daemonset to support the ENI mode.  ","version":null,"tagName":"h2"},{"title":"Validation​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#validation","content":" $ kubectl get pods,ds -n kube-system NAME READY STATUS RESTARTS AGE pod/aws-node-4x8sq 2/2 Running 0 16m pod/aws-node-vjtlx 2/2 Running 0 16m pod/aws-node-xp7vl 2/2 Running 0 16m pod/coredns-648485486-t5sxm 1/1 Running 0 20m pod/coredns-648485486-tv4h5 1/1 Running 0 20m pod/ebs-csi-controller-5df9db689f-8hmdm 6/6 Running 0 15m pod/ebs-csi-controller-5df9db689f-qmxhs 6/6 Running 0 15m pod/ebs-csi-node-2rspx 3/3 Running 0 15m pod/ebs-csi-node-gvtfj 3/3 Running 0 15m pod/ebs-csi-node-t96ch 3/3 Running 0 15m pod/kube-proxy-4jxlt 1/1 Running 0 16m pod/kube-proxy-hgx9h 1/1 Running 0 16m pod/kube-proxy-l877x 1/1 Running 0 16m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/aws-node 3 3 3 3 3 &lt;none&gt; 20m daemonset.apps/ebs-csi-node 3 3 3 3 3 kubernetes.io/os=linux 16m daemonset.apps/ebs-csi-node-windows 0 0 0 0 0 kubernetes.io/os=windows 16m daemonset.apps/kube-proxy 3 3 3 3 3 &lt;none&gt; 20m   ","version":null,"tagName":"h3"},{"title":"Delete kube-proxy Resources​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#delete-kube-proxy-resources","content":" $ export KUBECONFIG=&lt;directory of the EKS kubeconfig file&gt; $ kubectl delete ds kube-proxy -n kube-system # Remove the kube-proxy daemonset $ kubectl delete cm kube-proxy -n kube-system # Remove the kube-proxy ConfigMap   ","version":null,"tagName":"h3"},{"title":"Update aws-node Resources​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#update-aws-node-resources","content":" $ kubectl patch daemonset aws-node --type='strategic' -p='{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;nodeSelector&quot;:{&quot;io.cilium/aws-node-enabled&quot;:&quot;true&quot;}}}}}' -n kube-system # This is required based on the Cilium documentation to enable the ENI mode   $ kubectl get pods,ds -n kube-system NAME READY STATUS RESTARTS AGE pod/coredns-648485486-t5sxm 1/1 Running 0 22m pod/coredns-648485486-tv4h5 1/1 Running 0 22m pod/ebs-csi-controller-5df9db689f-8hmdm 6/6 Running 0 17m pod/ebs-csi-controller-5df9db689f-qmxhs 6/6 Running 0 17m pod/ebs-csi-node-2rspx 3/3 Running 0 17m pod/ebs-csi-node-gvtfj 3/3 Running 0 17m pod/ebs-csi-node-t96ch 3/3 Running 0 17m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/aws-node 0 0 0 0 0 io.cilium/aws-node-enabled=true 22m daemonset.apps/ebs-csi-node 3 3 3 3 3 kubernetes.io/os=linux 17m daemonset.apps/ebs-csi-node-windows 0 0 0 0 0 kubernetes.io/os=windows 17m   tip The aws-node daemonset scaled down to 0 replicas.  ","version":null,"tagName":"h3"},{"title":"Step 4: Create Sveltos ClusterProfile​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-4-create-sveltos-clusterprofile","content":" It is time to create a Sveltos ClusterProfile and deploy Cilium to the EKS cluster with the label set to env=test. Following the Cilium documentation, we will enable the required Helm values for the kube-proxy replacement and the ENI mode.  --- apiVersion: config.projectsveltos.io/v1alpha1 kind: ClusterProfile metadata: name: cilium-1148 spec: clusterSelector: env=test # Deploy Cilium v1.14.8 to any cluster with the cluster label set to env=test helmCharts: - chartName: cilium/cilium chartVersion: 1.14.8 helmChartAction: Install releaseName: cilium releaseNamespace: kube-system repositoryName: cilium repositoryURL: https://helm.cilium.io/ values: | eni: enabled: true ipam: mode: eni egressMasqueradeInterfaces: eth0 routingMode: native kubeProxyReplacement: true k8sServiceHost: &lt;The Server API FQDN or IP Address&gt; # The information can be exctracted from the kubeconfig file or the AWS UI k8sServicePort: &lt;The Server API listening port&gt; # The information can be extracted from the kubeconfig file or the AWS UI nodePort: enabled: true debug: enabled: true   The ClusterProfile will deploy Cilium CNI to any cluster with the cluster label set to env=test. It will then deploy the Cilium Helm chart in the kube-system namespace alongside the kube-proxy replacement and the ENI mode. Hubble is also enabled.  ","version":null,"tagName":"h3"},{"title":"Step 5: Deploy Cilium and Validate​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-5-deploy-cilium-andvalidate","content":" To see and evaluate the results, the Sveltos ClusterProfile will be deployed to the management cluster.  $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl apply -f &quot;clusterprofile_cilium1148.yaml&quot;   ","version":null,"tagName":"h2"},{"title":"Validation​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#validation-1","content":" $ ./sveltosctl show addons +-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+ | CLUSTER | RESOURCE TYPE | NAMESPACE | NAME | VERSION | TIME | PROFILES | +-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+ | test/eks-test01 | helm chart | kube-system | cilium | 1.14.8 | 2024-06-18 14:39:26 +0000 UTC | ClusterProfile/cilium-1148 | +-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+   $ export KUBECONFIG=&lt;directory of the EKS kubeconfig file&gt; $ kubectl get pods -n kube-system | grep -i cilium cilium-2vg4c 1/1 Running 0 54s cilium-operator-594f4858f6-km2wh 1/1 Running 0 54s cilium-operator-594f4858f6-xx2q6 1/1 Running 0 55s cilium-qrwwf 1/1 Running 0 55s cilium-s55v5 1/1 Running 0 54s   $ kubectl exec -it cilium-2vg4c -n kube-system -- cilium status Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) KVStore: Ok Disabled Kubernetes: Ok 1.28+ (v1.28.10-eks-49c6de4) [linux/amd64] Kubernetes APIs: [&quot;EndpointSliceOrEndpoint&quot;, &quot;cilium/v2::CiliumClusterwideNetworkPolicy&quot;, &quot;cilium/v2::CiliumEndpoint&quot;, &quot;cilium/v2::CiliumNetworkPolicy&quot;, &quot;cilium/v2::CiliumNode&quot;, &quot;cilium/v2alpha1::CiliumCIDRGroup&quot;, &quot;core/v1::Namespace&quot;, &quot;core/v1::Pods&quot;, &quot;core/v1::Service&quot;, &quot;networking.k8s.io/v1::NetworkPolicy&quot;] KubeProxyReplacement: True [eth0 10.0.1.150 (Direct Routing), eth1 10.0.1.37]   ","version":null,"tagName":"h3"},{"title":"Deploy Nginx Application​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#deploy-nginx-application","content":" $ kubectl apply -f &quot;nginx.yaml&quot; $ kubectl get pods,svc NAME READY STATUS RESTARTS AGE pod/my-nginx-684dd4dcd4-gl9rm 1/1 Running 0 18s pod/my-nginx-684dd4dcd4-nk9mm 1/1 Running 0 18s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 172.20.0.1 &lt;none&gt; 443/TCP 33m service/my-nginx NodePort 172.20.80.220 &lt;none&gt; 80:32449/TCP 3s   ","version":null,"tagName":"h3"},{"title":"Cilium Validation​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#cilium-validation","content":" $ kubectl -n kube-system exec ds/cilium -- cilium service list Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) ID Frontend Service Type Backend 1 172.20.0.1:443 ClusterIP 1 =&gt; 10.0.1.15:443 (active) 2 =&gt; 10.0.2.226:443 (active) 2 172.20.208.197:443 ClusterIP 1 =&gt; 10.0.1.150:4244 (active) 3 172.20.22.66:80 ClusterIP 1 =&gt; 10.0.3.36:4245 (active) 4 172.20.141.67:80 ClusterIP 1 =&gt; 10.0.2.229:8081 (active) 5 172.20.0.10:53 ClusterIP 1 =&gt; 10.0.1.144:53 (active) 2 =&gt; 10.0.3.123:53 (active) 6 172.20.80.220:80 ClusterIP 1 =&gt; 10.0.1.216:80 (active) 2 =&gt; 10.0.3.39:80 (active) 7 10.0.1.150:32449 NodePort 1 =&gt; 10.0.1.216:80 (active) 2 =&gt; 10.0.3.39:80 (active) 8 10.0.1.37:32449 NodePort 1 =&gt; 10.0.1.216:80 (active) 2 =&gt; 10.0.3.39:80 (active) 9 0.0.0.0:32449 NodePort 1 =&gt; 10.0.1.216:80 (active) 2 =&gt; 10.0.3.39:80 (active)   From the output above, we can observe that Cilium eBPF kube-proxy replacement created the NodePort service for Nginx.  As the blog post is not intended to outline in depth how the kube-proxy replacement works, check out the link for further tests.  ","version":null,"tagName":"h3"},{"title":"Conclusions​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#conclusions","content":" We demonstrated an easy way of deploying Cilium CNI to an EKS cluster with the Sveltos ClusterProfile. The complete lifecycle of the CNI is now controlled by Sveltos and without external dependencies.  Take advantage of the Sveltos Templating and the Sveltos Event Framework capabilities to make every Kubernetes deployment and add-on easier!  ","version":null,"tagName":"h2"},{"title":"Contact​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#contact","content":" We are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to join us us.  ","version":null,"tagName":"h2"},{"title":"👏 Support this project​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#-support-thisproject","content":" Every contribution counts! If you enjoyed this article, check out the Projectsveltos GitHub repo. You can star 🌟 the project if you find it helpful.  The GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.  Thanks for reading! ","version":null,"tagName":"h2"},{"title":"Cilium Cluster Mesh on RKE2","type":0,"sectionRef":"#","url":"/personal-blog/blog/cilium-cluster-mesh-rke2","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#introduction","content":" After spending some time working with the on-prem RKE2 lab setup, I came to notice a couple of issues while forming in an automated fashion the Cilium cluster mesh between on-prem clusters.  In today's post, we will go through the step-by-step process of forming a Cilium Cluster Mesh and explain any issues that might have arisen by following the GitOps approach. The cilium CLI will not be required. The deployment will be performed primarily via Helm and kubectl.  Additionally, we will use the shared CA (Certificate Authority) approach as this is a convenient way to form a cluster mesh in an automated fashion and also the best practise for the Hubble Relay setup. The approach will enable mTLS across clusters.  ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#lab-setup","content":" +-----------------+----------------------+----------------------+ | Cluster Name | Type | Version | +-----------------+----------------------+----------------------+ | mesh01 | RKE2 managed cluster | RKE2 v1.27.14+rke2r1 | | mesh02 | RKE2 managed cluster | RKE2 v1.27.14+rke2r1 | +-----------------+----------------------+----------------------+ +-------------------+----------+ | Deployment | Version | +-------------------+----------+ | Rancher2 Provider | 4.2.0 | | Cilium | 1.15.500 | +-------------------+----------+   ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#prerequisites","content":" ","version":null,"tagName":"h2"},{"title":"Infrastructure​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#infrastructure","content":" For this demonstration, we assume readers have at least two RKE2 clusters up and running. In our case, to create an RKE2 cluster on-prem we used the Rancher2 Terraform provider. The provider allows users to create different resources across different platforms alongside defining information for the RKE2 deployment like IP Address handling, and CNI (Container Network Interface) custom configuration.  ","version":null,"tagName":"h3"},{"title":"Cilium Cluster Mesh​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#cilium-cluster-mesh","content":" The Cluster Name and the Cluster ID must be unique.The Pods and the Services CIDR ranges must be unique across all the Kubernetes Clusters. The pods need to communicate over a unique IP address. See the IP address schema table above.Node CIDRs must be unique. The Nodes to have IP connectivity.The Cilium pods must connect to the ClusterMesh API Server service exposed on every Kubernetes cluster.  ","version":null,"tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#resources","content":" Ensure the below are satisfied.  Helm CLI installedkubectl installed  ","version":null,"tagName":"h3"},{"title":"Step 0: RKE2 Terraform Provider​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-0-rke2-terraform-provider","content":" The below snippet is an example configuration on how to deploy an RKE2 cluster via the Rancher2 Provider.   # RKE2 configuration resource &quot;rancher2_cluster_v2&quot; &quot;rke2&quot; { # Define basic cluster details like labels and annotations annotations = var.rancher_env.cluster_annotations kubernetes_version = var.rancher_env.rke2_version labels = var.rancher_env.cluster_labels enable_network_policy = var.rancher_env.network_policy # Option to enable or disable Project Network Isolation. name = var.rancher_env.cluster_id # Define the Cilium Configuration for the cluster chart_values = &lt;&lt;-EOF rke2-cilium: k8sServiceHost: 127.0.0.1 k8sServicePort: 6443 kubeProxyReplacement: true # Prepare the deployment for kube-proxy replacement operator: replicas: 1 hubble: # Enable Hubble for observability enabled: true peerService: clusterDomain: cluster.local relay: enabled: true tls: auto: certValidityDuration: 1095 enabled: true method: helm ui: enabled: true EOF # Apply machine global settings for the clusters machine_global_config = &lt;&lt;EOF cni: &quot;cilium&quot; # Enable Cilium CNI for every cluster cluster-cidr: ${var.rke_cluster_cidr} service-cidr: ${var.rke_service_cidr} disable-kube-proxy: true # Disable kube-proxy etcd-expose-metrics: false # Do not expose the etcd metrics EOF # Start building the controller and workder nodes dynamically dynamic &quot;machine_pools&quot; { for_each = var.node content { cloud_credential_secret_name = data.rancher2_cloud_credential.auth.id control_plane_role = machine_pools.key == &quot;ctl_plane&quot; ? true : false etcd_role = machine_pools.key == &quot;ctl_plane&quot; ? true : false name = machine_pools.value.name quantity = machine_pools.value.quantity worker_role = machine_pools.key != &quot;ctl_plane&quot; ? true : false machine_config { kind = rancher2_machine_config_v2.nodes[machine_pools.key].kind name = replace(rancher2_machine_config_v2.nodes[machine_pools.key].name, &quot;_&quot;, &quot;-&quot;) } } } machine_selector_config { config = null } } }   As the focus here is more on the Cilium Cluster Mesh setup, we will not go into much detail about the Terraform RKE2 deployment. If there is demand for an in-depth blog post about Terraform RKE2 deployments, feel free to get in touch.  ","version":null,"tagName":"h2"},{"title":"Step 1: Export kubeconfig​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-1-export-kubeconfig","content":" Either from the Terraform execution plan or via the Rancher UI, collect the kubeconfig of the RKE2 clusters. Alternatively, we can SSH into one of the RKE2 master nodes and collect the kubeconfig found in the directory /etc/rancher/rke2/rke2.yaml.  $ export KUBECONFIG=&lt;directory of kubeconfig&gt; $ kubectl nodes   ","version":null,"tagName":"h2"},{"title":"Step 2: Helm list and values export​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-2-helm-list-and-values-export","content":" RKE2 comes with its own Cilium CNI Helm chart. That means RKE2 clusters will have an RKE2 Cilium Helm chart deployment in the kube-system namespace.  ","version":null,"tagName":"h2"},{"title":"Validate​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#validate","content":" $ export KUBECONFIG=&lt;directory of kubeconfig&gt; $ helm list -n kube-system NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION rke2-cilium kube-system\t1 2024-07-13 09:32:09.981662 +0200 CEST deployed\trke2-cilium-1.15.500 1.15.5 rke2-coredns kube-system\t1 2024-07-13 07:05:49.846980773 +0000 UTC\tdeployed\trke2-coredns-1.29.002 1.11.1 rke2-ingress-nginx kube-system\t1 2024-07-13 07:06:24.63272854 +0000 UTC deployed\trke2-ingress-nginx-4.8.200 1.9.3 rke2-metrics-server kube-system\t1 2024-07-13 07:06:24.86243331 +0000 UTC deployed\trke2-metrics-server-2.11.100-build2023051513\t0.6.3 rke2-snapshot-controller kube-system\t1 2024-07-13 07:06:26.764326178 +0000 UTC\tdeployed\trke2-snapshot-controller-1.7.202 v6.2.1 rke2-snapshot-controller-crd kube-system\t1 2024-07-13 07:06:24.217899546 +0000 UTC\tdeployed\trke2-snapshot-controller-crd-1.7.202 v6.2.1 rke2-snapshot-validation-webhook\tkube-system\t1 2024-07-13 07:06:24.544748567 +0000 UTC\tdeployed\trke2-snapshot-validation-webhook-1.7.302 v6.2.2   ","version":null,"tagName":"h3"},{"title":"Collect rke2-cilium Helm Values​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#collect-rke2-cilium-helm-values","content":" mesh01  $ helm get values rke2-cilium -n kube-system -o yaml &gt; values_mesh01.yaml   mesh02  $ helm get values rke2-cilium -n kube-system -o yaml &gt; values_mesh02.yaml   Example values_mesh01.yaml  global: cattle: clusterId: c-m-8ffz659l clusterCIDR: 10.244.0.0/16 clusterCIDRv4: 10.244.0.0/16 clusterDNS: 10.96.0.10 clusterDomain: cluster.local rke2DataDir: /var/lib/rancher/rke2 serviceCIDR: 10.96.0.0/18 hubble: enabled: true peerService: clusterDomain: cluster.local relay: enabled: true tls: auto: certValidityDuration: 1095 enabled: true method: helm ui: enabled: true k8sServiceHost: 127.0.0.1 k8sServicePort: 6443 kubeProxyReplacement: true operator: replicas: 1   note The configuration comes from the machine_global_config and chart_values sections defined in the Terraform code found in Step 0.  ","version":null,"tagName":"h3"},{"title":"Step 3: Cilium Cluster Mesh Helm Values​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-3-cilium-cluster-mesh-helm-values","content":" To set up the Cilium cluster mesh, we need to include the rke2-charts repo and later on, update the Helm values with the required cluster mesh settings. For this demonstration, we will use the NodePort deployment. For production environments, a LoadBalancer deployment is recommended as we do not have to rely on Node availability.  ","version":null,"tagName":"h2"},{"title":"Add rke2-charts Repo​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#add-rke2-charts-repo","content":" The action should be performed in both clusters.  $ helm repo add rke2-charts https://rke2-charts.rancher.io/ $ helm repo update   ","version":null,"tagName":"h3"},{"title":"Update mesh01 Helm Values​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#update-mesh01-helm-values","content":" On the same level as global, add the below configuration.  tls: ca: cert: &quot;&quot; # Base64 encoded shared CA crt key: &quot;&quot; # Base64 encoded shared CA key cluster: name: mesh01 # The unique name of the cluster id: 1 # The unique ID of the cluster used for the cluster mesh formation clustermesh: apiserver: replicas: 2 service: type: NodePort # Set the Clustermesh API service to be of type NodePort. Not recommended for Production environments nodePort: 32379 # Define the listening port for the Clustermesh API service tls: authMode: cluster server: extraDnsNames: - &quot;mesh01.mesh.cilium.io&quot; # Define the extra DNS config: clusters: - address: &quot;&quot; ips: - &lt;Node IP&gt; # The Node IP of the mesh02 cluster name: mesh02 port: 32380 # The NodePort defined on mesh02 for the Clustermesh API service enabled: true domain: &quot;mesh.cilium.io&quot; # Define the default domain for the mesh useAPIServer: true # Enable the Clustermesh API deployment   ","version":null,"tagName":"h3"},{"title":"Update mesh02 Helm Values​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#update-mesh02-helm-values","content":" On the same level as global, add the below configuration.  tls: ca: cert: &quot;&quot; # Base64 encoded shared CA crt key: &quot;&quot; # Base64 encoded shared CA key cluster: name: mesh02 # The unique name of the cluster id: 2 # The unique ID of the cluster used for the cluster mesh formation clustermesh: apiserver: replicas: 2 service: type: NodePort # Set the Clustermesh API service to be of type NodePort. Not recommended for production environments nodePort: 32380 # Define the listening port for the Clustermesh API service tls: authMode: cluster server: extraDnsNames: - &quot;mesh02.mesh.cilium.io&quot; # Define the extra DNS config: clusters: - address: &quot;&quot; ips: - &lt;Node IP&gt; # The Node IP of the mesg01 cluster name: mesh01 # Define the name of the cluster port: 32379 # The NodePort defined on mesh02 for the Clustermesh API service enabled: true domain: &quot;mesh.cilium.io&quot; # Define the default domain for the mesh useAPIServer: true # Enable the Clustermesh API deployment   ","version":null,"tagName":"h3"},{"title":"Update mesh01/mesh02 Helm deployment​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#update-mesh01mesh02-helm-deployment","content":" To ensure the updated Helm values are applied, we will use the HELM CLI to update the rke2-cilium deployment.  $ helm upgrade rke2-cilium rke2-charts/rke2-cilium --version 1.15.500 --namespace kube-system -f values_mesh01.yaml $ helm list -n kube-system   Perform the commands for the mesh02 cluster.  tip The helm upgrade command will create a new revision of the rke2-cilium application and show if the update was successful or not. Additionally, the cilium daemonset will get restarted and the Clustermesh API deployment will get created. Execute the commands below to double-check the update action. $ kubectl rollout status daemonset cilium -n kube-system $ kubectl get pods,svc -n kube-system | grep -i clustermesh   ","version":null,"tagName":"h3"},{"title":"Step 4: Validate Cilium Cluster Mesh​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-4-validate-cilium-cluster-mesh","content":" As we do not use the Cilium CLI, to ensure the Cilium cluster mesh works as expected, we will exec into the cilium daemonset and check the required details.  $ kubectl get ds -n kube-system | grep -i cilium cilium 4 4 4 4 4 kubernetes.io/os=linux 7d6h   ","version":null,"tagName":"h2"},{"title":"On mesh01 and mesh02​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#on-mesh01-and-mesh02","content":" $ kubectl exec -it ds/cilium -n kube-system -- cilium status | grep -i clustermesh Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, install-portmap-cni-plugin (init), config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) ClusterMesh: 1/1 clusters ready, 11 global-services   On both sides, the ClusterMesh should point to 1/1 clusters ready.  $ kubectl exec -it ds/cilium -n kube-system -- cilium-health status Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, install-portmap-cni-plugin (init), config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) Probe time: 2024-07-20T13:58:47Z Nodes: mesh01/mesh01-controller-3d16581b-7q5bj (localhost): Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=693.829µs HTTP to agent: OK, RTT=118.583µs Endpoint connectivity to 10.244.1.71: ICMP to stack: OK, RTT=688.411µs HTTP to agent: OK, RTT=251.927µs mesh01/mesh01-controller-3d16581b-v58rq: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=671.007µs HTTP to agent: OK, RTT=237.395µs Endpoint connectivity to 10.244.0.75: ICMP to stack: OK, RTT=702.976µs HTTP to agent: OK, RTT=342.115µs mesh01/mesh01-worker-7ced0c6c-lz9sp: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=819.21µs HTTP to agent: OK, RTT=397.398µs Endpoint connectivity to 10.244.3.215: ICMP to stack: OK, RTT=821.223µs HTTP to agent: OK, RTT=465.965µs mesh01/mesh01-worker-7ced0c6c-w294x: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=738.787µs HTTP to agent: OK, RTT=335.803µs Endpoint connectivity to 10.244.2.36: ICMP to stack: OK, RTT=693.326µs HTTP to agent: OK, RTT=426.571µs mesh02/mesh02-controller-52d8e160-b27rn: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=683.278µs HTTP to agent: OK, RTT=335.076µs Endpoint connectivity to 10.245.0.106: ICMP to stack: OK, RTT=818.386µs HTTP to agent: OK, RTT=387.314µs mesh02/mesh02-controller-52d8e160-q4rvf: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=683.097µs HTTP to agent: OK, RTT=301.448µs Endpoint connectivity to 10.245.1.75: ICMP to stack: OK, RTT=748.101µs HTTP to agent: OK, RTT=510.124µs mesh02/mesh02-worker-a1c14ae0-5l759: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=631.954µs HTTP to agent: OK, RTT=266.391µs Endpoint connectivity to 10.245.3.232: ICMP to stack: OK, RTT=751.853µs HTTP to agent: OK, RTT=433.049µs mesh02/mesh02-worker-a1c14ae0-c7tcb: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=671.823µs HTTP to agent: OK, RTT=365.949µs Endpoint connectivity to 10.245.2.69: ICMP to stack: OK, RTT=690.894µs HTTP to agent: OK, RTT=466.73µs   note With the cilium-health status command, you should be able to see all the nodes from both clusters. Check the ICMP and HTTP status. Should be &quot;OK&quot;. Also, it might take a couple of minutes till the cilium-health status is available. If the time-out persists, have a look at the firewall rules and whether traffic between the clusters is allowed.  warning The NodePort IP addresses set for the cluster mesh need to be the IP addresses of the worker node instead of the master node. If they are the master node, the Cilium Cluster Mesh will not get deployed and we will get the below error. remote-etcd-cluster01 4m25s ago 4s ago 22 failed to detect whether the cluster configuration is required: etcdserver: permission denied   ","version":null,"tagName":"h3"},{"title":"Step 5: Hubble UI​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-5-hubble-ui","content":" To work with the Hubble UI we can use the kubectl port-forward of the Hubble UI service or update the existing rke2-cilium deployment on one of the nodes and expose the Hubble UI as a NodePort service. Just include the below in the values_mesh01.yaml or the values_mesh02.yaml file.   ui: enabled: true service: type: NodePort   For more information about the RKE2 Cilium Helm Chart values, have a look here.  ","version":null,"tagName":"h2"},{"title":"Conclusions​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#conclusions","content":" This is it! We performed a Cilium cluster mesh between two on-prem RKE2 clusters in just a few steps! 🎉  It's a wrap for this post! 🎉 Thanks for reading! Stay tuned for more exciting updates! ","version":null,"tagName":"h2"}],"options":{"languages":["en","de"],"id":"default"}}