{"searchDocs":[{"title":"Civo Navigate Berlin 2024","type":0,"sectionRef":"#","url":"/personal-blog/blog/civo-navigate-berlin-2024","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Civo Navigate Berlin 2024","url":"/personal-blog/blog/civo-navigate-berlin-2024#introduction","content":" Today's post will not be as technical as previous ones however, I wanted to share my experience at the Civo Navigate in Berlin. I had the chance to talk at the conference, present Sveltos and how it can be used to painlessly deploy different Kubernetes applications and monitoring capabilities on a fleet of clusters.  Apart from that, I attended many different sessions covering relevant topics (Cloud native, Security, Thought Leadership, AI) and meet fellow enthusiasts.  In the sections below, I will outline some of my highlights and afterwards provide an introduction to the Sveltos presentation alongside the useful resources.    ","version":null,"tagName":"h2"},{"title":"Civo Navigate - Day 1​","type":1,"pageTitle":"Civo Navigate Berlin 2024","url":"/personal-blog/blog/civo-navigate-berlin-2024#civo-navigate---day-1","content":" The day started quite chill by registering at the event, receiving a badge and becoming familiar with the location. Breakfast and coffee were provided which makes everything much easier.  Fully energized, the conference started with the keynotes and a positive outline of cloud computing, innovation and sustainability, which I can personally relate to it. More details about the Civo partnership with Deep Green can be found here. Afterwards, many empowering sessions took place, but some could not attend everything. I chose to attend those around developers empowerment and motivation. How developers can work best in diverse environments and how we can allow them to perform best by providing training, noise (notification) minimization, unnecessary meetings and freedom to think and decide on innovative solutions.  From a technical side, I enjoyed a session about testing in modern CI/CD pipelines which gave me a different view of Kubernetes testing. This particular session was an introduction to testkube. Not sure how much you can get from the free version in comparison to an enterprise one. Furthermore, I appreciated the informative session about the Event Driven Ansible to reduce automation reaction time.  After a long day and full of new ideas to try out in the lab, why not chill in the gaming area?    ","version":null,"tagName":"h2"},{"title":"Civo Navigate - Day 2​","type":1,"pageTitle":"Civo Navigate Berlin 2024","url":"/personal-blog/blog/civo-navigate-berlin-2024#civo-navigate---day-2","content":" The day started with empowering keynotes from Kelsey Hightower about deep tech, Kubernetes, AI, and how the future might look like. The day continued with mostly technical sessions about open-source tools for Identity Management in Cloud Native stacks, platform engineering and KUTTL for End-to-End (E2E) testing.  Some of the mentioned IAM tools are listed below.  AaiaiamaliveGCP Permissions  tip Once the session recordings are available, a link will be included.  ","version":null,"tagName":"h2"},{"title":"Civo Navigate - Sveltos​","type":1,"pageTitle":"Civo Navigate Berlin 2024","url":"/personal-blog/blog/civo-navigate-berlin-2024#civo-navigate---sveltos","content":" For the conference, we decided to demonstrate how Sveltos can be used to deploy and take over the control of the Container Network Interface (CNI) lifecycle with one manifest file while enabling Cilium Hubble for Network observability. In the second part of the presentation, we demonstrated how to create another set of manifest files to integrate Grafana, Prometheus and Loki for container logs output.  ","version":null,"tagName":"h2"},{"title":"Diagram​","type":1,"pageTitle":"Civo Navigate Berlin 2024","url":"/personal-blog/blog/civo-navigate-berlin-2024#diagram","content":"   ","version":null,"tagName":"h3"},{"title":"Git Repository​","type":1,"pageTitle":"Civo Navigate Berlin 2024","url":"/personal-blog/blog/civo-navigate-berlin-2024#git-repository","content":" The Git repository with the manifest files and the execution instructions are located here.  ","version":null,"tagName":"h3"},{"title":"Conclusions​","type":1,"pageTitle":"Civo Navigate Berlin 2024","url":"/personal-blog/blog/civo-navigate-berlin-2024#conclusions","content":" All in all, it was an awesome experience to have the chance to attend and speak at the conference. If you plan to attend any upcoming Civo Navigate conferences, check out the link. I am pretty confident you will have an enjoyable experience!  It's a wrap for this post! 🎉 Thanks for reading! Stay tuned for more exciting updates! ","version":null,"tagName":"h2"},{"title":"OSSummit Europe 2024","type":0,"sectionRef":"#","url":"/personal-blog/blog/ossummit-europe-2024","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#introduction","content":" Sveltos is on tour! Another non-technical post describing my experience at the OSSummit Europe 2024. Apart from outlining my experience, the post will include useful resources on open-source projects I learned during the event.  About Sveltos, Gianluca Mardente and I had the chance to talk at the conference and present Sveltos and how it is used to deploy and manage different Kubernetes applications and add-ons in a Multi-Cloud setup.  In the sections below, I will outline my highlights of the conference and what I have learned, while later on, I will describe what we presented about Sveltos and where to locate the required resources.    ","version":null,"tagName":"h2"},{"title":"OSSummit - Day 1​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#ossummit---day-1","content":" As with any other conference, the day began with the registration, badge pick-up and familiarisation with the location. Of course, coffee was widely available in almost every corner of the conference!    For a quick look at the Day 1 keynotes, find below my highlights alongside the resources for further reading.  Linux Foundation, CNCF, Unified Patents - partnership expansionLinux Foundation, Valkey 8.0Developer Relations Foundation AnnouncementOpen Source Innovation in Artificial Intelligence and DataThe Linux Foundation Europe  From a technical side, there was an announcement about the formation of the OpenSearch Software Foundation. AWS decided to move OpenSearch to the Linux Foundation. OpenSearch is a community-driven, Apache 2.0-licensed open-source search and analytics suite that makes it easy to ingest, search, visualise, and analyse data.  ","version":null,"tagName":"h2"},{"title":"OpenSearch - Resources​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#opensearch---resources","content":" GitHubOpenSearch Playground  Apart from that, I was introduced to cdk8s which is an open-source software development framework for defining Kubernetes applications. It synthesises applications into standard Kubernetes manifests files and supports a wide range of available programming languages like Typescript, Javascript, Go, Python, and Java.  ","version":null,"tagName":"h3"},{"title":"cdk8s - Resources​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#cdk8s---resources","content":" GitHubcdk8s Documentation  ","version":null,"tagName":"h3"},{"title":"OSSummit - Day 2​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#ossummit---day-2","content":" Why not start the day with a 5k run? Around 20ish people gathered together and enjoyed a morning run near the Danube River! The Day 2 keynotes involved topics around secure supply chain consumption with existing frameworks, how to be compliant in a cloud-native landscape and ways to perform Policy as Code.  ","version":null,"tagName":"h2"},{"title":"Frameworks​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#frameworks","content":" Secure Supply Chain Consumption Framework (S2C2F)SLSA  ","version":null,"tagName":"h3"},{"title":"Container Image Patch Tools​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#container-image-patch-tools","content":" COPA  COPA is an open-source tool written in Go that allows DevSecOps engineers to directly patch container images given vulnerability scanning results from popular tools like Trivy.  ","version":null,"tagName":"h3"},{"title":"Supply Chain Control Plane​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#supply-chain-control-plane","content":" Chainloop  Chainloop is an open-source project I found out while crowling at the booth. Chainloop provides a single source of truth for metadata and artifacts, plus a declarative attestation process. We can declaratively state the pieces of evidence and artifact expectations for different CI/CD pipelines.  ","version":null,"tagName":"h3"},{"title":"IoT​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#iot","content":" If you are interested in IoT, I had the chance to learn more about Zephyr while attending interactive sessions about IoT development.  ","version":null,"tagName":"h3"},{"title":"Open-source and Research​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#open-source-and-research","content":" If you have an interest in open-source Research and Enablement check out TODO and LERO/CURIOOS initialives alongside the opensource.net website for relevant topics.  ","version":null,"tagName":"h3"},{"title":"OSSummit - Day 3​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#ossummit---day-3","content":" Day 3 started with a group talk on Kernel development and what development looks like with Rust. I found interesting the talk from Paolo De Rosa who talked about the European Digital Identity (EUDI) Wallet and the fact that they want to achieve this with the use of open-source while getting help from the community.  European Digital Identity (EUDI) Wallet  ","version":null,"tagName":"h2"},{"title":"OSSummit - Sveltos​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#ossummit---sveltos","content":" For the conference, we decided to demonstrate how Sveltos can be used to deploy and manage the Container Network Interface (CNI) lifecycle on a fleet of clusters with one manifest file while enabling Cilium Hubble for Network observability. In the second part of the presentation, we demonstrated how to create another set of manifest files to deploy Kyverno and specific cluster policies down the clusters based on their scope.  ","version":null,"tagName":"h2"},{"title":"Diagram​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#diagram","content":"   ","version":null,"tagName":"h3"},{"title":"Git Repository​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#git-repository","content":" The Git repository with the manifest files and the execution instructions is located here.  ","version":null,"tagName":"h3"},{"title":"Conclusions​","type":1,"pageTitle":"OSSummit Europe 2024","url":"/personal-blog/blog/ossummit-europe-2024#conclusions","content":" I had a great fun at the conference! Not only had the chance to present alongside Gianluca, but I also met and interacted with cloud-native enthusiasts.  Till August 2025!    It's a wrap for this post! 🎉 Thanks for reading! Stay tuned for more exciting updates! ","version":null,"tagName":"h2"},{"title":"Rancher RKE2 Cluster on Azure","type":0,"sectionRef":"#","url":"/personal-blog/blog/rancher-rke2-cilium-azure","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#introduction","content":" For the last couple of days, I have been working on a new use case installing RKE2 clusters powered with Cilium on Azure Cloud. The requirement at hand was to use a Rancher instance and from there start deploying RKE2 clusters. After going through the official Rancher documentation, I have noticed that the instructions provided to pre-configure Azure Cloud are outdated.  In today's blog post, we will cover all the required steps taken to configure the Azure cloud-free credits to deploy RKE2 clusters with Cilium in that environment. Additionally, we will cover any limitations that come with the free credit concept.    ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#lab-setup","content":" +-----------------------------+------------------+----------------------+ | Cluster Name | Type | Version | +-----------------------------+------------------+----------------------+ | Rancher | k3s cluster | v1.28.7+k3s1 | | Downstream RKE2 cluster | RKE2 | v1.28.11+rke2r1 | +-----------------------------+------------------+----------------------+ +-------------------+----------+ | Deployment | Version | +-------------------+----------+ | Cilium | 1.15.500 | +-------------------+----------+   ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#prerequisites","content":" ","version":null,"tagName":"h2"},{"title":"Rancher Server​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#rancher-server","content":" We do not concentrate on installing Rancher. If you are not sure how to install Rancher, have a look at the official documentation here or go through the guide I created a couple of weeks back here.  ","version":null,"tagName":"h3"},{"title":"Azure Free Credits​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#azure-free-credits","content":" For this demonstration, we will use the Azure free credits offering. The approach taken is more than enough to give readers a free and good understanding of how to set up the Azure cloud environment to perform RKE2 deployments with Rancher.  Ensure the below are satisfied.  Helm CLI installed (Optional Step)kubectl installed  ","version":null,"tagName":"h3"},{"title":"Set up Azure Cloud Environment​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#set-up-azure-cloud-environment","content":" In this section, we will provide readers with all the needed guidance on setting up their environment for the RKE2 deployment  ","version":null,"tagName":"h2"},{"title":"Retrieve the Tenant ID​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#retrieve-the-tenant-id","content":" The Tenant ID identifies which Azure Active Directory (AD) instance the application sits under. To retrieve the Tenant ID, follow the steps below.  Login to Azure portalNavigate to HomeSearch for Microsoft Entra IDNavigate to Manage &gt; PropertiesGrab the Tenant ID once the new windows appear  ","version":null,"tagName":"h3"},{"title":"Create a Rancher App Registration​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#create-a-rancher-app-registration","content":" Azure App Registration is the process of registering an application with Azure AD.  Access the Azure portal Navigate to Home &gt; App registrations &gt; + New Registration Choose the below details: Name: What is the name your application will haveSupported account types: In my case, I chose the first one, &quot;Accounts in this organizational directory only (Default Directory only - Single tenant)&quot;Redirect URI (optional): set Web and leave the Sign-on URL empty or add your own URI Click the Register button to create the application  ","version":null,"tagName":"h3"},{"title":"Create an Azure Client Secret for App Registration​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#create-an-azure-client-secret-for-app-registration","content":" Access the Azure portalNavigate to Home &gt; App registrations &gt; App nameNavigate to Manage &gt; Certificates &amp; secretsClick on the + New client secretProvide a description and an expiry dateClick + AddCopy the Value and proceed with the configuraition  ","version":null,"tagName":"h3"},{"title":"Create App Registration Permissions​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#create-app-registration-permissions","content":" Access the Azure portalNavigate to Home &gt; Subscriptions &gt; Your subscription name &gt; Access Control (IAM)Click on the + Add &gt; Add role assigmentOpen the Privileged administrator roles tabFor Role, select ContributorClick on NextMembers and + Select members and then choose or type for Rancher. If your application name is something else, provide the application name created in a previous stepReview + assingProceed with the creation  ","version":null,"tagName":"h3"},{"title":"Azure Free Account Limitations​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#azure-free-account-limitations","content":" Find below some of the limitations spotted with the free-credit subscription.  You cannot have more than one resource poolsYou cannot have more than 3 Public IP addressesYou cannot create a resource pool with a VM Size greater than Standard_D2_v2  ","version":null,"tagName":"h2"},{"title":"Set up Rancher Cloud Credentials​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#set-up-rancher-cloud-credentials","content":" Once we have the Azure environment ready, we can move on with Rancher. The first thing we have to do is to create Azure Cloud Credentials. The cloud credentials will be used to provision clusters or can be used in other node templates.  Login to RancherNavigate to Home &gt; Cluster management &gt; Cloud Credentials &gt; Create &gt; Choose AzureProvide a name, tenant ID (Home &gt; Subscriptions), clientID (Home &gt; App registrations &gt; Rancher &gt; copy the Application (client) ID), client secret (Home &gt; App registrations &gt; App name &gt; manage &gt; certificates &amp; secrets)Click the &quot;Create&quot; button and ensure no error appears on the screen  ","version":null,"tagName":"h2"},{"title":"Create an RKE2 cluster with Cilium​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#create-an-rke2-cluster-with-cilium","content":" It is time to use the Azure cloud credentials to create an RKE2 cluster on Azure with Cilium.  Login to the Rancher UINavigiate to Home &gt; Cluster management &gt; Create &gt; ensure RKE2 is selected &gt; Choose AzureThe Cloud Credentials field will get populated automatically. Fill out the details below. Cluster Name: Set a cluster nameCluster Description: Set a cluster descriptionSet the Machine Pools details. This reflects the nodes we will have on the cluster and their specified role (controller or worker nodes). Pool Name: Left the defaultMachine Count: Set to 2 due to limitationsLocation: Choose your favourable location Continue with the Cluster Configuration &gt; Basics Kubernetes Version: Define the preferred Kubernetes versionContainer Network: Choose Cilium tip We can leave the rest of the configuration as default. However, if we want to enable Cilium with kube-proxy replacement, we can update the cluster by editing the YAML configuration instead. This can be done by clicking the Edit as YAML button at the bottom right-hand side. Continue with the Cluster Configuration &gt; Advanced Additional Controller Manager Args: Set --configure-cloud-routes=false Click &quot;Save&quot;  note The cluster creation might take up to 20 minutes. Be patient as a couple of resources and Azure items are created for this cluster. After 20 minutes:  ","version":null,"tagName":"h2"},{"title":"RKE2 Cluster Validation​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#rke2-cluster-validation","content":" Now that the RKE2 cluster is up and running, let's perform some validation steps to ensure the cluster is working as expected.  $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME test03-pool1-cff6be83-jfnrw Ready control-plane,etcd,master,worker 50m v1.28.11+rke2r1 192.168.0.5 &lt;none&gt; Ubuntu 18.04.6 LTS 5.4.0-1109-azure containerd://1.7.17-k3s1 test03-pool1-cff6be83-wml94 Ready control-plane,etcd,master,worker 56m v1.28.11+rke2r1 192.168.0.4 &lt;none&gt; Ubuntu 18.04.6 LTS 5.4.0-1109-azure containerd://1.7.17-k3s1 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE cilium-4wzld 1/1 Running 0 49m cilium-7st9q 1/1 Running 0 55m cilium-operator-695b4bfc8b-rrdpw 1/1 Running 3 (46m ago) 55m cilium-operator-695b4bfc8b-wvswc 1/1 Running 5 (44m ago) 55m cloud-controller-manager-test03-pool1-cff6be83-jfnrw 1/1 Running 4 (43m ago) 49m cloud-controller-manager-test03-pool1-cff6be83-wml94 1/1 Running 5 (44m ago) 55m etcd-test03-pool1-cff6be83-jfnrw 1/1 Running 0 49m etcd-test03-pool1-cff6be83-wml94 1/1 Running 0 55m helm-install-rke2-cilium-4g9m6 0/1 Completed 0 56m helm-install-rke2-coredns-4fcl8 0/1 Completed 0 56m helm-install-rke2-ingress-nginx-sllhl 0/1 Completed 0 56m helm-install-rke2-metrics-server-djld4 0/1 Completed 0 56m helm-install-rke2-snapshot-controller-6vktm 0/1 Completed 0 56m helm-install-rke2-snapshot-controller-crd-fmr6n 0/1 Completed 0 56m helm-install-rke2-snapshot-validation-webhook-tj4b8 0/1 Completed 0 56m kube-apiserver-test03-pool1-cff6be83-jfnrw 1/1 Running 1 (43m ago) 49m kube-apiserver-test03-pool1-cff6be83-wml94 1/1 Running 1 43m kube-controller-manager-test03-pool1-cff6be83-jfnrw 1/1 Running 3 (46m ago) 49m kube-controller-manager-test03-pool1-cff6be83-wml94 1/1 Running 1 (44m ago) 46m kube-proxy-test03-pool1-cff6be83-jfnrw 1/1 Running 0 49m kube-proxy-test03-pool1-cff6be83-wml94 1/1 Running 0 55m kube-scheduler-test03-pool1-cff6be83-jfnrw 1/1 Running 3 (46m ago) 49m kube-scheduler-test03-pool1-cff6be83-wml94 1/1 Running 1 (44m ago) 46m rke2-coredns-rke2-coredns-84b9cb946c-bd8p6 1/1 Running 0 49m rke2-coredns-rke2-coredns-84b9cb946c-npgl2 1/1 Running 0 55m rke2-coredns-rke2-coredns-autoscaler-b49765765-lp244 1/1 Running 0 55m rke2-ingress-nginx-controller-qk5mc 1/1 Running 0 53m rke2-ingress-nginx-controller-wbdf8 1/1 Running 0 48m rke2-metrics-server-655477f655-j9vzw 1/1 Running 0 54m rke2-snapshot-controller-59cc9cd8f4-8fhwb 1/1 Running 6 (44m ago) 54m rke2-snapshot-validation-webhook-54c5989b65-9vb9w 1/1 Running 0 54m $ kubectl get pods,svc -n cattle-system NAME READY STATUS RESTARTS AGE pod/cattle-cluster-agent-59df97fc7f-44q74 1/1 Running 2 (43m ago) 46m pod/cattle-cluster-agent-59df97fc7f-vbvbb 1/1 Running 0 45m pod/helm-operation-4clvk 0/2 Completed 0 50m pod/helm-operation-89qsx 0/2 Completed 0 51m pod/rancher-webhook-d677765b4-6vrkh 1/1 Running 1 (44m ago) 44m pod/system-upgrade-controller-6f86d6d4df-jvd9k 1/1 Running 0 51m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cattle-cluster-agent ClusterIP 10.43.82.34 &lt;none&gt; 80/TCP,443/TCP 56m service/rancher-webhook ClusterIP 10.43.80.125 &lt;none&gt; 443/TCP 51m   $ kubectl exec -it ds/cilium -n kube-system -- cilium status Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, install-portmap-cni-plugin (init), config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) KVStore: Ok Disabled Kubernetes: Ok 1.28 (v1.28.11+rke2r1) [linux/amd64] Kubernetes APIs: [&quot;EndpointSliceOrEndpoint&quot;, &quot;cilium/v2::CiliumClusterwideNetworkPolicy&quot;, &quot;cilium/v2::CiliumEndpoint&quot;, &quot;cilium/v2::CiliumNetworkPolicy&quot;, &quot;cilium/v2::CiliumNode&quot;, &quot;cilium/v2alpha1::CiliumCIDRGroup&quot;, &quot;core/v1::Namespace&quot;, &quot;core/v1::Pods&quot;, &quot;core/v1::Service&quot;, &quot;networking.k8s.io/v1::NetworkPolicy&quot;] KubeProxyReplacement: False [eth0 192.168.0.5 fe80::6245:bdff:fe94:cd06] Host firewall: Disabled SRv6: Disabled CNI Chaining: portmap CNI Config file: successfully wrote CNI configuration file to /host/etc/cni/net.d/05-cilium.conflist Cilium: Ok 1.15.5 (v1.15.5-8c7e442c) NodeMonitor: Disabled Cilium health daemon: Ok IPAM: IPv4: 9/254 allocated from 10.42.1.0/24, IPv4 BIG TCP: Disabled IPv6 BIG TCP: Disabled BandwidthManager: Disabled Host Routing: Legacy Masquerading: IPTables [IPv4: Enabled, IPv6: Disabled] Controller Status: 52/52 healthy Proxy Status: OK, ip 10.42.1.172, 0 redirects active on ports 10000-20000, Envoy: embedded Global Identity Range: min 256, max 65535 Hubble: Disabled Encryption: Disabled Cluster health: 2/2 reachable (2024-07-26T09:19:17Z) Modules Health: Stopped(0) Degraded(0) OK(11)   ","version":null,"tagName":"h2"},{"title":"✉️ Contact​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#️-contact","content":" If you have any questions, feel free to get in touch! You can use the Discussions option found here or reach out to me on any of the social media platforms provided. 😊  We look forward to hearing from you!  ","version":null,"tagName":"h2"},{"title":"Conclusions​","type":1,"pageTitle":"Rancher RKE2 Cluster on Azure","url":"/personal-blog/blog/rancher-rke2-cilium-azure#conclusions","content":" This is it! We performed an RKE2 cluster with Cilium using the Rancher UI in just a few clicks. 🎉 In an upcoming blog post, we will demonstrate how to perform the same with either Terraform or OpenTofu!  It's a wrap for this post! 🎉 Thanks for reading! Stay tuned for more exciting updates! ","version":null,"tagName":"h2"},{"title":"Cilium on EKS with Sveltos","type":0,"sectionRef":"#","url":"/personal-blog/blog/cilium-eks-sveltos","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#introduction","content":" In today's blog post, we will demonstrate an easy way of deploying and controlling Cilium on an EKS cluster with Sveltos.  As the majority of the documentation out there provides a step-by-step installation directly with the Helm chart commands, we decided to demonstrate a different approach, the GitOps approach, with the use of Sveltos ClusterProfile CRD (Custom Resource Definition).    We will utilise the Terraform AWS EKS module to create an EKS cluster. Once the cluster is up and running, we will register it with Sveltos. Then, we will update the aws-core daemonset to support ENI mode and remove the kube-proxy Kubernetes resources as Cilium will take over.  ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#lab-setup","content":" +-----------------+-------------------+--------------------------+ | Cluster Name | Type | Version | +-----------------+-------------------+--------------------------+ | mgmt | Management Cluster| RKE2 v1.28.9+rke2r1 | | eks-test01 | Managed Cluster | EKS v1.28.10-eks-49c6de4 | +-----------------+-------------------+--------------------------+ +-------------+----------+ | Deployment | Version | +-------------+----------+ | Cilium | v1.14.8 | | sveltosctl | v0.27.0 | +-------------+----------+   ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#prerequisites","content":" To follow along with the blog post, ensure the below are satisfied.  AWS Service AccountAWS CLI installedTerraform installedkubectl installedsveltosctl installed  ","version":null,"tagName":"h2"},{"title":"Step 1: Create EKS Cluster with Terraform​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-1-create-eks-cluster-with-terraform","content":" The easiest way to spin up an EKS cluster is by following the recommended training and resources from the Hashicorp website. Find the training material and the Git repository further below.  Training: https://developer.hashicorp.com/terraform/tutorials/kubernetes/eks GitHub Repository: https://github.com/hashicorp/learn-terraform-provision-eks-cluster  To execute the Terraform plan, a valid AWS Service Account should be available with the right permissions to create the required resources. For more information about the AWS Service Accounts, have a look here.  To get the cluster kubeconfig and start interacting with the cluster, the AWS CLI is used. Modify and execute the command below.  $ aws eks update-kubeconfig --region &lt;the region the cluster created&gt; --name &lt;the name of the cluster&gt;   tip The command will save the kubeconfig in the default directory ~/.kube/config. If the file should be stored elsewhere, pass the argument --kubeconfig and specify the output directory. For more details, check out the link.  ","version":null,"tagName":"h2"},{"title":"Step 2: Register Cluster with Sveltos​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-2-register-cluster-withsveltos","content":" Once we have access to the cluster, it is time to proceed with the Sveltos cluster registration. As this is a cloud Kubernetes cluster, we need to ensure Sveltos has the right set of permissions to perform the Kubernetes deployments and add-ons. To do that, we will utilise sveltosctl and generate a new kubeconfig file.  Download the sveltosctl binary here.  ","version":null,"tagName":"h2"},{"title":"Generate Sveltos kubeconfig​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#generate-sveltos-kubeconfig","content":" tip Ensure you are authenticated with the AWS service account before performing the command below.  $ export KUBECONFIG=&lt;directory of the EKS kubeconfig file&gt; $ sveltosctl generate kubeconfig --create --expirationSeconds=86400 &gt; eks_kubeconfig.yaml   The sveltosctl redirects the output of the command into a file named eks_kubeconfig.yaml. The file will be used by Sveltos for the managed cluster registration.  ","version":null,"tagName":"h3"},{"title":"Register EKS Cluster - Management Cluster​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#register-ekscluster---management-cluster","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ sveltosctl register cluster --namespace=&lt;namespace&gt; --cluster=&lt;cluster name&gt; \\ --kubeconfig=&lt;path to Sveltos file with Kubeconfig&gt; \\ --labels=env=test   The command above will register the EKS cluster with Sveltos on the mentioned namespace, and name and will attach the cluster label env=test defined.  note If the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.  $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl get sveltosclusters -A --show-labels NAMESPACE NAME READY VERSION LABELS mgmt mgmt true v1.28.9+rke2r1 sveltos-agent=present test eks-test01 true v1.28.10-eks-49c6de4 env=test,sveltos-agent=present   ","version":null,"tagName":"h3"},{"title":"Step 3: Update the EKS cluster​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-3-update-the-ekscluster","content":" As we would like to use Cilium with the Kube Proxy replacement and the ENI mode enabled, we need to perform additional actions. As the kube-proxy daemonset is already installed, we have to remove all related resources and update the aws-node daemonset to support the ENI mode.  ","version":null,"tagName":"h2"},{"title":"Validation​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#validation","content":" $ kubectl get pods,ds -n kube-system NAME READY STATUS RESTARTS AGE pod/aws-node-4x8sq 2/2 Running 0 16m pod/aws-node-vjtlx 2/2 Running 0 16m pod/aws-node-xp7vl 2/2 Running 0 16m pod/coredns-648485486-t5sxm 1/1 Running 0 20m pod/coredns-648485486-tv4h5 1/1 Running 0 20m pod/ebs-csi-controller-5df9db689f-8hmdm 6/6 Running 0 15m pod/ebs-csi-controller-5df9db689f-qmxhs 6/6 Running 0 15m pod/ebs-csi-node-2rspx 3/3 Running 0 15m pod/ebs-csi-node-gvtfj 3/3 Running 0 15m pod/ebs-csi-node-t96ch 3/3 Running 0 15m pod/kube-proxy-4jxlt 1/1 Running 0 16m pod/kube-proxy-hgx9h 1/1 Running 0 16m pod/kube-proxy-l877x 1/1 Running 0 16m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/aws-node 3 3 3 3 3 &lt;none&gt; 20m daemonset.apps/ebs-csi-node 3 3 3 3 3 kubernetes.io/os=linux 16m daemonset.apps/ebs-csi-node-windows 0 0 0 0 0 kubernetes.io/os=windows 16m daemonset.apps/kube-proxy 3 3 3 3 3 &lt;none&gt; 20m   ","version":null,"tagName":"h3"},{"title":"Delete kube-proxy Resources​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#delete-kube-proxy-resources","content":" $ export KUBECONFIG=&lt;directory of the EKS kubeconfig file&gt; $ kubectl delete ds kube-proxy -n kube-system # Remove the kube-proxy daemonset $ kubectl delete cm kube-proxy -n kube-system # Remove the kube-proxy ConfigMap   ","version":null,"tagName":"h3"},{"title":"Update aws-node Resources​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#update-aws-node-resources","content":" $ kubectl patch daemonset aws-node --type='strategic' -p='{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;nodeSelector&quot;:{&quot;io.cilium/aws-node-enabled&quot;:&quot;true&quot;}}}}}' -n kube-system # This is required based on the Cilium documentation to enable the ENI mode   $ kubectl get pods,ds -n kube-system NAME READY STATUS RESTARTS AGE pod/coredns-648485486-t5sxm 1/1 Running 0 22m pod/coredns-648485486-tv4h5 1/1 Running 0 22m pod/ebs-csi-controller-5df9db689f-8hmdm 6/6 Running 0 17m pod/ebs-csi-controller-5df9db689f-qmxhs 6/6 Running 0 17m pod/ebs-csi-node-2rspx 3/3 Running 0 17m pod/ebs-csi-node-gvtfj 3/3 Running 0 17m pod/ebs-csi-node-t96ch 3/3 Running 0 17m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/aws-node 0 0 0 0 0 io.cilium/aws-node-enabled=true 22m daemonset.apps/ebs-csi-node 3 3 3 3 3 kubernetes.io/os=linux 17m daemonset.apps/ebs-csi-node-windows 0 0 0 0 0 kubernetes.io/os=windows 17m   tip The aws-node daemonset scaled down to 0 replicas.  ","version":null,"tagName":"h3"},{"title":"Step 4: Create Sveltos ClusterProfile​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-4-create-sveltos-clusterprofile","content":" It is time to create a Sveltos ClusterProfile and deploy Cilium to the EKS cluster with the label set to env=test. Following the Cilium documentation, we will enable the required Helm values for the kube-proxy replacement and the ENI mode.  --- apiVersion: config.projectsveltos.io/v1alpha1 kind: ClusterProfile metadata: name: cilium-1148 spec: clusterSelector: env=test # Deploy Cilium v1.14.8 to any cluster with the cluster label set to env=test helmCharts: - chartName: cilium/cilium chartVersion: 1.14.8 helmChartAction: Install releaseName: cilium releaseNamespace: kube-system repositoryName: cilium repositoryURL: https://helm.cilium.io/ values: | eni: enabled: true ipam: mode: eni egressMasqueradeInterfaces: eth0 routingMode: native kubeProxyReplacement: true k8sServiceHost: &lt;The Server API FQDN or IP Address&gt; # The information can be exctracted from the kubeconfig file or the AWS UI k8sServicePort: &lt;The Server API listening port&gt; # The information can be extracted from the kubeconfig file or the AWS UI nodePort: enabled: true debug: enabled: true   The ClusterProfile will deploy Cilium CNI to any cluster with the cluster label set to env=test. It will then deploy the Cilium Helm chart in the kube-system namespace alongside the kube-proxy replacement and the ENI mode. Hubble is also enabled.  ","version":null,"tagName":"h3"},{"title":"Step 5: Deploy Cilium and Validate​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#step-5-deploy-cilium-andvalidate","content":" To see and evaluate the results, the Sveltos ClusterProfile will be deployed to the management cluster.  $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl apply -f &quot;clusterprofile_cilium1148.yaml&quot;   ","version":null,"tagName":"h2"},{"title":"Validation​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#validation-1","content":" $ ./sveltosctl show addons +-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+ | CLUSTER | RESOURCE TYPE | NAMESPACE | NAME | VERSION | TIME | PROFILES | +-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+ | test/eks-test01 | helm chart | kube-system | cilium | 1.14.8 | 2024-06-18 14:39:26 +0000 UTC | ClusterProfile/cilium-1148 | +-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+   $ export KUBECONFIG=&lt;directory of the EKS kubeconfig file&gt; $ kubectl get pods -n kube-system | grep -i cilium cilium-2vg4c 1/1 Running 0 54s cilium-operator-594f4858f6-km2wh 1/1 Running 0 54s cilium-operator-594f4858f6-xx2q6 1/1 Running 0 55s cilium-qrwwf 1/1 Running 0 55s cilium-s55v5 1/1 Running 0 54s   $ kubectl exec -it cilium-2vg4c -n kube-system -- cilium status Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) KVStore: Ok Disabled Kubernetes: Ok 1.28+ (v1.28.10-eks-49c6de4) [linux/amd64] Kubernetes APIs: [&quot;EndpointSliceOrEndpoint&quot;, &quot;cilium/v2::CiliumClusterwideNetworkPolicy&quot;, &quot;cilium/v2::CiliumEndpoint&quot;, &quot;cilium/v2::CiliumNetworkPolicy&quot;, &quot;cilium/v2::CiliumNode&quot;, &quot;cilium/v2alpha1::CiliumCIDRGroup&quot;, &quot;core/v1::Namespace&quot;, &quot;core/v1::Pods&quot;, &quot;core/v1::Service&quot;, &quot;networking.k8s.io/v1::NetworkPolicy&quot;] KubeProxyReplacement: True [eth0 10.0.1.150 (Direct Routing), eth1 10.0.1.37]   ","version":null,"tagName":"h3"},{"title":"Deploy Nginx Application​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#deploy-nginx-application","content":" $ kubectl apply -f &quot;nginx.yaml&quot; $ kubectl get pods,svc NAME READY STATUS RESTARTS AGE pod/my-nginx-684dd4dcd4-gl9rm 1/1 Running 0 18s pod/my-nginx-684dd4dcd4-nk9mm 1/1 Running 0 18s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 172.20.0.1 &lt;none&gt; 443/TCP 33m service/my-nginx NodePort 172.20.80.220 &lt;none&gt; 80:32449/TCP 3s   ","version":null,"tagName":"h3"},{"title":"Cilium Validation​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#cilium-validation","content":" $ kubectl -n kube-system exec ds/cilium -- cilium service list Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) ID Frontend Service Type Backend 1 172.20.0.1:443 ClusterIP 1 =&gt; 10.0.1.15:443 (active) 2 =&gt; 10.0.2.226:443 (active) 2 172.20.208.197:443 ClusterIP 1 =&gt; 10.0.1.150:4244 (active) 3 172.20.22.66:80 ClusterIP 1 =&gt; 10.0.3.36:4245 (active) 4 172.20.141.67:80 ClusterIP 1 =&gt; 10.0.2.229:8081 (active) 5 172.20.0.10:53 ClusterIP 1 =&gt; 10.0.1.144:53 (active) 2 =&gt; 10.0.3.123:53 (active) 6 172.20.80.220:80 ClusterIP 1 =&gt; 10.0.1.216:80 (active) 2 =&gt; 10.0.3.39:80 (active) 7 10.0.1.150:32449 NodePort 1 =&gt; 10.0.1.216:80 (active) 2 =&gt; 10.0.3.39:80 (active) 8 10.0.1.37:32449 NodePort 1 =&gt; 10.0.1.216:80 (active) 2 =&gt; 10.0.3.39:80 (active) 9 0.0.0.0:32449 NodePort 1 =&gt; 10.0.1.216:80 (active) 2 =&gt; 10.0.3.39:80 (active)   From the output above, we can observe that Cilium eBPF kube-proxy replacement created the NodePort service for Nginx.  As the blog post is not intended to outline in depth how the kube-proxy replacement works, check out the link for further tests.  ","version":null,"tagName":"h3"},{"title":"Conclusions​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#conclusions","content":" We demonstrated an easy way of deploying Cilium CNI to an EKS cluster with the Sveltos ClusterProfile. The complete lifecycle of the CNI is now controlled by Sveltos and without external dependencies.  Take advantage of the Sveltos Templating and the Sveltos Event Framework capabilities to make every Kubernetes deployment and add-on easier!  ","version":null,"tagName":"h2"},{"title":"Contact​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#contact","content":" We are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to join us us.  ","version":null,"tagName":"h2"},{"title":"👏 Support this project​","type":1,"pageTitle":"Cilium on EKS with Sveltos","url":"/personal-blog/blog/cilium-eks-sveltos#-support-thisproject","content":" Every contribution counts! If you enjoyed this article, check out the Projectsveltos GitHub repo. You can star 🌟 the project if you find it helpful.  The GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.  Thanks for reading! ","version":null,"tagName":"h2"},{"title":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","type":0,"sectionRef":"#","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#introduction","content":" Welcome to the the first post of the brand new Kubernetes Troubleshooting Insights section! The series of blog posts will share helpful information and troubleshooting tips for issues that might appear in a Kubernetes environment. The posts are focused on real-life scenarios from either test, staging or production environments.  In today’s blog post, we’ll explore an issue with CoreDNS setup on RKE2 clusters. Cilium CNI with Hubble were enabled for this setup. Let’s jump right in!    ","version":null,"tagName":"h2"},{"title":"Environment Setup​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#environment-setup","content":" +-----------------+-------------------+--------------------------+----------------+ | Cluster Name | Type | Version | OS | +-----------------+-------------------+--------------------------+----------------+ | cluster01 | Managed Cluster | v1.28.14+rke2r1 | Ubuntu 22.04 | +-----------------+-------------------+--------------------------+----------------+ +-------------+---------------------+ | Deployment | Version | +-------------+---------------------+ | Cilium | v1.16.1 | +-------------+---------------------+   ","version":null,"tagName":"h2"},{"title":"Scenario​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#scenario","content":" DNS issues are wide and can be are caused by many reasons. Unfortunately, there is no one-size-fits-all approach when it comes to troubleshooting. In our scenario, we started with the virtual machines, ensured routing and DNS work and then moved to the Kubernetes layer. The blog post is an attempt to provide readers with troubleshooting ideas about DNS issues.  Background: We performed a major migration to a new instance with new DNS servers and domains. We had to test if everything worked with the new setup. Everything appeared fine apart from synchronising ArgoCD with internal Git repositories. An error from an internal Kubernetes IP address on port 53 appeared. Weird, right? We were confident the underlying virtual machines were using the correct DNS, and the configuration of the DHCP server was updated.  note The troubleshooting session was performed on an Ubuntu 22.04 environment. If another operating system is used, the troubleshooting methodology remains the same. The Linux commands will be different.  ","version":null,"tagName":"h2"},{"title":"Troubleshooting Steps​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#troubleshooting-steps","content":" ","version":null,"tagName":"h2"},{"title":"Step 1: Underlying Infrastructure​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#step-1-underlying-infrastructure","content":" The below steps were performed to double-check the underlying infrastructure.  DHCP Server (if used): Ensure the configuration points to the new DNS servers $ cat /etc/dhcp/dhcpd.conf Check the domain-name-servers configuration. Cluster Node: Perform an SSH connection to one of the cluster nodes Check the DNS configuration $ cat /etc/resolv.conf # Check the local DNS configuration $ sudo resolvectl status # Check the global and per-link DNS settings currently in effect From the command above, we would like to see how the Ethernet network interface on the virtual machine resolves domains. The resolvectl status command will reveal the use of the new DNS servers. Check routing $ ping test-site.example-domain.com # Check whether we can reach the custom domain $ ping 8.8.8.8 # Check whether we can reach the Internet If one of the commands above fail, this might be an indication that routing is broken, or something else is blocking traffic. If the commands above are successful, we continue with the next step and dive into the Kubernetes world.  ","version":null,"tagName":"h3"},{"title":"Step 2: Kubernetes Troubleshooting​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#step-2-kubernetes-troubleshooting","content":" Depending on the Kubernetes environment in place, identify how DNS queries are resolved from a Kubernetes cluster point of view. In our case, the RKE2 clusters use CoreDNS.  Even from a Kubernetes point of view, we will perform the well used ping and curl commands to see what is going on. For that reason, we will deploy the dns-utils pod to perform network calls.  Deploy the dns-utils pod to check DNS queries $ kubectl apply -f - &lt;&lt;EOF apiVersion: v1 kind: Pod metadata: name: dnsutils namespace: default spec: containers: - name: dnsutils image: registry.k8s.io/e2e-test-images/agnhost:2.39 imagePullPolicy: IfNotPresent restartPolicy: Always EOF Exec into the dnsutils pod - Perform ping and dig to well known website $ kubectl exec -it dnsutils -- /bin/sh / # ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes 64 bytes from 8.8.8.8: icmp_seq=0 ttl=119 time=41.526 ms / # dig SOA www.google.com ;; AUTHORITY SECTION: google.com. 30\tIN\tSOA\t&lt;YOUR DNS Server&gt;.google.com. dns-admin.google.com. 689372973 900 900 1800 60 If the above commands are successful, we know that routing and resolution works for well known websites. tip Learn more about SOA. Exec into the dnsutils pod - Perform curl and dig to the custom domain $ kubectl exec -it dnsutils -- /bin/sh / # curl -vvv example-domain.com * Could not resolve host: example-domain.com * Closing connection 0 curl: (6) Could not resolve host: example-domain.com / # dig SOA example-domain.com ... ;; AUTHORITY SECTION: example-domain.com.\t20\tIN\tSOA\tns.dns.example-domain.com. hostmaster.example-domain.com. 1729344685 7200 1800 86400 30 / # dig @YOUR DNS Server SOA example-domain.com ... ;; AUTHORITY SECTION: example-domain.com. 3600\tIN\tSOA\t&lt;YOUR DNS SERVER NAME&gt;.example-domain.com. mail.example-domain.com. 1729687405 604800 86400 2419200 604800 From the above, we can see that we can resolve the custom domain by defining our DNS server. But, when we do not define it the responsible entity to answer the DNS queries of the custom zone is set to the name ns.dns.. What is going on here? We would expect the DNS quetries to be resolved by the DNS serve defined in the virtual machines. note It is important to check the AUTHORITY SECTION of the dig command output.SOA ns.dns.example-domain.com. signifies this is an SOA record, which provides essential information about a DNS zone. The ns.dns.example-domain.com. defines the primary DNS for that domain. But why? We would expect to see one of the new DNS servers instead. Identify the CoreDNS deployment $ kubectl get deploy -n kube-system rke2-coredns-rke2-coredns 2/2 2 2 23h rke2-coredns-rke2-coredns-autoscaler 1/1 1 1 23h Check the CoreDNS ConfigMap $ kubectl get cm -n kube-system chart-content-rke2-coredns 1 23h rke2-coredns-rke2-coredns 1 23h rke2-coredns-rke2-coredns-autoscaler 1 23h $ kubectl get cm rke2-coredns-rke2-coredns -n kube-system -o jsonpath='{.data.Corefile}' .:53 { errors health { lameduck 5s } ready kubernetes example-domain.com cluster.local cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus 0.0.0.0:9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } Exec to the CoreDNS deployment and cat the /etc/resolv.conf $ kubectl exec -it deploy/rke2-coredns-rke2-coredns -n kube-system -- cat /etc/resolv.conf nameserver &lt;DNS01 IP&gt; nameserver &lt;DNS02 IP&gt; If the above output returns the expected nameservers, continue with the next step. Analyse the CoreDNS config The output kubernetes example-domain.com cluster.local cluster.local in-addr.arpa ip6.arpa indicates that CoreDNS is responsible for responsing to DNS queries of the specified zones including example-domain.com. Should CoreDNS be responsible, or this is just a misconfiguration?  In our case, the custom domain was included to the CoreDNS configuration by mistake. Responses to DNS queries related to the custom domain should be forwarded to the defined DNS server instead.  If this is the case for your environment, edit the ConfigMap and remove the custom domain from the configuration. Use the commands below.  $ kubectl patch cm rke2-coredns-rke2-coredns -n kube-system --type='json' -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/data/Corefile&quot;, &quot;value&quot;: &quot;.:53 {\\n errors \\n health {\\n lameduck 5s\\n }\\n ready \\n kubernetes cluster.local cluster.local in-addr.arpa ip6.arpa {\\n pods insecure\\n fallthrough in-addr.arpa ip6.arpa\\n ttl 30\\n }\\n prometheus 0.0.0.0:9153\\n forward . /etc/resolv.conf\\n cache 30\\n loop \\n reload \\n loadbalance \\n}&quot;}]' configmap/rke2-coredns-rke2-coredns patched $ kubectl get cm rke2-coredns-rke2-coredns -n kube-system -o jsonpath='{.data.Corefile}' # Ensure the custom domain is removed $ kubectl exec -it dnsutils -- /bin/sh / # curl &lt;domain&gt;:&lt;port&gt; # You should be able to resolved the domain now   ","version":null,"tagName":"h3"},{"title":"Optional: Kubernetes Troubleshoot with Cilium Hubble​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#optional-kubernetes-troubleshoot-with-cilium-hubble","content":" Another option to troubleshoot network issues is with Hubble. If it is available as part of your installation, you can exec into the Cilium daemonset and start using the Hubble CLI to observe traffic. For example, you can use something like the below.  $ kubectl exec -it ds/cilium -n kube-system -- /bin/sh # hubble observe --pod rke2-coredns-rke2-coredns-84b9cb946c-b7l9k --namespace kube-system --protocol UDP -f   The command above will display UDP packages between the dnsutils pod and CoreDNS. The Hubble cheat sheet can be found here.  ","version":null,"tagName":"h3"},{"title":"Optional: Kubernetes Troubleshoot with Netshoot Pod and TCPDump​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#optional-kubernetes-troubleshoot-with-netshoot-pod-and-tcpdump","content":" If we want to see what happens with the UDP packages when we perform a CURL request on a custom domain, it might be easier to instantiate a tcpdump using the netshoot pod. Follow the commands below.  $ kubectl run -it --rm debug --image=nicolaka/netshoot -- /bin/bash debug:~# tcpdump -i eth0 -n udp port 53   Once tcpdump is enabled, exec into the same pod and perform CURL requests.  $ kubectl exec -it dnsutils -n kube-system -- /bin/sh / # curl example-domain.com   ","version":null,"tagName":"h3"},{"title":"tcpdump Output​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#tcpdump-output","content":" tcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes 16:31:52.197604 IP 10.42.2.184.49109 &gt; 10.43.0.10.53: 59274+ [1au] A? example-domain.com.default.svc.cluster.local. (86) 16:31:52.197677 IP 10.42.2.184.49109 &gt; 10.43.0.10.53: 134+ [1au] AAAA? example-domain.com.default.svc.cluster.local. (86) 16:31:52.198333 IP 10.43.0.10.53 &gt; 10.42.2.184.49109: 59274 NXDomain*- 0/1/1 (179) 16:31:52.198553 IP 10.43.0.10.53 &gt; 10.42.2.184.49109: 134 NXDomain*- 0/1/1 (179)   The output above indicates that a client queries a DNS server, and the server responds that the domain does not exist. This would be a hint to check the CoreDNS configuration! :)  ","version":null,"tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#resources","content":" Debugging DNS: https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/  ","version":null,"tagName":"h2"},{"title":"✉️ Contact​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#️-contact","content":" If you have any questions, feel free to get in touch! You can use the Discussions option found here or reach out to me on any of the social media platforms provided. 😊  We look forward to hearing from you!  ","version":null,"tagName":"h2"},{"title":"Conclusions​","type":1,"pageTitle":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","url":"/personal-blog/blog/k8s-troubleshooting-insights-coredns#conclusions","content":" Is it DNS at the end? This is something you will have to find out! Hopefully, the post gave you some ideas to troubleshoot with confidence DNS issues in a Kubernetes environment.  It's a wrap for this post! 🎉 Thanks for reading! Stay tuned for more exciting updates! ","version":null,"tagName":"h2"},{"title":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","type":0,"sectionRef":"#","url":"/personal-blog/blog/sveltos-introduction-to-tiers","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#introduction","content":" In previous posts, we outlined how Sveltos allows Platform and tenant administrators to streamline Kubernetes applications and add-on deployments to a fleet of clusters. In today's blog post, we will take a step further and demonstrate how easy it is to target and update a subset of resources targeted by multiple configurations. By multiple configurations, we refer to the Sveltos ClusterProfile or Profile Custom Resource Definitions (CRDs). The demonstration focuses on day-2 operations as we provide a way to update and/or remove resources without affecting production operations.  This functionality is called tiers. Sveltos tiers provide a solution for managing the deployment priority when resources are targeted by multiple configurations. Tiers are easily integrated into existing ClusterProfile/Profile definitions alongside defining the deployment order control and straightforwardly override behaviour.  Today, we will cover the case of updating the Cilium CNI in a subnet of clusters with the label set to tier:zone2 without affecting the monitoring capabilities defined in the same ClusterProfile/Profile.    ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#lab-setup","content":" +-----------------+-------------------+--------------------------+ | Cluster Name | Type | Version | +-----------------+-------------------+--------------------------+ | mgmt | Mgmt Cluster | v1.28.9+rke2r1 | | prod-zone01 | Managed Cluster | v1.29.2+k3s1 | | prod-zone02 | Managed Cluster | v1.29.2+k3s1 | +-----------------+-------------------+--------------------------+ +-------------+---------------------+ | Deployment | Version | +-------------+---------------------+ | Cilium | v1.15.6, v1.16.1 | | sveltosctl | v0.37.0 | +-------------+---------------------+   ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#prerequisites","content":" To follow along, ensure the below are satisfied.  A management cluster with Sveltos installedkubectl installedsveltosctl installed  tip If you are unaware of installing Sveltos in a Kubernetes cluster, follow the instructions mentioned here.  ","version":null,"tagName":"h2"},{"title":"Step 1: Register Clusters with Sveltos​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#step-1-register-clusters-withsveltos","content":" For this demo, two Civo Kubernetes clusters are used. Once the clusters are ready, we can proceed with the Sveltos cluster registration. To do that, we will utilise sveltosctl. The sveltosctl can be downloaded here.  $ sveltosctl register cluster --namespace=&lt;namespace&gt; --cluster=&lt;cluster name&gt; \\ --kubeconfig=&lt;path to Sveltos file with Kubeconfig&gt; \\ --labels=key1=value1,key2=value2   ","version":null,"tagName":"h2"},{"title":"Example Registration​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#example-registration","content":" $ sveltosctl register cluster --namespace=civo --cluster=mesh01 \\ --kubeconfig=/home/test/prod-zone01.yaml \\ --labels=env=prod,tier=zone01   We will register the clusters with Sveltos on the mentioned namespace, and name, and will attach the cluster labels to perform different deployment versions.  note If the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.  ","version":null,"tagName":"h3"},{"title":"Validation​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#validation","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl get sveltoscluster -A --show-labels NAMESPACE NAME READY VERSION LABELS mgmt mgmt true v1.28.9+rke2r1 projectsveltos.io/k8s-version=v1.28.9,sveltos-agent=present prod prod-zone01 true v1.28.7+k3s1 env=prod,projectsveltos.io/k8s-version=v1.28.7,sveltos-agent=present,tier=zone01 prod prod-zone02 true v1.28.7+k3s1 env=prod,projectsveltos.io/k8s-version=v1.28.7,sveltos-agent=present,tier=zone02   tip Ensure the labels set to the managed clusters are correct. We will use them at a later step.  ","version":null,"tagName":"h3"},{"title":"Step 2: Deploy Cilium and Monitoring Capabilities​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#step-2-deploy-cilium-and-monitoring-capabilities","content":" As platform administrators, for every managed cluster, we want to have Cilium as our CNI and monitoring capabilities with Grafana, Prometheus and Loki. For that, we will use the Sveltos ClusterProfile Kubernetes resource and deploy the required deployments in clusters with the label set to env:prod.  ","version":null,"tagName":"h2"},{"title":"ClusterProfile - Cilium, Grafana, Prometheus​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#clusterprofile---cilium-grafana-prometheus","content":" --- apiVersion: config.projectsveltos.io/v1beta1 kind: ClusterProfile metadata: name: cluster-prod-initial-setup spec: clusterSelector: matchLabels: env: prod continueOnConflict: true helmCharts: - chartName: cilium/cilium chartVersion: 1.15.6 helmChartAction: Install releaseName: cilium releaseNamespace: kube-system repositoryName: cilium repositoryURL: https://helm.cilium.io/ - chartName: prometheus-community/kube-prometheus-stack chartVersion: v60.2.0 helmChartAction: Install releaseName: prometheus-community releaseNamespace: monitoring repositoryName: prometheus-community repositoryURL: https://prometheus-community.github.io/helm-charts values: | grafana: service: type: LoadBalancer prometheus-node-exporter: service: port: 9200 targetPort: 9200 validateHealths: - name: deployment-health featureID: Helm group: &quot;apps&quot; version: &quot;v1&quot; kind: &quot;Deployment&quot; namespace: monitoring script: | function evaluate() local hs = {healthy = false, message = &quot;Available replicas not match requested replicas&quot;} if obj.status and obj.status.availableReplicas ~= nil and obj.status.availableReplicas == obj.spec.replicas then hs.healthy = true end return hs end   We instruct Sveltos firstly to install Cilium as our CNI. Next, we deploy the Grafana and Prometheus stack and ensure the second deployment is in a healthy state using the validateHealths. Afterwards, we proceed with the Loki integration.  ","version":null,"tagName":"h3"},{"title":"ClusterProfile - Loki​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#clusterprofile---loki","content":" --- apiVersion: config.projectsveltos.io/v1beta1 kind: ClusterProfile metadata: name: loki-2102-prod spec: dependsOn: - cluster-prod-initial-setup clusterSelector: matchLabels: env: prod continueOnConflict: true helmCharts: - repositoryURL: https://grafana.github.io/helm-charts repositoryName: grafana chartName: grafana/loki-stack chartVersion: v2.10.2 releaseName: loki releaseNamespace: loki-stack helmChartAction: Install   note Ensure the dependsOn contains the correct name definition. In our example, it iscluster-prod-initial-setup.  ","version":null,"tagName":"h3"},{"title":"Deploy ClusterProfiles - Management Cluster​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#deploy-clusterprofiles---management-cluster","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl apply -f clusterprofile_prod_setup.yaml,clusterprofile_loki.yaml   ","version":null,"tagName":"h3"},{"title":"Validation - Management Cluster​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#validation---management-cluster","content":" $ ./sveltosctl show addons +------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+ | CLUSTER | RESOURCE TYPE | NAMESPACE | NAME | VERSION | TIME | PROFILES | +------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+ | prod/prod-zone01 | helm chart | kube-system | cilium | 1.15.6 | 2024-09-23 12:18:37 +0200 CEST | ClusterProfile/cluster-prod-initial-setup | | prod/prod-zone01 | helm chart | monitoring | prometheus-community | 60.2.0 | 2024-09-23 12:18:47 +0200 CEST | ClusterProfile/cluster-prod-initial-setup | | prod/prod-zone01 | helm chart | loki-stack | loki | 2.10.2 | 2024-09-23 12:20:38 +0200 CEST | ClusterProfile/loki-2102-prod | | prod/prod-zone02 | helm chart | kube-system | cilium | 1.15.6 | 2024-09-23 12:18:47 +0200 CEST | ClusterProfile/cluster-prod-initial-setup | | prod/prod-zone02 | helm chart | monitoring | prometheus-community | 60.2.0 | 2024-09-23 12:18:56 +0200 CEST | ClusterProfile/cluster-prod-initial-setup | | prod/prod-zone02 | helm chart | loki-stack | loki | 2.10.2 | 2024-09-23 12:20:47 +0200 CEST | ClusterProfile/loki-2102-prod | +------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+   ","version":null,"tagName":"h3"},{"title":"Validation - Managed Cluster​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#validation---managed-cluster","content":" $ kubectl get pods -n kube-system | grep -i cilium cilium-operator-579c6c96c4-47wsk 1/1 Running 0 10m cilium-operator-579c6c96c4-djk8s 1/1 Running 0 10m cilium-mwk7x 1/1 Running 0 10m cilium-x5z5v 1/1 Running 0 10m $ kubectl get ds/cilium -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' quay.io/cilium/cilium:v1.15.6@sha256:6aa840986a3a9722cd967ef63248d675a87add7e1704740902d5d3162f0c0def $ kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE prometheus-community-prometheus-node-exporter-78llc 1/1 Running 0 11m prometheus-community-prometheus-node-exporter-xhcf9 1/1 Running 0 11m prometheus-community-kube-operator-5746cfbb9-4p45x 1/1 Running 0 11m prometheus-community-kube-state-metrics-7fc779fc58-z4ktb 1/1 Running 0 11m alertmanager-prometheus-community-kube-alertmanager-0 2/2 Running 0 11m prometheus-prometheus-community-kube-prometheus-0 2/2 Running 0 11m prometheus-community-grafana-7494f9df89-pjqdd 3/3 Running 0 11m $ kubectl get pods -n loki-stack NAME READY STATUS RESTARTS AGE loki-promtail-ttdgx 1/1 Running 0 10m loki-promtail-jq5td 1/1 Running 0 10m loki-0 1/1 Running 0 10m   We installed Cilium, Grafana, Prometheus and Loki in our production clusters. Awesome! 🎉 Now, we will continue with the update of Cilium on a subnet of clusters only.  ","version":null,"tagName":"h3"},{"title":"Step 3: Update Cilium tier:zone02 Cluster​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#step-3-update-cilium-tier-cluster","content":" As mentioned, we would like to use the Sveltos tier feature to update Cilium only on the clusters with the label set to tier:zone02. The matching cluster in our example will be the prod-zone01 cluster.  Issue: Because we have a ClusterProfile that installed Cilium to the cluster prod-zone01, how can we instruct Sveltos to update this cluster and only the Cilium deployment?  To achieve this, we will create a new ClusterProfile with the user of tier. We will instruct Sveltos to take the new ClusterProfile with a lower tier value set and update Cilium CNI only on the matching clusters.  note The default tier value for every ClusterProfile/Profile is set to 100. If you set this to a lower value, Sveltos will take the lower value as a higher priority deployment.  ","version":null,"tagName":"h2"},{"title":"ClusterProfile - Update Cilium​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#clusterprofile---update-cilium","content":" --- apiVersion: config.projectsveltos.io/v1beta1 kind: ClusterProfile metadata: name: cilium-1161 spec: tier: 50 clusterSelector: matchLabels: tier: zone02 continueOnConflict: true helmCharts: - chartName: cilium/cilium chartVersion: 1.16.1 helmChartAction: Install releaseName: cilium releaseNamespace: kube-system repositoryName: cilium repositoryURL: https://helm.cilium.io/   ","version":null,"tagName":"h3"},{"title":"Validation - Management Cluster​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#validation---management-cluster-1","content":" $ ./sveltosctl show addons --cluster=prod-zone02 +------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+ | CLUSTER | RESOURCE TYPE | NAMESPACE | NAME | VERSION | TIME | PROFILES | +------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+ | prod/prod-zone02 | helm chart | monitoring | prometheus-community | 60.2.0 | 2024-09-23 12:18:56 +0200 CEST | ClusterProfile/cluster-prod-initial-setup | | prod/prod-zone02 | helm chart | loki-stack | loki | 2.10.2 | 2024-09-23 12:20:47 +0200 CEST | ClusterProfile/loki-2102-prod | | prod/prod-zone02 | helm chart | kube-system | cilium | 1.16.1 | 2024-09-23 12:32:27 +0200 CEST | ClusterProfile/cilium-1161 | +------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+   ","version":null,"tagName":"h3"},{"title":"Validation - Managed Cluster​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#validation---managed-cluster-1","content":" $ kubectl get pods -n kube-system | grep -i cilium cilium-operator-66dcfc4678-76j6m 1/1 Running 0 2m44s cilium-operator-66dcfc4678-6wvbr 1/1 Running 0 2m44s cilium-xnt7k 1/1 Running 0 2m44s cilium-envoy-rj8pj 1/1 Running 0 2m45s cilium-8fbzc 1/1 Running 0 2m44s cilium-envoy-hkhmc 1/1 Running 0 2m45s $ kubectl get ds/cilium -n kube-system -o jsonpath='{.spec.template.spec.containers[0].image}' quay.io/cilium/cilium:v1.16.1@sha256:0b4a3ab41a4760d86b7fc945b8783747ba27f29dac30dd434d94f2c9e3679f39   ","version":null,"tagName":"h3"},{"title":"Sveltos Tiers Benefits​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#sveltos-tiers-benefits","content":" Sveltos tiers allow seamless targeting 🎯 and updating 🔄 of different Kubernetes applications and add-ons in a subset of clusters. Now, we have a way to perform updates without headaches 😌 and be confident and full of control 🛠️ using the GitOps approach.  ","version":null,"tagName":"h2"},{"title":"Conclusions​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#conclusions","content":" In a few minutes ⏳, with minimal configuration effort and following the GitOps approach, we updated Cilium CNI in a subset of clusters painless! 🎉 Find more about the Sveltos tiers here.  ","version":null,"tagName":"h2"},{"title":"✉️ Contact​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#️-contact","content":" We are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to join us us.  ","version":null,"tagName":"h2"},{"title":"👏 Support this project​","type":1,"pageTitle":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","url":"/personal-blog/blog/sveltos-introduction-to-tiers#-support-thisproject","content":" Every contribution counts! If you enjoyed this article, check out the Projectsveltos GitHub repo. You can star 🌟 the project if you find it helpful.  The GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.  Thanks for reading! ","version":null,"tagName":"h2"},{"title":"Sveltos Templating: Cilium Cluster Mesh in One Run","type":0,"sectionRef":"#","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#introduction","content":" Have you ever wondered how to dynamically instantiate Kubernetes resources before deploying them to a cluster? What if I tell you there is an easy way to do it? Sveltos lets you define add-ons and applications using templates. Before deploying any resource down the managed clusters, Sveltos instantiates the templates using information gathered from the management cluster.  In a previous post, we outlined a step-by-step approach to forming a Cilium cluster mesh between two clusters. In today's post, we will demonstrate how the Sveltos templating is used to deploy a Cilium cluster mesh dynamically in one go.    ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#lab-setup","content":" +-----------------+-------------------+--------------------------+ | Cluster Name | Type | Version | +-----------------+-------------------+--------------------------+ | mgmt | Mgmt Cluster | v1.28.9+rke2r1 | | mesh01 | Managed Cluster | v1.29.2+k3s1 | | mesh02 | Managed Cluster | v1.29.2+k3s1 | +-----------------+-------------------+--------------------------+ +-------------+----------+ | Deployment | Version | +-------------+----------+ | Cilium | v1.15.6 | | sveltosctl | v0.32.0 | +-------------+----------+   ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#prerequisites","content":" To follow along, ensure the below are satisfied.  A management cluster with Sveltos installedkubectl installedsveltosctl installed  tip If you are unaware of how to install Sveltos in a Kubernetes cluster, follow the instructions mentioned here.  ","version":null,"tagName":"h2"},{"title":"Step 1: Register Clusters with Sveltos​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#step-1-register-clusters-withsveltos","content":" For this demonstration the Civo Kubernetes cluster offering was used. Once the clusters are ready, it is time to proceed with the Sveltos cluster registration. To do that, we will utilise sveltosctl and generate a new kubeconfig file.  $ sveltosctl register cluster --namespace=&lt;namespace&gt; --cluster=&lt;cluster name&gt; \\ --kubeconfig=&lt;path to Sveltos file with Kubeconfig&gt; \\ --labels=key=value   ","version":null,"tagName":"h2"},{"title":"Example - mesh01 registration​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#example---mesh01-registration","content":" $ sveltosctl register cluster --namespace=civo --cluster=mesh01 \\ --kubeconfig=/home/test/mesh01.yaml \\ --labels=cilium=zone01   We will register the clusters with Sveltos on the mentioned namespace, name, and will attach the cluster labels cilium=zone01 and cilium=zone02 respectively.  note If the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.  ","version":null,"tagName":"h3"},{"title":"Validation​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#validation","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl get sveltoscluster -A --show-labels NAMESPACE NAME READY VERSION LABELS civo mesh01 true v1.29.2+k3s1 cilium=zone01 civo mesh02 true v1.29.2+k3s1 cilium=zone02 mgmt mgmt true v1.28.9+rke2r1 sveltos-agent=present   ","version":null,"tagName":"h3"},{"title":"Step 2: Deploy Cilium Cluster Mesh ConfigMap​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#step-2-deploy-cilium-cluster-mesh-configmap","content":" Before we even start working with the Sveltos templating, we will deploy two ConfigMap resources that include the Cilium Cluster Mesh configuration.  ","version":null,"tagName":"h2"},{"title":"Example - mesh01 ConfigMap​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#example---mesh01-configmap","content":" apiVersion: v1 kind: ConfigMap metadata: name: cilium-config-mesh01 namespace: civo data: id: &quot;1&quot; k8sServiceHost: &quot;74.220.x.x&quot; k8sServicePort: &quot;6443&quot; nodePort: &quot;32379&quot; crt: &quot;Define the base64 ca.crt&quot; key: &quot;Define the base64 ca.key&quot; clusterPoolIPv4PodCIDRList: &quot;10.244.0.0/16&quot; peermeshname: &quot;mesh02&quot; peermeship: &quot;192.168.x.x&quot; peermeshport: &quot;32380&quot;   From the YAML definition, it is clear the resources we would like to deploy are in a key-value format. The ConfigMap for both clusters contains the required information to form a Cilium cluster mesh seamlessly.  ","version":null,"tagName":"h3"},{"title":"Deploy ConfigMap Management Cluster​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#deploy-configmap-management-cluster","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl apply -f config_mesh01.yaml,config_mesh02.yaml $ kubectl get cm -n civo NAME DATA AGE cilium-config-mesh01 10 9s cilium-config-mesh02 10 5s kube-root-ca.crt 1 23m   note As the Kubernetes managed clusters are registered in the civo namespace, the ConfigMaps are defined there.  ","version":null,"tagName":"h3"},{"title":"Step 3: Deploy Sveltos ClusterProfile​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#step-3-deploy-sveltos-clusterprofile","content":" We will create a Sveltos ClusterProfile to install Cilium as a CNI alongside the Hubble UI and Cilium cluster mesh by instantiating the configuration from the information located in the ConfigMap created in previous step.  ","version":null,"tagName":"h2"},{"title":"Example - mesh01 ClusterProfile​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#example---mesh01-clusterprofile","content":" apiVersion: config.projectsveltos.io/v1beta1 kind: ClusterProfile metadata: name: cilium-1156-zone01 spec: clusterSelector: matchLabels: cilium: zone01 templateResourceRefs: - resource: apiVersion: v1 kind: ConfigMap name: cilium-config-{{ .Cluster.metadata.name }} identifier: CiliumConfig helmCharts: - chartName: cilium/cilium chartVersion: 1.15.6 helmChartAction: Install releaseName: cilium releaseNamespace: kube-system repositoryName: cilium repositoryURL: https://helm.cilium.io/ values: | tls: ca: cert: {{ (getResource &quot;CiliumConfig&quot;).data.crt }} key: {{ (getResource &quot;CiliumConfig&quot;).data.key }} cluster: id: {{ (getResource &quot;CiliumConfig&quot;).data.id }} name: {{ .Cluster.metadata.name }} clustermesh: apiserver: replicas: 2 service: type: NodePort nodePort: {{ (getResource &quot;CiliumConfig&quot;).data.nodePort }} tls: authMode: cluster server: extraDnsNames: - &quot;{{ .Cluster.metadata.name }}.mesh.cilium.io&quot; config: clusters: - address: &quot;&quot; ips: - {{ (getResource &quot;CiliumConfig&quot;).data.peermeship }} # The Node IP of the available nodes or a resolvable hostname name: {{ (getResource &quot;CiliumConfig&quot;).data.peermeshname }} port: {{ (getResource &quot;CiliumConfig&quot;).data.peermeshport }} enabled: true domain: &quot;mesh.cilium.io&quot; useAPIServer: true # This is required for the Cluster Mesh setup kubeProxyReplacement: true k8sServiceHost: {{ (getResource &quot;CiliumConfig&quot;).data.k8sServiceHost }} k8sServicePort: {{ (getResource &quot;CiliumConfig&quot;).data.k8sServicePort }} hubble: enabled: true peerService: clusterDomain: cluster.local relay: enabled: true tls: auto: certValidityDuration: 1095 enabled: true method: helm ui: enabled: true nodeinit: enabled: true ipam: mode: cluster-pool operator: clusterPoolIPv4MaskSize: &quot;24&quot; clusterPoolIPv4PodCIDRList: - {{ (getResource &quot;CiliumConfig&quot;).data.clusterPoolIPv4PodCIDRList }} nodePort: enabled: true debug: enabled: true   The ClusterProfile will get deployed to the managed clusters with the label set to cilium:zone01. Once a cluster is found, we instruct Sveltos to use the templateResourceRefs capability and use the details found in the ConfgiMap with the name set to cilium-config-{{ .Cluster.metadata.name }}. This will match the cilium-config-mesh01 and cilium-config-mesh02 ConfigMap. Then, we instruct Sveltos to install the Cilium Helm chart v1.15.6.  For the Cilium Helm chart instantiation, we go through the ConfigMap and populate the values based on the key definition.  tip We deploy a Cilium cluster mesh with a common TLS certificate and a NodePort service. For the production environment, it is recommended to deploy a LoadBalancer setup. The above template can be used by updating the clustermesh.service.type=LoadBalancer and setting the standard service Port.  ","version":null,"tagName":"h3"},{"title":"Deploy ClusterProfile Management Cluster​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#deploy-clusterprofile-management-cluster","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl apply -f clusterprofile_mesh01.yaml,clusterprofile_mesh02.yaml   ","version":null,"tagName":"h3"},{"title":"Step 4: Validate Results​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#step-4-validate-results","content":" ","version":null,"tagName":"h2"},{"title":"Validation - sveltosctl​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#validation---sveltosctl","content":" $ ./sveltosctl show addons +-------------+---------------+-------------+--------+---------+--------------------------------+-----------------------------------+ | CLUSTER | RESOURCE TYPE | NAMESPACE | NAME | VERSION | TIME | PROFILES | +-------------+---------------+-------------+--------+---------+--------------------------------+-----------------------------------+ | civo/mesh01 | helm chart | kube-system | cilium | 1.15.6 | 2024-08-10 22:38:33 +0200 CEST | ClusterProfile/cilium-1156-zone01 | | civo/mesh02 | helm chart | kube-system | cilium | 1.15.6 | 2024-08-10 22:38:55 +0200 CEST | ClusterProfile/cilium-1156-zone02 | +-------------+---------------+-------------+--------+---------+--------------------------------+-----------------------------------+   ","version":null,"tagName":"h3"},{"title":"Validation - mesh01​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#validation---mesh01","content":" $ kubectl exec -it -n kube-system cilium-888jc -c cilium-agent -- cilium-dbg troubleshoot clustermesh mesh02 Found 1 remote cluster configurations Troubleshooting filtered subset of clusters: mesh02 Remote cluster &quot;mesh02&quot;: 📄 Configuration path: /var/lib/cilium/clustermesh/mesh02 🔌 Endpoints: - https://mesh02.mesh.cilium.io:32380 ✅ Hostname resolved to: 192.168.x.x ✅ TCP connection successfully established to 192.168.x.x:32380 ✅ TLS connection successfully established to 192.168.x.x:32380 ℹ️ Negotiated TLS version: TLS 1.3, ciphersuite TLS_AES_128_GCM_SHA256 ℹ️ Etcd server version: 3.5.14 🔑 Digital certificates: ✅ TLS Root CA certificates: - Serial number: 2c:b4:43:9c:fb:82:62:4f:55:0f:eb:5e:a4:fe:af:5e:14:95:18:74 Subject: CN=Cilium LAB CA Issuer: CN=Cilium LAB CA Validity: Not before: 2024-04-25 14:04:31 +0000 UTC Not after: 2034-04-23 14:04:31 +0000 UTC ✅ TLS client certificates: - Serial number: 13:8f:38:4e:e9:08:bb:81:93:20:ff:30:33:ed:fb:13 Subject: CN=remote-mesh01 Issuer: CN=Cilium LAB CA Validity: Not before: 2024-08-10 20:53:31 +0000 UTC Not after: 2027-08-10 20:53:31 +0000 UTC ⚙️ Etcd client: ✅ Etcd connection successfully established ℹ️ Etcd cluster ID: 820847063f6cabce   ","version":null,"tagName":"h3"},{"title":"Validation - mesh02​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#validation---mesh02","content":" $ kubectl exec -it -n kube-system cilium-94ddz -c cilium-agent -- cilium-dbg troubleshoot clustermesh mesh01 Found 1 remote cluster configurations Troubleshooting filtered subset of clusters: mesh01 Remote cluster &quot;mesh01&quot;: 📄 Configuration path: /var/lib/cilium/clustermesh/mesh01 🔌 Endpoints: - https://mesh01.mesh.cilium.io:32379 ✅ Hostname resolved to: 192.168.x.x ✅ TCP connection successfully established to 192.168.x.x:32379 ✅ TLS connection successfully established to 192.168.x.x:32379 ℹ️ Negotiated TLS version: TLS 1.3, ciphersuite TLS_AES_128_GCM_SHA256 ℹ️ Etcd server version: 3.5.14 🔑 Digital certificates: ✅ TLS Root CA certificates: - Serial number: 2c:b4:43:9c:fb:82:62:4f:55:0f:eb:5e:a4:fe:af:5e:14:95:18:74 Subject: CN=Cilium LAB CA Issuer: CN=Cilium LAB CA Validity: Not before: 2024-04-25 14:04:31 +0000 UTC Not after: 2034-04-23 14:04:31 +0000 UTC ✅ TLS client certificates: - Serial number: 94:02:e1:4a:b8:74:4c:d7:62:af:c1:d8:19:a8:3b:8f Subject: CN=remote-mesh02 Issuer: CN=Cilium LAB CA Validity: Not before: 2024-08-10 20:59:07 +0000 UTC Not after: 2027-08-10 20:59:07 +0000 UTC ⚙️ Etcd client: ✅ Etcd connection successfully established ℹ️ Etcd cluster ID: 21f7360bef94b707   Cilium cluster mesh is deployed in one run without the need for an additional Infrastructure as Code (IaC) tool or additional rendering!    ","version":null,"tagName":"h3"},{"title":"Sveltos Templating Benefits​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#sveltos-templating-benefits","content":" Sveltos templating enables users to utilise the same add-on configuration across different clusters while allowing variations like different add-on configuration values.  Sveltos lets users define add-ons and applications in a reusable way. We can deploy the definitions across multiple clusters with minor adjustments. The approach saves time, effort, and headaches, especially in large-scale environments.  ","version":null,"tagName":"h2"},{"title":"Conclusions​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#conclusions","content":" In a couple of minutes and with a minimal configuration effort we formed a cluster mesh between two Kubernetes clusters. This is the power of Sveltos templating.  If you want to explore a step-by-step guide on how to setup a Cilium cluster mesh, have a look at my previous blog post here.  ","version":null,"tagName":"h2"},{"title":"✉️ Contact​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#️-contact","content":" We are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to join us us.  ","version":null,"tagName":"h2"},{"title":"👏 Support this project​","type":1,"pageTitle":"Sveltos Templating: Cilium Cluster Mesh in One Run","url":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh#-support-thisproject","content":" Every contribution counts! If you enjoyed this article, check out the Projectsveltos GitHub repo. You can star 🌟 the project if you find it helpful.  The GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.  Thanks for reading! ","version":null,"tagName":"h2"},{"title":"OpenTofu: RKE2 Cluster with Cilium on Azure","type":0,"sectionRef":"#","url":"/personal-blog/blog/opentofu-rke2-cilium-azure","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#introduction","content":" In a previous post, we covered how to create an RKE2 cluster on Azure Cloud using the cloud-free credits from the Rancher UI. As this is a convenient approach to get started with Rancher, in today's post, we will demonstrate how to use OpenTofu to automate the deployment.  OpenTofu is a fork of Terraform. It is an open-source project, community-driven, and managed by the Linux Foundation. If you want to get familiar with what OpenTofu is and how to get started, check out the link here.  Additionally, we will demonstrate how easy it is to customise the Cilium configuration and enable kube-vip for LoadBalancer services from the HCL (HashiCorp Configuration Language) definition.    ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#lab-setup","content":" +-----------------------------+------------------+----------------------+ | Cluster Name | Type | Version | +-----------------------------+------------------+----------------------+ | Rancher | k3s cluster | v1.28.7+k3s1 | | Downstream RKE2 cluster | RKE2 | v1.28.11+rke2r1 | +-----------------------------+------------------+----------------------+ +-------------------+----------+ | Deployment | Version | +-------------------+----------+ | Cilium | 1.15.500 | | OpenTofu | v1.8.1 | +-------------------+----------+   ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#prerequisites","content":" ","version":null,"tagName":"h2"},{"title":"Rancher Server​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#rancher-server","content":" We do not concentrate on installing Rancher. If you are unsure how to install Rancher, take a look at the official documentation here or go through the guide I created a couple of weeks back found here.  ","version":null,"tagName":"h3"},{"title":"Azure Free Credits​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#azure-free-credits","content":" For this demonstration, we will use the Azure free credits offering. The approach taken allows readers to understand how to set up the Azure cloud environment to perform RKE2 deployments with Rancher without spending money outside the free-credits offering.  Ensure the below are satisfied.  Helm CLI installed (Optional Step)kubectl installed  ","version":null,"tagName":"h3"},{"title":"Install OpenTofu​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#install-opentofu","content":" There is a wide variety of options provided to install OpenTofu. To follow along, check out the link and install OpenTofu.  Validation​  $ tofu version OpenTofu v1.8.1 on darwin_arm64   ","version":null,"tagName":"h3"},{"title":"Step 0: Pre-work​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#step-0-pre-work","content":" ","version":null,"tagName":"h2"},{"title":"Step 0.1: Familiarise with OpenTofu Registry​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#step-01-familiarise-with-opentofu-registry","content":" As with the Terraform registry, the OpenTofu registry is a centralised service for distributing and managing providers/modules. Users can share, discover, and consume reusable infrastructure modules and providers.  A list of the available providers/modules is located here.  The rancher2 provider is supported by OpenTofu. The details can be found here.  ","version":null,"tagName":"h3"},{"title":"Step 0.2: Familiarise with Rancher2 Provider​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#step-02-familiarise-with-rancher2-provider","content":" Before we even begin with the actual coding, it is a nice opportunity to familiarise with the Rancher2 provider.    tip Check out the example sections of the resources available and the supported Cloud providers.  warning Be mindful this is an alpha preview of the OpenTofu Registry UI. If you encounter any issues, report them here.  ","version":null,"tagName":"h3"},{"title":"Step 0.3: Choose Integrated Development Environment (IDE)​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#step-03-choose-integrated-development-environment-ide","content":" As with any other project, we will use Git to store our code in a central location and Visual Studio Code to perform the coding. Choose your favourite source control system and IDE, and dive into the next sections! 🚀  ","version":null,"tagName":"h3"},{"title":"GitHub Repo​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#github-repo","content":" The showcase repository is available here.  ","version":null,"tagName":"h2"},{"title":"Outline Project Structure​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#outline-project-structure","content":" Like with any Terraform project, we will create several .tf files to store the Infrastructure as Code (IaC) definitions. For best practices, have a look at the link.  In your favourite IDE, create a new project and create the below file structure.  ","version":null,"tagName":"h2"},{"title":"File structure​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#file-structure","content":" main.tf: Contains the resource blocks that define the resources to be created in the Azure cloudvariables.tf: Contains the variable declaration used in the resource blocksproviders.tf: Contains the required providers used in the resource blocksdata.tf: Contains several data retrieved from the outside and used it through the resource creationoutput.tf: Contains the output that needs to be generated on successful completion of the OpenTofu plan/apply*.tfvars: Contains the default values of the specified variables  ","version":null,"tagName":"h3"},{"title":"providers.tf​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#providerstf","content":" The providers.tf file holds the required providers that will be used for the creation of the relevant resources. OpenTofu configurations must declare which providers they require so that OpenTofu can install and use them.  terraform { required_version = &quot;~&gt; 1.8.1&quot; required_providers { rancher2 = { source = &quot;opentofu/rancher2&quot; version = &quot;4.1.0&quot; } local = { source = &quot;opentofu/local&quot; version = &quot;2.5.1&quot; } http = { source = &quot;opentofu/http&quot; version = &quot;3.4.4&quot; } } } provider &quot;rancher2&quot; { api_url = var.rancher2_api_url token_key = var.rancher2_token_key }   tip It is a good practice to avoid specifying sensitive data in the variables.tf file. The providers.tf file expects the rancher2_api_url and rancher2_token_key variables. Following the best practices, we can have a file that exports the required variable name and value. From a terminal window, we set the source pointing to the file before performing any IaC actions.  ","version":null,"tagName":"h2"},{"title":"data.tf​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#datatf","content":" The data.tf file holds the code to download relevant information about the kube-vip installation. The information will be used later on in the main.tf file while defining the RKE2 cluster configuration.  # Download the kube-vip required RBAC manifest data &quot;http&quot; &quot;kube_vip_rbac&quot; { url = &quot;https://kube-vip.io/manifests/rbac.yaml&quot; } # Download a specific kube-vip version data &quot;http&quot; &quot;kube_vip_version&quot; { method = &quot;GET&quot; url = &quot;https://api.github.com/repos/kube-vip/kube-vip/releases#v0.8.2&quot; } # Download the kube-vip-cloud-provider required manifest data &quot;http&quot; &quot;kube_vip_cloud_provider&quot; { url = &quot;https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml&quot; } ...   ","version":null,"tagName":"h2"},{"title":"output.tf​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#outputtf","content":" In the file, we can specify anything we want based on the use case at hand. For this demonstration, we keep it simple. We would display only the RKE2 cluster-name and cluster-id.  # Display the RKE2 Cluster Name output &quot;rke2_cluster_name&quot; { value = rancher2_cluster_v2.rke2.name } # Display the RKE2 Cluster ID output &quot;rancher_cluster_id&quot; { value = data.rancher2_project.system.cluster_id } ...   ","version":null,"tagName":"h2"},{"title":"main.tf​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#maintf","content":" The file contains the logic for creating virtual machines and installing RKE2 on top. We will break the main.tf file into smaller pieces and try to go through them in more detail.  ","version":null,"tagName":"h2"},{"title":"Define the Azure Cloud Credentials​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#define-the-azure-cloud-credentials","content":" It is a requirement to have valid Azure cloud credentials before proceeding with the RKE2 installation. If you are unsure how to get the below variable details, have a look at my previous post here.  # Create the Azure Cloud Credentials in Rancher resource &quot;rancher2_cloud_credential&quot; &quot;azure_creds&quot; { name = &quot;Azure Credentials&quot; azure_credential_config { client_id = var.azure_env.az_client_id client_secret = var.azure_env.az_client_secret subscription_id = var.azure_env.az_subscription_id } }   ","version":null,"tagName":"h3"},{"title":"Define the Machine Configuration​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#define-the-machine-configuration","content":" The below resource will create the required virtual machines for the RKE2 cluster. Here, we define two types of nodes, the controller and the worker node. They could have the same or different hardware specifications based on the use case scenario that needs to be covered.  # Create the different nodes for RKE2 (controller and worker node) resource &quot;rancher2_machine_config_v2&quot; &quot;nodes&quot; { for_each = var.node generate_name = each.value.name azure_config { disk_size = each.value.agent_disk image = each.value.image location = each.value.location managed_disks = true open_port = each.value.open_port private_address_only = false resource_group = each.value.resource_group storage_type = each.value.storage_type size = each.value.agent_type } }   ","version":null,"tagName":"h3"},{"title":"Define the RKE2 Condifugration​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#define-the-rke2-condifugration","content":" # RKE2 configuration resource &quot;rancher2_cluster_v2&quot; &quot;rke2&quot; { annotations = var.rancher_env.cluster_annotations kubernetes_version = var.rancher_env.rke2_version labels = var.rancher_env.cluster_labels enable_network_policy = var.rancher_env.network_policy # Option to enable or disable Project Network Isolation. name = var.rancher_env.cluster_id rke_config { # You can create a Terraform template and polulate the values of the file based on the variables defined below additional_manifest = templatefile(&quot;${path.module}/files/kube-vip-daemonset.tfmpl&quot;, { int_name = var.kube_vip.int_name kube_vip_rbac = data.http.kube_vip_rbac.response_body kube_vip_version = jsondecode(data.http.kube_vip_version.response_body)[0][&quot;tag_name&quot;] kube_vip_address = var.kube_vip.kube_vip_address kube_vip_pool = var.kube_vip.kube_vip_pool kube_vip_cloud_provider = data.http.kube_vip_cloud_provider.response_body }) # Define the Helm chart values for the Cilium installation chart_values = &lt;&lt;-EOF # Have a look at https://github.com/cilium/cilium/blob/main/install/kubernetes/cilium/values.yaml to include additional custom values rke2-cilium: k8sServiceHost: 127.0.0.1 k8sServicePort: 6443 kubeProxyReplacement: true # Enable Cilium with Kube-Proxy replacement on EOF # Define the Rancher global settings for the whole cluster machine_global_config = &lt;&lt;EOF cni: &quot;cilium&quot; cluster-cidr: ${var.rke_cluster_cidr} service-cidr: ${var.rke_service_cidr} disable-kube-proxy: true EOF # Sepcify the role of each node based on the name of the node dynamic &quot;machine_pools&quot; { for_each = var.node content { cloud_credential_secret_name = rancher2_cloud_credential.azure_creds.id control_plane_role = machine_pools.key == &quot;controller&quot; ? true : false etcd_role = machine_pools.key == &quot;controller&quot; ? true : false name = machine_pools.value.name quantity = machine_pools.value.quantity worker_role = machine_pools.key != &quot;controller&quot; ? true : false machine_config { kind = rancher2_machine_config_v2.nodes[machine_pools.key].kind name = rancher2_machine_config_v2.nodes[machine_pools.key].name } } } machine_selector_config { config = null } } ...   ","version":null,"tagName":"h3"},{"title":"variables.tf​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#variablestf","content":" Outline how the variables used in the main.tf file should look like. If required, perform additional validations to the code.  variable &quot;azure_env&quot; { description = &quot;Azure required details&quot; type = object({ az_client_id = string az_client_secret = string az_subscription_id = string }) } variable &quot;kube_vip&quot; { description = &quot;kube-vip basic settings&quot; type = object({ int_name = string kube_vip_address = string kube_vip_pool = string }) } variable &quot;node&quot; { description = &quot;Two RKE2 nodes to be configured&quot; type = object({ controller = object({ name = string agent_disk = optional(number) image = optional(string) location = optional(string) open_port = optional(list(string)) resource_group = optional(string) storage_type = optional(string) agent_type = optional(string) quantity = number }) worker = object({ name = string agent_disk = optional(number) image = optional(string) location = optional(string) open_port = optional(list(string)) resource_group = optional(string) storage_type = optional(string) agent_type = optional(string) quantity = number }) }) } variable &quot;rancher2_api_url&quot; { description = &quot;URL to Rancher Server API&quot; type = string } variable &quot;rancher2_token_key&quot; { description = &quot;Rancher API Token key&quot; type = string } ...   ","version":null,"tagName":"h2"},{"title":"terraform.tfvars​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#terraformtfvars","content":" The file holds the input for the resource creation. Depending on how the variables.tf file looks like, we should set a similar structure to define the variables initialisation.  kube_vip = { int_name = &quot;eth0&quot; kube_vip_address = &quot;x.x.x.x&quot; kube_vip_pool = &quot;x.x.x.x-x.x.x.x&quot; } node = { controller = { name = &quot;controller&quot;, quantity = 1, agent_disk = 30, image = &quot;canonical:UbuntuServer:18.04-LTS:latest&quot;, location = &quot;westus&quot;, resource_group = &quot;rancher-rg&quot;, storage_type = &quot;Standard_LRS&quot;, agent_type = &quot;Standard_D2_v2&quot; }, worker = { name = &quot;worker&quot;, quantity = 1, agent_disk = 30, image = &quot;canonical:UbuntuServer:18.04-LTS:latest&quot;, location = &quot;westus&quot;, resource_group = &quot;rancher-rg&quot;, storage_type = &quot;Standard_LRS&quot;, agent_type = &quot;Standard_D2_v2&quot; } } rancher_env = { cluster_annotations = { &quot;rke2&quot; = &quot;demo&quot; } cluster_labels = { &quot;rke2&quot; = &quot;azure-demo&quot; } rke2_version = &quot;v1.28.11+rke2r1&quot; cluster_id = &quot;eleni-azure-01&quot; network_policy = &quot;false&quot; } rke_cluster_cidr = &quot;10.42.0.0/16&quot; rke_service_cidr = &quot;10.43.0.0/16&quot;   note The kube-vip interface name defined in the file represents the network interface from the virtual machines created in the Azure Cloud environment.  tip The node definition will allow you to create an RKE2 cluster based on the free-credits subscription. If the above are changed, the deployment might fail due to subscription limitations.  ","version":null,"tagName":"h3"},{"title":"Execution​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#execution","content":" To plan and apply the resources, use the below commands.   $ tofu init $ tofu plan $ tofu apply   note When performing the tofu init command, I received the below warning. - Installing opentofu/rancher2 v4.1.0... - Installed opentofu/rancher2 v4.1.0. Signature validation was skipped due to the registry not containing GPG keys for this provider I raised a GitHub issue with the Terraform Rancher2 Provider.  tip Check out the .terraform/providers/registry.opentofu.org directory with the providers sourced from the OpenTofu registry.  The above will first create the Azure Cloud Credentials in the Rancher instance, then continue with the RKE2 cluster creation. The tofu apply command might take up to 10 min. Just wait for it to complete.  ","version":null,"tagName":"h2"},{"title":"Validation​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#validation-1","content":" If the tofu apply command completes successfully, we should have a cluster with two nodes. One controller and one worker node in the westus region.    $ kubectk get nodes NAME STATUS ROLES AGE VERSION eleni-azure-01-controller-49abc099-ftvnv Ready control-plane,etcd,master 11m v1.28.11+rke2r1 eleni-azure-01-worker-87b90346-swd64 Ready worker 7m59s v1.28.11+rke2r1 $ kubectk get pods -n kube-system NAME READY STATUS RESTARTS AGE cilium-5rfh4 1/1 Running 0 11m cilium-operator-6bd79b68b5-ch979 1/1 Running 0 11m cilium-vmt9d 1/1 Running 0 8m8s cloud-controller-manager-eleni-azure-01-controller-49abc099-ftvnv 1/1 Running 0 11m etcd-eleni-azure-01-controller-49abc099-ftvnv 1/1 Running 0 11m helm-install-rke2-cilium-kqmkc 0/1 Completed 0 11m helm-install-rke2-coredns-m5f8f 0/1 Completed 0 11m helm-install-rke2-ingress-nginx-vzdps 0/1 Completed 0 11m helm-install-rke2-metrics-server-5t4sj 0/1 Completed 0 11m helm-install-rke2-snapshot-controller-crd-jvdtd 0/1 Completed 0 11m helm-install-rke2-snapshot-controller-zpkhv 0/1 Completed 0 11m helm-install-rke2-snapshot-validation-webhook-6qlpx 0/1 Completed 0 11m kube-apiserver-eleni-azure-01-controller-49abc099-ftvnv 1/1 Running 0 11m kube-controller-manager-eleni-azure-01-controller-49abc099-ftvnv 1/1 Running 0 11m kube-scheduler-eleni-azure-01-controller-49abc099-ftvnv 1/1 Running 0 11m kube-vip-5vlxw 1/1 Running 0 11m kube-vip-cloud-provider-85fd9b9cf7-n24fd 1/1 Running 0 11m rke2-coredns-rke2-coredns-84b9cb946c-5wch4 1/1 Running 0 11m rke2-coredns-rke2-coredns-84b9cb946c-zfkm5 1/1 Running 0 8m5s rke2-coredns-rke2-coredns-autoscaler-b49765765-4gkwf 1/1 Running 0 11m rke2-ingress-nginx-controller-hljpx 1/1 Running 0 6m15s rke2-metrics-server-655477f655-v2j6g 1/1 Running 0 6m38s rke2-snapshot-controller-59cc9cd8f4-66942 1/1 Running 0 6m39s rke2-snapshot-validation-webhook-54c5989b65-zqxgz 1/1 Running 0 6m38s   ","version":null,"tagName":"h2"},{"title":"Delete Resources​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#delete-resources","content":" It is very easy to delete the resources created, simply perform the tofu destroy and confirm the action. The deletion of the resources will take up to 2 minutes.  ","version":null,"tagName":"h2"},{"title":"✉️ Contact​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#️-contact","content":" If you have any questions, feel free to get in touch! You can use the Discussions option found here or reach out to me on any of the social media platforms provided. 😊  We look forward to hearing from you!  ","version":null,"tagName":"h2"},{"title":"Conclusions​","type":1,"pageTitle":"OpenTofu: RKE2 Cluster with Cilium on Azure","url":"/personal-blog/blog/opentofu-rke2-cilium-azure#conclusions","content":" This is it! Automate the creation of RKE2 clusters in Azure with OpenTofu! 🎉  It's a wrap for this post! 🎉 Thanks for reading! Stay tuned for more exciting updates! ","version":null,"tagName":"h2"},{"title":"Cilium Cluster Mesh on RKE2","type":0,"sectionRef":"#","url":"/personal-blog/blog/cilium-cluster-mesh-rke2","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#introduction","content":" After spending some time working with the on-prem RKE2 lab setup, I came to notice a couple of issues while forming in an automated fashion the Cilium cluster mesh between on-prem clusters.  In today's post, we will go through the step-by-step process of forming a Cilium Cluster Mesh and explain any issues that might have arisen by following the GitOps approach. The cilium CLI will not be required. The deployment will be performed primarily via Helm and kubectl.  Additionally, we will use the shared CA (Certificate Authority) approach as this is a convenient way to form a cluster mesh in an automated fashion and also the best practise for the Hubble Relay setup. The approach will enable mTLS across clusters.  ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#lab-setup","content":" +-----------------+----------------------+----------------------+ | Cluster Name | Type | Version | +-----------------+----------------------+----------------------+ | mesh01 | RKE2 managed cluster | RKE2 v1.27.14+rke2r1 | | mesh02 | RKE2 managed cluster | RKE2 v1.27.14+rke2r1 | +-----------------+----------------------+----------------------+ +-------------------+----------+ | Deployment | Version | +-------------------+----------+ | Rancher2 Provider | 4.2.0 | | Cilium | 1.15.500 | +-------------------+----------+   ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#prerequisites","content":" ","version":null,"tagName":"h2"},{"title":"Infrastructure​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#infrastructure","content":" For this demonstration, we assume readers have at least two RKE2 clusters up and running. In our case, to create an RKE2 cluster on-prem we used the Rancher2 Terraform provider. The provider allows users to create different resources across different platforms alongside defining information for the RKE2 deployment like IP Address handling, and CNI (Container Network Interface) custom configuration.  ","version":null,"tagName":"h3"},{"title":"Cilium Cluster Mesh​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#cilium-cluster-mesh","content":" The Cluster Name and the Cluster ID must be unique.The Pods and the Services CIDR ranges must be unique across all the Kubernetes Clusters. The pods need to communicate over a unique IP address. See the IP address schema table above.Node CIDRs must be unique. The Nodes to have IP connectivity.The Cilium pods must connect to the ClusterMesh API Server service exposed on every Kubernetes cluster.  ","version":null,"tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#resources","content":" Ensure the below are satisfied.  Helm CLI installedkubectl installed  ","version":null,"tagName":"h3"},{"title":"Step 0: RKE2 Terraform Provider​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-0-rke2-terraform-provider","content":" The below snippet is an example configuration on how to deploy an RKE2 cluster via the Rancher2 Provider.   # RKE2 configuration resource &quot;rancher2_cluster_v2&quot; &quot;rke2&quot; { # Define basic cluster details like labels and annotations annotations = var.rancher_env.cluster_annotations kubernetes_version = var.rancher_env.rke2_version labels = var.rancher_env.cluster_labels enable_network_policy = var.rancher_env.network_policy # Option to enable or disable Project Network Isolation. name = var.rancher_env.cluster_id # Define the Cilium Configuration for the cluster chart_values = &lt;&lt;-EOF rke2-cilium: k8sServiceHost: 127.0.0.1 k8sServicePort: 6443 kubeProxyReplacement: true # Prepare the deployment for kube-proxy replacement operator: replicas: 1 hubble: # Enable Hubble for observability enabled: true peerService: clusterDomain: cluster.local relay: enabled: true tls: auto: certValidityDuration: 1095 enabled: true method: helm ui: enabled: true EOF # Apply machine global settings for the clusters machine_global_config = &lt;&lt;EOF cni: &quot;cilium&quot; # Enable Cilium CNI for every cluster cluster-cidr: ${var.rke_cluster_cidr} service-cidr: ${var.rke_service_cidr} disable-kube-proxy: true # Disable kube-proxy etcd-expose-metrics: false # Do not expose the etcd metrics EOF # Start building the controller and workder nodes dynamically dynamic &quot;machine_pools&quot; { for_each = var.node content { cloud_credential_secret_name = data.rancher2_cloud_credential.auth.id control_plane_role = machine_pools.key == &quot;ctl_plane&quot; ? true : false etcd_role = machine_pools.key == &quot;ctl_plane&quot; ? true : false name = machine_pools.value.name quantity = machine_pools.value.quantity worker_role = machine_pools.key != &quot;ctl_plane&quot; ? true : false machine_config { kind = rancher2_machine_config_v2.nodes[machine_pools.key].kind name = replace(rancher2_machine_config_v2.nodes[machine_pools.key].name, &quot;_&quot;, &quot;-&quot;) } } } machine_selector_config { config = null } } }   As the focus here is more on the Cilium Cluster Mesh setup, we will not go into much detail about the Terraform RKE2 deployment. If there is demand for an in-depth blog post about Terraform RKE2 deployments, feel free to get in touch.  ","version":null,"tagName":"h2"},{"title":"Step 1: Export kubeconfig​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-1-export-kubeconfig","content":" Either from the Terraform execution plan or via the Rancher UI, collect the kubeconfig of the RKE2 clusters. Alternatively, we can SSH into one of the RKE2 master nodes and collect the kubeconfig found in the directory /etc/rancher/rke2/rke2.yaml.  $ export KUBECONFIG=&lt;directory of kubeconfig&gt; $ kubectl nodes   ","version":null,"tagName":"h2"},{"title":"Step 2: Helm list and values export​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-2-helm-list-and-values-export","content":" RKE2 comes with its own Cilium CNI Helm chart. That means RKE2 clusters will have an RKE2 Cilium Helm chart deployment in the kube-system namespace.  ","version":null,"tagName":"h2"},{"title":"Validate​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#validate","content":" $ export KUBECONFIG=&lt;directory of kubeconfig&gt; $ helm list -n kube-system NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION rke2-cilium kube-system\t1 2024-07-13 09:32:09.981662 +0200 CEST deployed\trke2-cilium-1.15.500 1.15.5 rke2-coredns kube-system\t1 2024-07-13 07:05:49.846980773 +0000 UTC\tdeployed\trke2-coredns-1.29.002 1.11.1 rke2-ingress-nginx kube-system\t1 2024-07-13 07:06:24.63272854 +0000 UTC deployed\trke2-ingress-nginx-4.8.200 1.9.3 rke2-metrics-server kube-system\t1 2024-07-13 07:06:24.86243331 +0000 UTC deployed\trke2-metrics-server-2.11.100-build2023051513\t0.6.3 rke2-snapshot-controller kube-system\t1 2024-07-13 07:06:26.764326178 +0000 UTC\tdeployed\trke2-snapshot-controller-1.7.202 v6.2.1 rke2-snapshot-controller-crd kube-system\t1 2024-07-13 07:06:24.217899546 +0000 UTC\tdeployed\trke2-snapshot-controller-crd-1.7.202 v6.2.1 rke2-snapshot-validation-webhook\tkube-system\t1 2024-07-13 07:06:24.544748567 +0000 UTC\tdeployed\trke2-snapshot-validation-webhook-1.7.302 v6.2.2   ","version":null,"tagName":"h3"},{"title":"Collect rke2-cilium Helm Values​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#collect-rke2-cilium-helm-values","content":" mesh01  $ helm get values rke2-cilium -n kube-system -o yaml &gt; values_mesh01.yaml   mesh02  $ helm get values rke2-cilium -n kube-system -o yaml &gt; values_mesh02.yaml   Example values_mesh01.yaml  global: cattle: clusterId: c-m-8ffz659l clusterCIDR: 10.244.0.0/16 clusterCIDRv4: 10.244.0.0/16 clusterDNS: 10.96.0.10 clusterDomain: cluster.local rke2DataDir: /var/lib/rancher/rke2 serviceCIDR: 10.96.0.0/18 hubble: enabled: true peerService: clusterDomain: cluster.local relay: enabled: true tls: auto: certValidityDuration: 1095 enabled: true method: helm ui: enabled: true k8sServiceHost: 127.0.0.1 k8sServicePort: 6443 kubeProxyReplacement: true operator: replicas: 1   note The configuration comes from the machine_global_config and chart_values sections defined in the Terraform code found in Step 0.  ","version":null,"tagName":"h3"},{"title":"Step 3: Cilium Cluster Mesh Helm Values​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-3-cilium-cluster-mesh-helm-values","content":" To set up the Cilium cluster mesh, we need to include the rke2-charts repo and later on, update the Helm values with the required cluster mesh settings. For this demonstration, we will use the NodePort deployment. For production environments, a LoadBalancer deployment is recommended as we do not have to rely on Node availability.  ","version":null,"tagName":"h2"},{"title":"Add rke2-charts Repo​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#add-rke2-charts-repo","content":" The action should be performed in both clusters.  $ helm repo add rke2-charts https://rke2-charts.rancher.io/ $ helm repo update   ","version":null,"tagName":"h3"},{"title":"Update mesh01 Helm Values​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#update-mesh01-helm-values","content":" On the same level as global, add the below configuration.  tls: ca: cert: &quot;&quot; # Base64 encoded shared CA crt key: &quot;&quot; # Base64 encoded shared CA key cluster: name: mesh01 # The unique name of the cluster id: 1 # The unique ID of the cluster used for the cluster mesh formation clustermesh: apiserver: replicas: 2 service: type: NodePort # Set the Clustermesh API service to be of type NodePort. Not recommended for Production environments nodePort: 32379 # Define the listening port for the Clustermesh API service tls: authMode: cluster server: extraDnsNames: - &quot;mesh01.mesh.cilium.io&quot; # Define the extra DNS config: clusters: - address: &quot;&quot; ips: - &lt;Node IP&gt; # The Node IP of the mesh02 cluster name: mesh02 port: 32380 # The NodePort defined on mesh02 for the Clustermesh API service enabled: true domain: &quot;mesh.cilium.io&quot; # Define the default domain for the mesh useAPIServer: true # Enable the Clustermesh API deployment   ","version":null,"tagName":"h3"},{"title":"Update mesh02 Helm Values​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#update-mesh02-helm-values","content":" On the same level as global, add the below configuration.  tls: ca: cert: &quot;&quot; # Base64 encoded shared CA crt key: &quot;&quot; # Base64 encoded shared CA key cluster: name: mesh02 # The unique name of the cluster id: 2 # The unique ID of the cluster used for the cluster mesh formation clustermesh: apiserver: replicas: 2 service: type: NodePort # Set the Clustermesh API service to be of type NodePort. Not recommended for production environments nodePort: 32380 # Define the listening port for the Clustermesh API service tls: authMode: cluster server: extraDnsNames: - &quot;mesh02.mesh.cilium.io&quot; # Define the extra DNS config: clusters: - address: &quot;&quot; ips: - &lt;Node IP&gt; # The Node IP of the mesg01 cluster name: mesh01 # Define the name of the cluster port: 32379 # The NodePort defined on mesh02 for the Clustermesh API service enabled: true domain: &quot;mesh.cilium.io&quot; # Define the default domain for the mesh useAPIServer: true # Enable the Clustermesh API deployment   ","version":null,"tagName":"h3"},{"title":"Update mesh01/mesh02 Helm deployment​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#update-mesh01mesh02-helm-deployment","content":" To ensure the updated Helm values are applied, we will use the HELM CLI to update the rke2-cilium deployment.  $ helm upgrade rke2-cilium rke2-charts/rke2-cilium --version 1.15.500 --namespace kube-system -f values_mesh01.yaml $ helm list -n kube-system   Perform the commands for the mesh02 cluster.  tip The helm upgrade command will create a new revision of the rke2-cilium application and show if the update was successful or not. Additionally, the cilium daemonset will get restarted and the Clustermesh API deployment will get created. Execute the commands below to double-check the update action. $ kubectl rollout status daemonset cilium -n kube-system $ kubectl get pods,svc -n kube-system | grep -i clustermesh   ","version":null,"tagName":"h3"},{"title":"Step 4: Validate Cilium Cluster Mesh​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-4-validate-cilium-cluster-mesh","content":" As we do not use the Cilium CLI, to ensure the Cilium cluster mesh works as expected, we will exec into the cilium daemonset and check the required details.  $ kubectl get ds -n kube-system | grep -i cilium cilium 4 4 4 4 4 kubernetes.io/os=linux 7d6h   ","version":null,"tagName":"h2"},{"title":"On mesh01 and mesh02​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#on-mesh01-and-mesh02","content":" $ kubectl exec -it ds/cilium -n kube-system -- cilium status | grep -i clustermesh Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, install-portmap-cni-plugin (init), config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) ClusterMesh: 1/1 clusters ready, 11 global-services   On both sides, the ClusterMesh should point to 1/1 clusters ready.  $ kubectl exec -it ds/cilium -n kube-system -- cilium-health status Defaulted container &quot;cilium-agent&quot; out of: cilium-agent, install-portmap-cni-plugin (init), config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) Probe time: 2024-07-20T13:58:47Z Nodes: mesh01/mesh01-controller-3d16581b-7q5bj (localhost): Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=693.829µs HTTP to agent: OK, RTT=118.583µs Endpoint connectivity to 10.244.1.71: ICMP to stack: OK, RTT=688.411µs HTTP to agent: OK, RTT=251.927µs mesh01/mesh01-controller-3d16581b-v58rq: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=671.007µs HTTP to agent: OK, RTT=237.395µs Endpoint connectivity to 10.244.0.75: ICMP to stack: OK, RTT=702.976µs HTTP to agent: OK, RTT=342.115µs mesh01/mesh01-worker-7ced0c6c-lz9sp: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=819.21µs HTTP to agent: OK, RTT=397.398µs Endpoint connectivity to 10.244.3.215: ICMP to stack: OK, RTT=821.223µs HTTP to agent: OK, RTT=465.965µs mesh01/mesh01-worker-7ced0c6c-w294x: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=738.787µs HTTP to agent: OK, RTT=335.803µs Endpoint connectivity to 10.244.2.36: ICMP to stack: OK, RTT=693.326µs HTTP to agent: OK, RTT=426.571µs mesh02/mesh02-controller-52d8e160-b27rn: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=683.278µs HTTP to agent: OK, RTT=335.076µs Endpoint connectivity to 10.245.0.106: ICMP to stack: OK, RTT=818.386µs HTTP to agent: OK, RTT=387.314µs mesh02/mesh02-controller-52d8e160-q4rvf: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=683.097µs HTTP to agent: OK, RTT=301.448µs Endpoint connectivity to 10.245.1.75: ICMP to stack: OK, RTT=748.101µs HTTP to agent: OK, RTT=510.124µs mesh02/mesh02-worker-a1c14ae0-5l759: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=631.954µs HTTP to agent: OK, RTT=266.391µs Endpoint connectivity to 10.245.3.232: ICMP to stack: OK, RTT=751.853µs HTTP to agent: OK, RTT=433.049µs mesh02/mesh02-worker-a1c14ae0-c7tcb: Host connectivity to x.x.x.x: ICMP to stack: OK, RTT=671.823µs HTTP to agent: OK, RTT=365.949µs Endpoint connectivity to 10.245.2.69: ICMP to stack: OK, RTT=690.894µs HTTP to agent: OK, RTT=466.73µs   note With the cilium-health status command, you should be able to see all the nodes from both clusters. Check the ICMP and HTTP status. Should be &quot;OK&quot;. Also, it might take a couple of minutes till the cilium-health status is available. If the time-out persists, have a look at the firewall rules and whether traffic between the clusters is allowed.  warning The NodePort IP addresses set for the cluster mesh need to be the IP addresses of the worker node instead of the master node. If they are the master node, the Cilium Cluster Mesh will not get deployed and we will get the below error. remote-etcd-cluster01 4m25s ago 4s ago 22 failed to detect whether the cluster configuration is required: etcdserver: permission denied   ","version":null,"tagName":"h3"},{"title":"Step 5: Hubble UI​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#step-5-hubble-ui","content":" To work with the Hubble UI we can use the kubectl port-forward of the Hubble UI service or update the existing rke2-cilium deployment on one of the nodes and expose the Hubble UI as a NodePort service. Just include the below in the values_mesh01.yaml or the values_mesh02.yaml file.   ui: enabled: true service: type: NodePort   For more information about the RKE2 Cilium Helm Chart values, have a look here.  ","version":null,"tagName":"h2"},{"title":"✉️ Contact​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#️-contact","content":" If you have any questions, feel free to get in touch! You can use the Discussions option found here or reach out to me on any of the social media platforms provided. 😊  We look forward to hearing from you!  ","version":null,"tagName":"h2"},{"title":"Conclusions​","type":1,"pageTitle":"Cilium Cluster Mesh on RKE2","url":"/personal-blog/blog/cilium-cluster-mesh-rke2#conclusions","content":" This is it! We performed a Cilium cluster mesh between two on-prem RKE2 clusters in just a few steps! 🎉  It's a wrap for this post! 🎉 Thanks for reading! Stay tuned for more exciting updates! ","version":null,"tagName":"h2"},{"title":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","type":0,"sectionRef":"#","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations","content":"","keywords":"","version":null},{"title":"Introduction​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#introduction","content":" How easy is it to handle Day-2 operations with existing CI/CD tooling? Sveltos provides the ability to perform not only Day-1 operations but also helps platform administrators, tenant administrators and other operators with Day-2 operations. For example, we can use the HealthCheck and the ClusterHealthCheck features to not only watch the health of a cluster but also collect information from the managed clusters and display them in the management cluster.  In today's blog post, we will cover a way of deploying Cilium as our CNI alongside Cilium Tetragon for observability. We will then continue with a simple TracingPolicy deployment to capture socket connections and then use Sveltos to display the tracing results back to the management cluster.  The goal of the demonstration is to showcase how Sveltos can be used for different Kubernetes cluster operations based on the use case at hand.    ","version":null,"tagName":"h2"},{"title":"Lab Setup​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#lab-setup","content":" +-----------------+-------------------+--------------------------+ | Cluster Name | Type | Version | +-----------------+-------------------+--------------------------+ | mgmt | Mgmt Cluster | v1.28.9+rke2r1 | | tetragon-test | Managed Cluster | v1.29.2+k3s1 | +-----------------+-------------------+--------------------------+ +-------------+---------------------+ | Deployment | Version | +-------------+---------------------+ | Cilium | v1.16.1 | | Tetragon | v1.2.0 | | sveltosctl | v0.37.0 | +-------------+---------------------+   ","version":null,"tagName":"h2"},{"title":"GitHub Resources​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#github-resources","content":" The YAML definition files are located here.  ","version":null,"tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#prerequisites","content":" To follow along, ensure the below are satisfied.  A management cluster with Sveltos installedkubectl installedsveltosctl installed  tip If you are unaware of installing Sveltos in a Kubernetes cluster, follow the instructions mentioned here.  ","version":null,"tagName":"h2"},{"title":"Step 1: Cluster Registration with Sveltos​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#step-1-cluster-registration-withsveltos","content":" Once the Kubernetes cluster is ready, we can continue with the Sveltos registration. To do that, we will utilise sveltosctl. The sveltosctl can be downloaded here.  ","version":null,"tagName":"h2"},{"title":"Example Registration​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#example-registration","content":" $ sveltosctl register cluster --namespace=test --cluster=tetragon-test \\ --kubeconfig=/home/test/tetragon-test.yaml \\ --labels=env=test   The cluster above will be registered with Sveltos on the mentioned namespace, and name, and will attach the cluster labels to perform different deployment versions.  note If the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.  ","version":null,"tagName":"h3"},{"title":"Validation​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#validation","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl get sveltoscluster -A --show-labels NAMESPACE NAME READY VERSION LABELS mgmt mgmt true v1.28.9+rke2r1 projectsveltos.io/k8s-version=v1.28.9,sveltos-agent=present test tetragon-test true v1.29.2+k3s1 env=test,projectsveltos.io/k8s-version=v1.29.2,sveltos-agent=present   tip Ensure the labels set are correct. We will use them at a later step.  ","version":null,"tagName":"h3"},{"title":"Step 2: Custom ConfigMap, Cilium, Tetragon Deployment​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#step-2-custom-configmap-cilium-tetragon-deployment","content":" As a first step, we will deploy Cilium and Cilium Tetragon to the clusters with the label set to env:test. Then, we will deploy a ConfigMap on the management cluster and allow Sveltos to deploy a TracingPolicy alongside a CronJob that polls tracing events every 2 minutes from the targeted managed cluster.  ","version":null,"tagName":"h2"},{"title":"ClusterProfile - Cilium, Tetragon, and ConfigMap​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#clusterprofile---cilium-tetragon-and-configmap","content":" --- apiVersion: config.projectsveltos.io/v1beta1 kind: ClusterProfile metadata: name: tetragon-test-deploy spec: clusterSelector: matchLabels: env: test helmCharts: - chartName: cilium/cilium chartVersion: 1.16.1 helmChartAction: Install releaseName: cilium releaseNamespace: kube-system repositoryName: cilium repositoryURL: https://helm.cilium.io/ - chartName: cilium/tetragon chartVersion: 1.2.0 helmChartAction: Install releaseName: tetragon releaseNamespace: kube-system repositoryName: tetragon repositoryURL: https://helm.cilium.io/ policyRefs: - name: tetragon-policy-socket-log namespace: default kind: ConfigMap   Sveltos follows the top-down approach when it comes to add-on and application deployment. First, Cilium will get deployed as our CNI. Next, then Tetragon and afterwards, we proceed with the deployment of a ConfigMap with the name tetragon-policy-socket-log which has already been deployed in the management cluster.  tip A copy of the ConfigMap YAML definition is located here.  ","version":null,"tagName":"h3"},{"title":"Deploy ConfigMap and ClusterProfile - Management Cluster​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#deploy-configmap-and-clusterprofile---management-cluster","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl apply -f tetragon_cm.yaml,clusterprofile_tetragon.yaml   ","version":null,"tagName":"h3"},{"title":"Validation - Management Cluster​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#validation---management-cluster","content":" $ ./sveltosctl show addons +--------------------+----------------------------------------------+-------------+-------------------------------+---------+--------------------------------+-------------------------------------+ | CLUSTER | RESOURCE TYPE | NAMESPACE | NAME | VERSION | TIME | PROFILES | +--------------------+----------------------------------------------+-------------+-------------------------------+---------+--------------------------------+-------------------------------------+ | test/tetragon-test | helm chart | kube-system | cilium | 1.16.1 | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy | | test/tetragon-test | helm chart | kube-system | tetragon | 1.2.0 | 2024-10-06 09:28:15 +0200 CEST | ClusterProfile/tetragon-test-deploy | | test/tetragon-test | rbac.authorization.k8s.io:ClusterRoleBinding | | tetragon-cluster-role-binding | N/A | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy | | test/tetragon-test | batch:CronJob | default | tetragon-log-fetcher | N/A | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy | | test/tetragon-test | cilium.io:TracingPolicy | | networking | N/A | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy | | test/tetragon-test | :ServiceAccount | default | tetragon-sa | N/A | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy | | test/tetragon-test | rbac.authorization.k8s.io:ClusterRole | | tetragon-cluster-role | N/A | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy | +--------------------+----------------------------------------------+-------------+-------------------------------+---------+--------------------------------+-------------------------------------+   ","version":null,"tagName":"h3"},{"title":"Validation - Managed Cluster​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#validation---managed-cluster","content":" $ kubectl get pods -n kube-system | grep -E &quot;cilium|tetragon&quot; cilium-operator-8547744bd7-qhl7r 1/1 Running 0 4m30s cilium-operator-8547744bd7-m26lh 1/1 Running 0 4m30s tetragon-mln8g 2/2 Running 0 4m29s tetragon-c7gwj 2/2 Running 0 4m29s tetragon-tjx54 2/2 Running 0 4m29s cilium-g7ftd 1/1 Running 0 4m30s cilium-pv9gj 1/1 Running 0 4m30s cilium-9cr4l 1/1 Running 0 4m30s cilium-envoy-9kjnv 1/1 Running 0 4m30s cilium-envoy-fpqkl 1/1 Running 0 4m30s cilium-envoy-25gvv 1/1 Running 0 4m30s tetragon-operator-55c555fcf4-s5mvs 1/1 Running 0 4m29s $ kubectl get cronjobs,jobs,pods NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE cronjob.batch/tetragon-log-fetcher */2 * * * * False 0 98s 5m26s NAME COMPLETIONS DURATION AGE job.batch/install-traefik2-nodeport-te 1/1 11s 47h job.batch/tetragon-log-fetcher-28803330 1/1 10s 3m38s job.batch/tetragon-log-fetcher-28803332 1/1 9s 98s NAME READY STATUS RESTARTS AGE pod/install-traefik2-nodeport-te-zrh57 0/1 Completed 0 47h pod/tetragon-log-fetcher-28803330-wpl9r 0/1 Completed 0 3m38s pod/tetragon-log-fetcher-28803332-r7q5v 0/1 Completed 0 98s   Based on the output above, we have deployed Cilium, Tetragon and a CronJob to collect Tetragon logs based on a tracing policy every 2 minutes with a timeout of 5 sec. 🎉 We can proceed further and use the Sveltos ClucterHealthCheck and HealthCheck to collect the data of the newly created ConfiMap in the managed cluster.  note If the defined ConfigMap or CronJob does not fit your needs, feel free to update the YAML definitions based on your liking.  ","version":null,"tagName":"h3"},{"title":"Step 3: Deploy ClusterHealthCheck and HealthCheck​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#step-3-deploy-clusterhealthcheck-and-healthcheck","content":" To be able to collect resources from the Sveltos managed cluster, we will use a new YAML definition to collect the data from the ConfigMap with the name tetragon-logs.  tip The ConfigMap tetragon-logs is created and patched with a periodic execution of Jobs mentioned in Step 2.  ","version":null,"tagName":"h2"},{"title":"HealthCheck and ClusterHealthCheck Defintion​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#healthcheck-and-clusterhealthcheck-defintion","content":" # Collect the resource of the ConfigMap with the name `tetragon-logs` --- apiVersion: lib.projectsveltos.io/v1beta1 kind: HealthCheck metadata: name: tetragon-log-fetcher spec: collectResources: true resourceSelectors: - group: &quot;&quot; version: &quot;v1&quot; kind: &quot;ConfigMap&quot; name: tetragon-logs namespace: default evaluateHealth: | function evaluate() local statuses = {} for _,resource in ipairs(resources) do status = &quot;Degraded&quot; table.insert(statuses, {resource=resource, status = status, message = resource.data[&quot;tetragon-logs.txt&quot;]}) end local hs = {} if #statuses &gt; 0 then hs.resources = statuses end return hs end # Get the ConfigMap data and send it to the management cluster --- apiVersion: lib.projectsveltos.io/v1beta1 kind: ClusterHealthCheck metadata: name: tetragon-log-fetcher spec: clusterSelector: matchLabels: env: test livenessChecks: - name: tetragon-log-fetcher type: HealthCheck livenessSourceRef: kind: HealthCheck apiVersion: lib.projectsveltos.io/v1beta1 name: tetragon-log-fetcher notifications: - name: event type: KubernetesEvent   ","version":null,"tagName":"h3"},{"title":"Deploy HealthCheck and ClusterHealthCheck - Management Cluster​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#deploy-healthcheck-and-clusterhealthcheck---management-cluster","content":" $ export KUBECONFIG=&lt;Sveltos managament cluster&gt; $ kubectl apply -f tetragon_healthcheck_logs.yaml   ","version":null,"tagName":"h3"},{"title":"Validation - Management Cluster​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#validation---management-cluster-1","content":" $ ./sveltosctl show resources +--------------------+---------------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+ | CLUSTER | GVK | NAMESPACE | NAME | MESSAGE | +--------------------+---------------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+ | test/tetragon-test | /v1, Kind=ConfigMap | default | tetragon-logs | 🔌 connect kube-system/coredns-6799fbcd5-4bv5r /coredns tcp 127.0.0.1:35846 -&gt; | | | | | | 127.0.0.1:8080 🧹 close kube-system/coredns-6799fbcd5-4bv5r /coredns tcp 127.0.0.1:35846 | | | | | | -&gt; 127.0.0.1:8080 🧹 close kube-system/coredns-6799fbcd5-4bv5r /coredns tcp 127.0.0.1:8080 | | | | | | -&gt; 127.0.0.1:35846 🔌 connect kube-system/coredns-6799fbcd5-4bv5r /coredns tcp | | | | | | 127.0.0.1:35852 -&gt; 127.0.0.1:8080 🧹 close kube-system/coredns-6799fbcd5-4bv5r /coredns tcp | | | | | | 127.0.0.1:35852 -&gt; 127.0.0.1:8080 🧹 close kube-system/coredns-6799fbcd5-4bv5r /coredns tcp | | | | | | 127.0.0.1:8080 -&gt; 127.0.0.1:35852 🔌 connect k3s-tetragon-9ab2-92fa7d-node-pool-df07-7inhp | | | | | | /var/lib/rancher/k3s/data/7d0aa19ffc230d4322f04d1ae8783e54ce189dfc4cbfa0a6afcdcabec2346d0c/bin/k3s | +--------------------+---------------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+   From the output above, we see the logs collected from the Tetragon TracingPolicy coming from a managed cluster and making them available in a management cluster! Cool, right? The same approach can be used with different data located in the managed clusters. A post written by Gianluca outlining the collection of kube-bench scanning results can be found here.  ","version":null,"tagName":"h3"},{"title":"Sveltos for Day-2 Operations Benefits​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#sveltos-for-day-2-operations-benefits","content":" Sveltos allows users to deploy the required add-on and deployments to a fleet of clusters while allowing platform administrators and operators to enhance the security posture and observability of the clusters in a simple and meaningful way. Use the Sveltos Event Framework, Tiers, ClusterHealthCheck, and HealthCheck features to enhance the posture of different platforms!  ","version":null,"tagName":"h2"},{"title":"Conclusions​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#conclusions","content":" In a few minutes ⏳, with minimal configuration effort and following the GitOps approach, we deployed Cilium as our CNI, Cilium Tetragon for observability alongside polling and displaying of critical tracing results to the management cluster painlessly! 🎉  In the next blog posts, we will touch on topics around Day-2 operations.  ","version":null,"tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#resources","content":" Cilium Labs: https://isovalent.com/resource-library/labs/Tetragon - Getting Started Lab: https://isovalent.com/labs/tetragon-getting-started/Sveltos ClusterHealthCheck/HealthCheck: https://projectsveltos.github.io/sveltos/observability/notifications/#example-configmap-healthcheckSveltos Event Framework: https://projectsveltos.github.io/sveltos/events/addon_event_deployment/Sveltos Tiers: https://projectsveltos.github.io/sveltos/deployment_order/tiers/  ","version":null,"tagName":"h2"},{"title":"✉️ Contact​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#️-contact","content":" We are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to join us us.  ","version":null,"tagName":"h2"},{"title":"👏 Support this project​","type":1,"pageTitle":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","url":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations#-support-thisproject","content":" Every contribution counts! If you enjoyed this article, check out the Projectsveltos GitHub repo. You can star 🌟 the project if you find it helpful.  The GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.  Thanks for reading! ","version":null,"tagName":"h2"},{"title":"Welcome","type":0,"sectionRef":"#","url":"/personal-blog/blog/welcome","content":"","keywords":"","version":null},{"title":"🌟 Welcome!​","type":1,"pageTitle":"Welcome","url":"/personal-blog/blog/welcome#-welcome","content":" Hello and welcome to my blog! 🚀  Here, you will discover everything you need to know about open source tools, DevOps, and GitOps practices.  Dive in and let's explore together! 💡🔧 ","version":null,"tagName":"h2"},{"title":"intro","type":0,"sectionRef":"#","url":"/personal-blog/docs/intro","content":"intro","keywords":"","version":"Next"}],"options":{"languages":["en","de"],"id":"default"}}