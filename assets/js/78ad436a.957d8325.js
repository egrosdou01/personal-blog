"use strict";(self.webpackChunkpersonal_blog=self.webpackChunkpersonal_blog||[]).push([[3977],{936:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"k8s-troubleshooting-insights-coredns","metadata":{"permalink":"/personal-blog/blog/k8s-troubleshooting-insights-coredns","source":"@site/blog/2024-10-27-k8s-troubleshooting-insights/k8s-troubleshooting-insights-coredns.md","title":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","description":"Introduction","date":"2024-10-27T00:00:00.000Z","tags":[{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Troubleshooting Insights","permalink":"/personal-blog/blog/tags/troubleshooting-insights","description":"Label attached to posts related to K8s insights"},{"inline":false,"label":"RKE2","permalink":"/personal-blog/blog/tags/rke2","description":"Rancher Kubernetes Engine 2 (RKE2)"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":7.87,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"k8s-troubleshooting-insights-coredns","title":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","authors":["egrosdou01"],"date":"2024-10-27T00:00:00.000Z","tags":["kubernetes","open-source","troubleshooting-insights","rke2","2024"]},"unlisted":false,"nextItem":{"title":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","permalink":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations"}},"content":"## Introduction\\n\\nWelcome to the the first post of the brand\xa0new\xa0Kubernetes Troubleshooting Insights section! The series of blog posts will share helpful information and troubleshooting tips for issues that might appear in a Kubernetes environment. The posts are focused on real-life scenarios from either `test`, `staging` or `production` environments.\\n\\nIn today\u2019s blog post, we\u2019ll explore an issue with [CoreDNS](https://kubernetes.io/docs/tasks/administer-cluster/coredns/) setup on [RKE2](https://docs.rke2.io/) clusters. [Cilium](https://docs.cilium.io/en/stable/overview/intro/#what-is-cilium) CNI with [Hubble](https://docs.cilium.io/en/stable/overview/intro/#what-is-hubble) were enabled for this setup. Let\u2019s jump right in!\\n\\n![title image reading \\"It\'s not DNS\\"](its_not_dns.jpeg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Environment Setup\\n\\n```bash\\n+-----------------+-------------------+--------------------------+----------------+\\n|   Cluster Name  |        Type       |         Version          |       OS       |\\n+-----------------+-------------------+--------------------------+----------------+\\n|    cluster01    |  Managed Cluster  |      v1.28.14+rke2r1     |   Ubuntu 22.04 |\\n+-----------------+-------------------+--------------------------+----------------+\\n\\n\\n+-------------+---------------------+\\n|  Deployment  |        Version     |\\n+-------------+---------------------+\\n|    Cilium    |       v1.16.1      |\\n+-------------+---------------------+\\n\\n```\\n\\n## Scenario\\n\\nDNS issues are wide and can be are caused by many reasons. Unfortunately, there is no one-size-fits-all approach when it comes to troubleshooting. In our scenario, we started with the virtual machines, ensured routing and DNS work and then moved to the Kubernetes layer. The blog post is an attempt to provide readers with troubleshooting ideas about DNS issues.\\n\\n**Background**: We performed a major migration to a new instance with **new DNS** servers and **domains**. We had to test if everything worked with the new setup. Everything appeared fine apart from synchronising **ArgoCD** with internal Git repositories. An error from an **internal Kubernetes IP address** on port **53** appeared. Weird, right? We were confident the underlying virtual machines were using the correct DNS, and the configuration of the DHCP server was updated.\\n\\n:::note\\nThe troubleshooting session was performed on an Ubuntu 22.04 environment. If another operating system is used, the troubleshooting methodology remains the same. The Linux commands will be different.\\n:::\\n\\n## Troubleshooting Steps\\n\\n### Step 1: Underlying Infrastructure\\n\\nThe below steps were performed to double-check the underlying infrastructure.\\n\\n1. **DHCP Server (if used)**: Ensure the configuration points to the new DNS servers\\n\\n    ```bash\\n    $ cat /etc/dhcp/dhcpd.conf\\n    ```\\n    Check the `domain-name-servers` configuration.\\n\\n1. **Cluster Node**: Perform an SSH connection to one of the cluster nodes\\n\\n    1. Check the DNS configuration\\n\\n        ```bash\\n        $ cat /etc/resolv.conf # Check the local DNS configuration\\n\\n        $ sudo resolvectl status # Check the global and per-link DNS settings currently in effect\\n        ```\\n\\n        From the command above, we would like to see how the Ethernet network interface on the virtual machine resolves domains. The ```resolvectl status``` command will reveal the use of the new DNS servers.\\n\\n    1. Check routing\\n\\n        ```bash\\n        $ ping test-site.example-domain.com # Check whether we can reach the custom domain\\n\\n        $ ping 8.8.8.8 # Check whether we can reach the Internet\\n        ```\\n\\n        If one of the commands above fail, this might be an indication that routing is broken, or something else is blocking traffic.\\n\\n        If the commands above are successful, we continue with the next step and dive into the Kubernetes world.\\n\\n### Step 2: Kubernetes Troubleshooting\\n\\nDepending on the Kubernetes environment in place, identify how DNS queries are resolved from a Kubernetes cluster point of view. In our case, the RKE2 clusters use `CoreDNS`.\\n\\nEven from a Kubernetes point of view, we will perform the well used `ping` and `curl` commands to see what is going on. For that reason, we will deploy the `dns-utils` pod to perform network calls. \\n\\n1. Deploy the `dns-utils` [pod](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/) to check DNS queries\\n\\n    ```bash\\n      $ kubectl apply -f - <<EOF\\n        apiVersion: v1\\n        kind: Pod\\n        metadata:\\n          name: dnsutils\\n          namespace: default\\n        spec:\\n          containers:\\n          - name: dnsutils\\n            image: registry.k8s.io/e2e-test-images/agnhost:2.39\\n            imagePullPolicy: IfNotPresent\\n          restartPolicy: Always\\n      EOF\\n    ```\\n1. Exec into the `dnsutils` pod - Perform `ping` and `dig` to well known website\\n\\n    ```bash\\n      $ kubectl exec -it dnsutils -- /bin/sh\\n\\n      / # ping 8.8.8.8\\n      PING 8.8.8.8 (8.8.8.8): 56 data bytes\\n      64 bytes from 8.8.8.8: icmp_seq=0 ttl=119 time=41.526 ms\\n\\n      / # dig SOA www.google.com\\n      ;; AUTHORITY SECTION:\\n      google.com.\\t\\t30\\tIN\\tSOA\\t<YOUR DNS Server>.google.com. dns-admin.google.com. 689372973 900 900 1800 60\\n    ```\\n\\n    If the above commands are successful, we know that routing and resolution works for well known websites.\\n\\n    :::tip\\n    Learn more about [SOA](https://www.cloudflare.com/en-gb/learning/dns/dns-records/dns-soa-record/).\\n    :::\\n\\n1. Exec into the `dnsutils` pod - Perform `curl` and `dig` to the custom domain\\n\\n    ```bash\\n      $ kubectl exec -it dnsutils -- /bin/sh\\n      / # curl -vvv example-domain.com\\n        * Could not resolve host: example-domain.com\\n        * Closing connection 0\\n        curl: (6) Could not resolve host: example-domain.com\\n      \\n      / # dig SOA example-domain.com\\n      ...\\n        ;; AUTHORITY SECTION:\\n        example-domain.com.\\t20\\tIN\\tSOA\\tns.dns.example-domain.com. hostmaster.example-domain.com. 1729344685 7200 1800 86400 30\\n\\n      / # dig @YOUR DNS Server SOA example-domain.com\\n      ...\\n      ;; AUTHORITY SECTION:\\n      example-domain.com. 3600\\tIN\\tSOA\\t<YOUR DNS SERVER NAME>.example-domain.com. mail.example-domain.com. 1729687405 604800 86400 2419200 604800\\n    ```\\n\\n      From the above, we can see that we can resolve the custom domain by defining our DNS server. But, when we do not define it the responsible entity to answer the DNS queries of the custom zone is set to the name\xa0`ns.dns.`. What is going on here? We would expect the DNS quetries to be resolved by the DNS serve defined in the virtual machines.\\n\\n    :::note\\n    It is important to check the `AUTHORITY SECTION` of the `dig` command output.\\n    `SOA ns.dns.example-domain.com.` signifies this is an SOA record, which provides essential information about a DNS zone. The `ns.dns.example-domain.com.` defines the primary DNS for that domain. But why? We would expect to see one of the new DNS servers instead.\\n    :::\\n\\n1. Identify the CoreDNS deployment\\n\\n    ```bash\\n    $ kubectl get deploy -n kube-system\\n    rke2-coredns-rke2-coredns              2/2     2            2           23h\\n    rke2-coredns-rke2-coredns-autoscaler   1/1     1            1           23h\\n    ```\\n1. Check the `CoreDNS ConfigMap`\\n\\n    ```bash\\n    $ kubectl get cm -n kube-system\\n    chart-content-rke2-coredns                             1      23h\\n    rke2-coredns-rke2-coredns                              1      23h\\n    rke2-coredns-rke2-coredns-autoscaler                   1      23h\\n\\n    $ kubectl get cm rke2-coredns-rke2-coredns -n kube-system -o jsonpath=\'{.data.Corefile}\'\\n      .:53 {\\n      errors \\n      health  {\\n          lameduck 5s\\n      }\\n      ready \\n      kubernetes example-domain.com   cluster.local  cluster.local in-addr.arpa ip6.arpa {\\n          pods insecure\\n          fallthrough in-addr.arpa ip6.arpa\\n          ttl 30\\n      }\\n      prometheus   0.0.0.0:9153\\n      forward   . /etc/resolv.conf\\n      cache   30\\n      loop \\n      reload \\n      loadbalance \\n      }\\n    ```\\n\\n1. Exec to the CoreDNS deployment and `cat` the `/etc/resolv.conf`\\n\\n    ```bash\\n    $ kubectl exec -it deploy/rke2-coredns-rke2-coredns -n kube-system -- cat /etc/resolv.conf\\n      nameserver <DNS01 IP>\\n      nameserver <DNS02 IP>\\n    ```\\n\\n    If the above output returns the expected nameservers, continue with the next step.\\n\\n1. Analyse the CoreDNS config\\n\\n    The output `kubernetes example-domain.com   cluster.local  cluster.local in-addr.arpa ip6.arpa` indicates that `CoreDNS` is responsible for responsing to DNS queries of the specified zones including `example-domain.com`. Should `CoreDNS` be responsible, or this is just a misconfiguration?\\n\\nIn our case, the custom domain was included to the `CoreDNS` configuration by mistake. Responses to DNS queries related to the custom domain should be forwarded to the defined DNS server instead.\\n\\nIf this is the case for your environment, edit the `ConfigMap` and remove the custom domain from the configuration. Use the commands below.\\n\\n```bash\\n$ kubectl patch cm rke2-coredns-rke2-coredns -n kube-system --type=\'json\' -p=\'[{\\"op\\": \\"replace\\", \\"path\\": \\"/data/Corefile\\", \\"value\\": \\".:53 {\\\\n    errors \\\\n    health  {\\\\n        lameduck 5s\\\\n    }\\\\n    ready \\\\n    kubernetes cluster.local cluster.local in-addr.arpa ip6.arpa {\\\\n        pods insecure\\\\n        fallthrough in-addr.arpa ip6.arpa\\\\n        ttl 30\\\\n    }\\\\n    prometheus 0.0.0.0:9153\\\\n    forward . /etc/resolv.conf\\\\n    cache 30\\\\n    loop \\\\n    reload \\\\n    loadbalance \\\\n}\\"}]\'\\n\\nconfigmap/rke2-coredns-rke2-coredns patched\\n\\n$ kubectl get cm rke2-coredns-rke2-coredns -n kube-system -o jsonpath=\'{.data.Corefile}\' # Ensure the custom domain is removed\\n\\n$ kubectl exec -it dnsutils -- /bin/sh\\n/ # curl <domain>:<port> # You should be able to resolved the domain now\\n```\\n\\n### Optional: Kubernetes Troubleshoot with Cilium Hubble\\n\\nAnother option to troubleshoot network issues is with Hubble. If it is available as part of your installation, you can exec into the Cilium `daemonset` and start using the Hubble CLI to observe traffic. For example, you can use something like the below.\\n\\n```bash\\n$ kubectl exec -it ds/cilium -n kube-system -- /bin/sh\\n# hubble observe --pod rke2-coredns-rke2-coredns-84b9cb946c-b7l9k --namespace kube-system --protocol UDP -f\\n```\\nThe command above will display UDP packages between the `dnsutils` pod and `CoreDNS`. The Hubble cheat sheet can be found [here](https://cilium.isovalent.com/hubfs/marketing%20briefs/Isovalent%20-%20Cilium%20Hubble%20Cheat%20Sheet.pdf).\\n\\n### Optional: Kubernetes Troubleshoot with Netshoot Pod and TCPDump\\n\\nIf we want to see what happens with the UDP packages when we perform a `CURL` request on a custom domain, it might be easier to instantiate a `tcpdump` using the [netshoot pod](https://hub.docker.com/r/nicolaka/netshoot/tags). Follow the commands below.\\n\\n```bash\\n$ kubectl run -it --rm debug --image=nicolaka/netshoot -- /bin/bash\\ndebug:~# tcpdump -i eth0 -n udp port 53\\n```\\n\\nOnce tcpdump is enabled, exec into the same pod and perform `CURL` requests.\\n\\n```bash\\n$ kubectl exec -it dnsutils -n kube-system -- /bin/sh\\n/ # curl example-domain.com\\n```\\n\\n### tcpdump Output\\n\\n```bash\\ntcpdump: verbose output suppressed, use -v[v]... for full protocol decode\\nlistening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes\\n16:31:52.197604 IP 10.42.2.184.49109 > 10.43.0.10.53: 59274+ [1au] A? example-domain.com.default.svc.cluster.local. (86)\\n16:31:52.197677 IP 10.42.2.184.49109 > 10.43.0.10.53: 134+ [1au] AAAA? example-domain.com.default.svc.cluster.local. (86)\\n16:31:52.198333 IP 10.43.0.10.53 > 10.42.2.184.49109: 59274 NXDomain*- 0/1/1 (179)\\n16:31:52.198553 IP 10.43.0.10.53 > 10.42.2.184.49109: 134 NXDomain*- 0/1/1 (179)\\n```\\n\\nThe output above indicates that a client queries a DNS server, and the server responds that the domain **does not exist**. This would be a hint to check the `CoreDNS` configuration! :)\\n\\n## Resources\\n\\n- **Debugging DNS**: https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/\\n\\n## \u2709\ufe0f Contact\\n\\nIf you have any questions, feel free to get in touch! You can use the `Discussions` option found [here](https://github.com/egrosdou01/personal-blog/discussions) or reach out to me on any of the social media platforms provided. \ud83d\ude0a\\n\\nWe look forward to hearing from you!\\n\\n## Conclusions\\n\\nIs it DNS at the end? This is something you will have to find out! Hopefully, the post gave you some ideas to troubleshoot with confidence DNS issues in a Kubernetes environment.\\n\\nIt\'s a wrap for this post! \ud83c\udf89 Thanks for reading! Stay tuned for more exciting updates!"},{"id":"sveltos-cilium-tetragon-day2-operations","metadata":{"permalink":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations","source":"@site/blog/2024-10-05-sveltos-day2-operations/sveltos-cilium-tetragon-day2-operations.md","title":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","description":"Introduction","date":"2024-10-05T00:00:00.000Z","tags":[{"inline":false,"label":"Sveltos","permalink":"/personal-blog/blog/tags/sveltos","description":"Open source project Sveltos"},{"inline":false,"label":"Cilium","permalink":"/personal-blog/blog/tags/cilium","description":"eBPF-based Networking, Security, and Observability for Kubernetes"},{"inline":false,"label":"Tetragon","permalink":"/personal-blog/blog/tags/tetragon","description":"Cilium Tetragon - powerful real-time, eBPF-based Security Observability and Runtime Enforcement"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":7.95,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"sveltos-cilium-tetragon-day2-operations","title":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","authors":["egrosdou01"],"date":"2024-10-05T00:00:00.000Z","tags":["sveltos","cilium","tetragon","open-source","kubernetes","gitops","devops","2024"]},"unlisted":false,"prevItem":{"title":"K8s Troubleshooting Insights: Looking into CoreDNS Issues","permalink":"/personal-blog/blog/k8s-troubleshooting-insights-coredns"},"nextItem":{"title":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","permalink":"/personal-blog/blog/sveltos-introduction-to-tiers"}},"content":"## Introduction\\n\\nHow easy is it to handle **Day-2** operations with existing CI/CD tooling? [Sveltos](https://github.com/projectsveltos) provides the ability to perform not only **Day-1** operations but also helps platform administrators, tenant administrators and other operators with **Day-2** operations. For example, we can use the [HealthCheck](https://github.com/projectsveltos/libsveltos/blob/main/api/v1beta1/healthcheck_type.go) and the [ClusterHealthCheck](https://raw.githubusercontent.com/projectsveltos/libsveltos/main/api/v1beta1/clusterhealthcheck_type.go) features to not only watch the health of a cluster but also collect information from the `managed` clusters and display them in the `management` cluster.\\n\\nIn today\'s blog post, we will cover a way of deploying [Cilium](https://cilium.io/) as our CNI alongside [Cilium Tetragon](https://tetragon.io/) for observability. We will then continue with a simple `TracingPolicy` deployment to capture socket connections and then use Sveltos to display the tracing results back to the `management` cluster.\\n\\nThe goal of the demonstration is to showcase how Sveltos can be used for different Kubernetes cluster operations based on the use case at hand.\\n\\n![title image reading \\"Sveltos Health Check\\"](Sveltos_Cilium_Tetragon_HealthCheck.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Lab Setup\\n\\n```bash\\n+-----------------+-------------------+--------------------------+\\n|   Cluster Name  |        Type       |         Version          |\\n+-----------------+-------------------+--------------------------+\\n|      mgmt       |   Mgmt Cluster    |      v1.28.9+rke2r1      |\\n| tetragon-test   |  Managed Cluster  |      v1.29.2+k3s1        |\\n+-----------------+-------------------+--------------------------+\\n\\n+-------------+---------------------+\\n|  Deployment  |        Version     |\\n+-------------+---------------------+\\n|    Cilium    |       v1.16.1      |\\n|   Tetragon   |       v1.2.0       |\\n|  sveltosctl  |       v0.37.0      |\\n+-------------+---------------------+\\n\\n```\\n\\n## GitHub Resources\\n\\nThe YAML definition files are located [here](https://github.com/egrosdou01/sveltos-demo-resources/tree/main/day-2-operations/sveltos-cilium-tetragon).\\n\\n## Prerequisites\\n\\nTo follow along, ensure the below are satisfied.\\n\\n1. A management cluster with Sveltos installed\\n1. kubectl installed\\n1. sveltosctl installed\\n\\n:::tip\\nIf you are unaware of installing Sveltos in a Kubernetes cluster, follow the instructions mentioned [here](https://projectsveltos.github.io/sveltos/getting_started/install/install/).\\n:::\\n\\n## Step 1: Cluster Registration with\xa0Sveltos\\n\\nOnce the Kubernetes cluster is ready, we can continue with the Sveltos registration. To do that, we will utilise `sveltosctl`. The `sveltosctl` can be downloaded [here](https://github.com/projectsveltos/sveltosctl/releases).\\n\\n### Example Registration\\n\\n```bash\\n$ sveltosctl register cluster --namespace=test --cluster=tetragon-test \\\\\\n    --kubeconfig=/home/test/tetragon-test.yaml \\\\\\n    --labels=env=test\\n```\\n\\nThe cluster above will be registered with Sveltos on the mentioned **namespace**, and **name**, and will attach the cluster labels to perform different deployment versions.\\n\\n:::note\\nIf the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.\\n:::\\n\\n### Validation\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster>\\n\\n$ kubectl get sveltoscluster -A --show-labels\\nNAMESPACE   NAME          READY   VERSION          LABELS\\nmgmt        mgmt            true    v1.28.9+rke2r1   projectsveltos.io/k8s-version=v1.28.9,sveltos-agent=present\\ntest        tetragon-test   true    v1.29.2+k3s1     env=test,projectsveltos.io/k8s-version=v1.29.2,sveltos-agent=present\\n```\\n\\n:::tip\\nEnsure the labels set are correct. We will use them at a later step.\\n:::\\n\\n## Step 2: Custom ConfigMap, Cilium, Tetragon Deployment\\n\\nAs a first step, we will deploy **Cilium** and **Cilium Tetragon** to the clusters with the label set to `env:test`. Then, we will deploy a `ConfigMap` on the `management` cluster and allow Sveltos to deploy a `TracingPolicy` alongside a `CronJob` that polls tracing events every 2 minutes from the targeted `managed` cluster.\\n\\n### ClusterProfile - Cilium, Tetragon, and ConfigMap\\n\\n```yaml\\n---\\napiVersion: config.projectsveltos.io/v1beta1\\nkind: ClusterProfile\\nmetadata:\\n  name: tetragon-test-deploy\\nspec:\\n  clusterSelector:\\n    matchLabels:\\n      env: test\\n  helmCharts:\\n  - chartName: cilium/cilium\\n    chartVersion: 1.16.1\\n    helmChartAction: Install\\n    releaseName: cilium\\n    releaseNamespace: kube-system\\n    repositoryName: cilium\\n    repositoryURL: https://helm.cilium.io/\\n  - chartName: cilium/tetragon\\n    chartVersion: 1.2.0\\n    helmChartAction: Install\\n    releaseName: tetragon\\n    releaseNamespace: kube-system\\n    repositoryName: tetragon\\n    repositoryURL: https://helm.cilium.io/\\n  policyRefs:\\n  - name: tetragon-policy-socket-log\\n    namespace: default\\n    kind: ConfigMap\\n```\\n\\nSveltos follows the top-down approach when it comes to add-on and application deployment. First, Cilium will get deployed as our CNI. Next, then **Tetragon** and afterwards, we proceed with the deployment of a `ConfigMap` with the name `tetragon-policy-socket-log` which has already been deployed in the `management` cluster.\\n\\n:::tip\\nA copy of the `ConfigMap` YAML definition is located [here](https://github.com/egrosdou01/sveltos-demo-resources/blob/main/day-2-operations/sveltos-cilium-tetragon/env-test/tetragon_configmap.yaml).\\n:::\\n\\n### Deploy ConfigMap and ClusterProfile - Management Cluster\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster> \\n\\n$ kubectl apply -f tetragon_cm.yaml,clusterprofile_tetragon.yaml\\n```\\n\\n### Validation - Management Cluster\\n\\n```bash\\n$ ./sveltosctl show addons                                         \\n+--------------------+----------------------------------------------+-------------+-------------------------------+---------+--------------------------------+-------------------------------------+\\n|      CLUSTER       |                RESOURCE TYPE                 |  NAMESPACE  |             NAME              | VERSION |              TIME              |              PROFILES               |\\n+--------------------+----------------------------------------------+-------------+-------------------------------+---------+--------------------------------+-------------------------------------+\\n| test/tetragon-test | helm chart                                   | kube-system | cilium                        | 1.16.1  | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy |\\n| test/tetragon-test | helm chart                                   | kube-system | tetragon                      | 1.2.0   | 2024-10-06 09:28:15 +0200 CEST | ClusterProfile/tetragon-test-deploy |\\n| test/tetragon-test | rbac.authorization.k8s.io:ClusterRoleBinding |             | tetragon-cluster-role-binding | N/A     | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy |\\n| test/tetragon-test | batch:CronJob                                | default     | tetragon-log-fetcher          | N/A     | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy |\\n| test/tetragon-test | cilium.io:TracingPolicy                      |             | networking                    | N/A     | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy |\\n| test/tetragon-test | :ServiceAccount                              | default     | tetragon-sa                   | N/A     | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy |\\n| test/tetragon-test | rbac.authorization.k8s.io:ClusterRole        |             | tetragon-cluster-role         | N/A     | 2024-10-06 09:28:12 +0200 CEST | ClusterProfile/tetragon-test-deploy |\\n+--------------------+----------------------------------------------+-------------+-------------------------------+---------+--------------------------------+-------------------------------------+\\n```\\n\\n### Validation - Managed Cluster\\n\\n```bash\\n$ kubectl get pods -n kube-system | grep -E \\"cilium|tetragon\\"\\ncilium-operator-8547744bd7-qhl7r     1/1     Running   0          4m30s\\ncilium-operator-8547744bd7-m26lh     1/1     Running   0          4m30s\\ntetragon-mln8g                       2/2     Running   0          4m29s\\ntetragon-c7gwj                       2/2     Running   0          4m29s\\ntetragon-tjx54                       2/2     Running   0          4m29s\\ncilium-g7ftd                         1/1     Running   0          4m30s\\ncilium-pv9gj                         1/1     Running   0          4m30s\\ncilium-9cr4l                         1/1     Running   0          4m30s\\ncilium-envoy-9kjnv                   1/1     Running   0          4m30s\\ncilium-envoy-fpqkl                   1/1     Running   0          4m30s\\ncilium-envoy-25gvv                   1/1     Running   0          4m30s\\ntetragon-operator-55c555fcf4-s5mvs   1/1     Running   0          4m29s\\n\\n$ kubectl get cronjobs,jobs,pods\\nNAME                                 SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\\ncronjob.batch/tetragon-log-fetcher   */2 * * * *   False     0        98s             5m26s\\n\\nNAME                                      COMPLETIONS   DURATION   AGE\\njob.batch/install-traefik2-nodeport-te    1/1           11s        47h\\njob.batch/tetragon-log-fetcher-28803330   1/1           10s        3m38s\\njob.batch/tetragon-log-fetcher-28803332   1/1           9s         98s\\n\\nNAME                                      READY   STATUS      RESTARTS   AGE\\npod/install-traefik2-nodeport-te-zrh57    0/1     Completed   0          47h\\npod/tetragon-log-fetcher-28803330-wpl9r   0/1     Completed   0          3m38s\\npod/tetragon-log-fetcher-28803332-r7q5v   0/1     Completed   0          98s\\n```\\n\\nBased on the output above, we have deployed **Cilium**, **Tetragon** and a **CronJob** to collect Tetragon logs based on a tracing policy every 2 minutes with a timeout of 5 sec. \ud83c\udf89 We can proceed further and use the Sveltos `ClucterHealthCheck` and `HealthCheck` to collect the data of the newly created `ConfiMap` in the managed cluster.\\n\\n:::note\\nIf the defined `ConfigMap` or `CronJob` does not fit your needs, feel free to update the YAML definitions based on your liking.\\n:::\\n\\n## Step 3: Deploy ClusterHealthCheck and HealthCheck\\n\\nTo be able to collect resources from the Sveltos managed cluster, we will use a new YAML definition to collect the data from the `ConfigMap` with the name `tetragon-logs`.\\n\\n:::tip\\nThe `ConfigMap tetragon-logs` is **created** and **patched** with a periodic execution of `Jobs` mentioned in Step 2.\\n:::\\n\\n### HealthCheck and ClusterHealthCheck Defintion\\n\\n```yaml\\n# Collect the resource of the ConfigMap with the name `tetragon-logs`\\n---\\napiVersion: lib.projectsveltos.io/v1beta1\\nkind: HealthCheck\\nmetadata:\\n  name: tetragon-log-fetcher\\nspec:\\n  collectResources: true\\n  resourceSelectors:\\n  - group: \\"\\"\\n    version: \\"v1\\"\\n    kind: \\"ConfigMap\\"\\n    name: tetragon-logs\\n    namespace: default\\n  evaluateHealth: |\\n    function evaluate()\\n      local statuses = {}\\n\\n      for _,resource in ipairs(resources) do\\n        status = \\"Degraded\\"\\n        table.insert(statuses, {resource=resource, status = status, message = resource.data[\\"tetragon-logs.txt\\"]})\\n      end\\n\\n      local hs = {}\\n      if #statuses > 0 then\\n        hs.resources = statuses \\n      end\\n      return hs\\n    end\\n# Get the ConfigMap data and send it to the management cluster\\n---\\napiVersion: lib.projectsveltos.io/v1beta1\\nkind: ClusterHealthCheck\\nmetadata:\\n  name: tetragon-log-fetcher\\nspec:\\n  clusterSelector:\\n    matchLabels:\\n      env: test\\n  livenessChecks:\\n  - name: tetragon-log-fetcher\\n    type: HealthCheck\\n    livenessSourceRef:\\n      kind: HealthCheck\\n      apiVersion: lib.projectsveltos.io/v1beta1\\n      name: tetragon-log-fetcher\\n  notifications:\\n  - name: event\\n    type: KubernetesEvent\\n```\\n\\n### Deploy HealthCheck and ClusterHealthCheck - Management Cluster\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster> \\n\\n$ kubectl apply -f tetragon_healthcheck_logs.yaml\\n```\\n\\n\\n### Validation - Management Cluster\\n\\n```bash\\n$ ./sveltosctl show resources\\n+--------------------+---------------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+\\n|      CLUSTER       |         GVK         | NAMESPACE |     NAME      |                                                   MESSAGE                                                    |\\n+--------------------+---------------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+\\n| test/tetragon-test | /v1, Kind=ConfigMap | default   | tetragon-logs | \ud83d\udd0c connect kube-system/coredns-6799fbcd5-4bv5r /coredns tcp 127.0.0.1:35846 ->                               |\\n|                    |                     |           |               | 127.0.0.1:8080 \ud83e\uddf9 close   kube-system/coredns-6799fbcd5-4bv5r /coredns tcp 127.0.0.1:35846                   |\\n|                    |                     |           |               | -> 127.0.0.1:8080 \ud83e\uddf9 close   kube-system/coredns-6799fbcd5-4bv5r /coredns tcp 127.0.0.1:8080                 |\\n|                    |                     |           |               | -> 127.0.0.1:35846 \ud83d\udd0c connect kube-system/coredns-6799fbcd5-4bv5r /coredns tcp                               |\\n|                    |                     |           |               | 127.0.0.1:35852 -> 127.0.0.1:8080 \ud83e\uddf9 close   kube-system/coredns-6799fbcd5-4bv5r /coredns tcp                |\\n|                    |                     |           |               | 127.0.0.1:35852 -> 127.0.0.1:8080 \ud83e\uddf9 close   kube-system/coredns-6799fbcd5-4bv5r /coredns tcp                |\\n|                    |                     |           |               | 127.0.0.1:8080 -> 127.0.0.1:35852 \ud83d\udd0c connect k3s-tetragon-9ab2-92fa7d-node-pool-df07-7inhp                   |\\n|                    |                     |           |               | /var/lib/rancher/k3s/data/7d0aa19ffc230d4322f04d1ae8783e54ce189dfc4cbfa0a6afcdcabec2346d0c/bin/k3s           |\\n+--------------------+---------------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+\\n```\\n\\nFrom the output above, we see the logs collected from the Tetragon TracingPolicy coming from a `managed` cluster and making them available in a `management` cluster! Cool, right? The same approach can be used with different data located in the `managed` clusters. A post written by Gianluca outlining the collection of `kube-bench` scanning results can be found [here](https://itnext.io/ensurincis-benchmark-compliance-across-multiple-kubernetes-clusters-dd544682e786).\\n\\n## Sveltos for Day-2 Operations Benefits\\n\\nSveltos allows users to deploy the required add-on and deployments to a fleet of clusters while allowing platform administrators and operators to **enhance** the **security posture** and **observability** of the clusters in a simple and meaningful way. Use the Sveltos `Event Framework`, `Tiers`, `ClusterHealthCheck`, and `HealthCheck` features to enhance the posture of different platforms!\\n\\n## Conclusions\\n\\nIn a few minutes \u23f3, with minimal configuration effort and following the GitOps approach, we deployed Cilium as our CNI, Cilium Tetragon for observability alongside polling and displaying of critical tracing results to the management cluster painlessly! \ud83c\udf89\\n\\nIn the next blog posts, we will touch on topics around Day-2 operations.\\n\\n## Resources\\n\\n- **Cilium Labs**: https://isovalent.com/resource-library/labs/\\n- **Tetragon - Getting Started Lab**: https://isovalent.com/labs/tetragon-getting-started/\\n- **Sveltos ClusterHealthCheck/HealthCheck**: https://projectsveltos.github.io/sveltos/observability/notifications/#example-configmap-healthcheck\\n- **Sveltos Event Framework**: https://projectsveltos.github.io/sveltos/events/addon_event_deployment/\\n- **Sveltos Tiers**: https://projectsveltos.github.io/sveltos/deployment_order/tiers/\\n\\n## \u2709\ufe0f Contact\\n\\nWe are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to [join us](https://app.slack.com/client/T0471SNT5CZ/C06UZCXQLGP) us.\\n\\n## \ud83d\udc4f Support this\xa0project\\n\\nEvery contribution counts! If you enjoyed this article, check out the Projectsveltos [GitHub repo](https://github.com/projectsveltos). You can [star \ud83c\udf1f the project](https://github.com/projectsveltos) if you find it helpful.\\n\\nThe GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.\\n\\nThanks for reading!"},{"id":"sveltos-introduction-to-tiers","metadata":{"permalink":"/personal-blog/blog/sveltos-introduction-to-tiers","source":"@site/blog/2024-09-26-sveltos-introduction-to-tiers/sveltos-introduction-to-tiers.md","title":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","description":"Introduction","date":"2024-09-26T00:00:00.000Z","tags":[{"inline":false,"label":"Sveltos","permalink":"/personal-blog/blog/tags/sveltos","description":"Open source project Sveltos"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":7.565,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"sveltos-introduction-to-tiers","title":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","authors":["egrosdou01"],"date":"2024-09-26T00:00:00.000Z","tags":["sveltos","open-source","kubernetes","gitops","devops","2024"]},"unlisted":false,"prevItem":{"title":"Sveltos: Optimising Day-2 Operations with Cilium and Tetragon","permalink":"/personal-blog/blog/sveltos-cilium-tetragon-day2-operations"},"nextItem":{"title":"OSSummit Europe 2024","permalink":"/personal-blog/blog/ossummit-europe-2024"}},"content":"## Introduction\\n\\nIn previous posts, we outlined how [Sveltos](https://github.com/projectsveltos) allows Platform and tenant administrators to streamline Kubernetes applications and add-on deployments to a fleet of clusters. In today\'s blog post, we will take a step further and demonstrate how easy it is to **target** and **update** a **subset** of resources targeted by **multiple configurations**. By multiple configurations, we refer to the [Sveltos ClusterProfile or Profile](https://projectsveltos.github.io/sveltos/addons/addons/) Custom Resource Definitions (CRDs). The demonstration focuses on **day-2 operations** as we provide a way to **update and/or remove** resources without affecting **production** operations.\\n\\nThis functionality is called [tiers](https://projectsveltos.github.io/sveltos/deployment_order/tiers/). Sveltos tiers provide a solution for managing the **deployment priority** when resources are targeted by multiple configurations. Tiers are easily integrated into existing ClusterProfile/Profile definitions alongside defining the deployment order control and straightforwardly override behaviour.\\n\\nToday, we will cover the case of updating the [Cilium CNI](https://docs.cilium.io/en/latest/) in a subnet of clusters with the label set to `tier:zone2` without affecting the monitoring capabilities defined in the **same** ClusterProfile/Profile.\\n\\n![title image reading \\"Sveltos Tiers\\"](sveltos_tiers.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Lab Setup\\n\\n```bash\\n+-----------------+-------------------+--------------------------+\\n|   Cluster Name  |        Type       |         Version          |\\n+-----------------+-------------------+--------------------------+\\n|   mgmt          | Mgmt Cluster      |      v1.28.9+rke2r1      |\\n|  prod-zone01    | Managed Cluster   |      v1.29.2+k3s1        |\\n|  prod-zone02    | Managed Cluster   |      v1.29.2+k3s1        |\\n+-----------------+-------------------+--------------------------+\\n\\n+-------------+---------------------+\\n| Deployment  | Version             |\\n+-------------+---------------------+\\n| Cilium      | v1.15.6, v1.16.1    |\\n| sveltosctl  | v0.37.0             |\\n+-------------+---------------------+\\n```\\n\\n## Prerequisites\\n\\nTo follow along, ensure the below are satisfied.\\n\\n1. A management cluster with Sveltos installed\\n1. kubectl installed\\n1. sveltosctl installed\\n\\n:::tip\\nIf you are unaware of installing Sveltos in a Kubernetes cluster, follow the instructions mentioned [here](https://projectsveltos.github.io/sveltos/getting_started/install/install/).\\n:::\\n\\n## Step 1: Register Clusters with\xa0Sveltos\\n\\nFor this demo, two [Civo Kubernetes](https://www.civo.com/kubernetes) clusters are used. Once the clusters are ready, we can proceed with the Sveltos cluster registration. To do that, we will utilise `sveltosctl`. The `sveltosctl` can be downloaded [here](https://github.com/projectsveltos/sveltosctl/releases).\\n\\n```bash\\n$ sveltosctl register cluster --namespace=<namespace> --cluster=<cluster name> \\\\\\n--kubeconfig=<path to Sveltos file with Kubeconfig> \\\\\\n--labels=key1=value1,key2=value2\\n```\\n\\n### Example Registration\\n\\n```bash\\n$ sveltosctl register cluster --namespace=civo --cluster=mesh01 \\\\\\n    --kubeconfig=/home/test/prod-zone01.yaml \\\\\\n    --labels=env=prod,tier=zone01\\n```\\n\\nWe will register the clusters with Sveltos on the mentioned **namespace**, and **name**, and will attach the cluster labels to perform different deployment versions.\\n\\n:::note\\nIf the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.\\n:::\\n\\n### Validation\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster>\\n\\n$ kubectl get sveltoscluster -A --show-labels\\nNAMESPACE   NAME          READY   VERSION          LABELS\\nmgmt        mgmt          true    v1.28.9+rke2r1   projectsveltos.io/k8s-version=v1.28.9,sveltos-agent=present\\nprod        prod-zone01   true    v1.28.7+k3s1     env=prod,projectsveltos.io/k8s-version=v1.28.7,sveltos-agent=present,tier=zone01\\nprod        prod-zone02   true    v1.28.7+k3s1     env=prod,projectsveltos.io/k8s-version=v1.28.7,sveltos-agent=present,tier=zone02\\n```\\n\\n:::tip\\nEnsure the labels set to the managed clusters are correct. We will use them at a later step.\\n:::\\n\\n## Step 2: Deploy Cilium and Monitoring Capabilities\\n\\nAs platform administrators, for every managed cluster, we want to have Cilium as our CNI and monitoring capabilities with Grafana, Prometheus and Loki. For that, we will use the Sveltos ClusterProfile Kubernetes resource and deploy the required deployments in clusters with the label set to `env:prod`.\\n\\n### ClusterProfile - Cilium, Grafana, Prometheus\\n\\n```yaml\\n---\\napiVersion: config.projectsveltos.io/v1beta1\\nkind: ClusterProfile\\nmetadata:\\n  name: cluster-prod-initial-setup\\nspec:\\n  clusterSelector:\\n    matchLabels:\\n      env: prod\\n  continueOnConflict: true\\n  helmCharts:\\n  - chartName: cilium/cilium\\n    chartVersion: 1.15.6\\n    helmChartAction: Install\\n    releaseName: cilium\\n    releaseNamespace: kube-system\\n    repositoryName: cilium\\n    repositoryURL: https://helm.cilium.io/\\n  - chartName: prometheus-community/kube-prometheus-stack\\n    chartVersion: v60.2.0\\n    helmChartAction: Install\\n    releaseName: prometheus-community\\n    releaseNamespace: monitoring\\n    repositoryName: prometheus-community\\n    repositoryURL: https://prometheus-community.github.io/helm-charts\\n    values: |\\n      grafana:\\n        service:\\n          type: LoadBalancer\\n      prometheus-node-exporter:\\n        service:\\n          port: 9200\\n          targetPort: 9200\\n  validateHealths:\\n  - name: deployment-health\\n    featureID: Helm\\n    group: \\"apps\\"\\n    version: \\"v1\\"\\n    kind: \\"Deployment\\"\\n    namespace: monitoring\\n    script: |\\n      function evaluate()\\n        local hs = {healthy = false, message = \\"Available replicas not match requested replicas\\"}\\n        if obj.status and obj.status.availableReplicas ~= nil and obj.status.availableReplicas == obj.spec.replicas then\\n          hs.healthy = true\\n        end\\n        return hs\\n      end\\n```\\n\\nWe instruct Sveltos firstly to install Cilium as our CNI. Next, we deploy the **Grafana** and **Prometheus** stack and ensure the second deployment is in a `healthy` state using the `validateHealths`. Afterwards, we proceed with the Loki integration.\\n\\n### ClusterProfile - Loki\\n\\n```yaml\\n---\\napiVersion: config.projectsveltos.io/v1beta1\\nkind: ClusterProfile\\nmetadata:\\n  name: loki-2102-prod\\nspec:\\n  dependsOn:\\n  - cluster-prod-initial-setup\\n  clusterSelector:\\n    matchLabels:\\n      env: prod\\n  continueOnConflict: true\\n  helmCharts:\\n  - repositoryURL: https://grafana.github.io/helm-charts\\n    repositoryName: grafana\\n    chartName: grafana/loki-stack\\n    chartVersion: v2.10.2\\n    releaseName: loki\\n    releaseNamespace: loki-stack\\n    helmChartAction: Install\\n```\\n\\n:::note\\nEnsure the `dependsOn` contains the correct name definition. In our example, it is`cluster-prod-initial-setup`.\\n:::\\n\\n### Deploy ClusterProfiles - Management Cluster\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster> \\n\\n$ kubectl apply -f clusterprofile_prod_setup.yaml,clusterprofile_loki.yaml\\n```\\n\\n### Validation - Management Cluster\\n\\n```bash\\n$ ./sveltosctl show addons\\n+------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+\\n|     CLUSTER      | RESOURCE TYPE |  NAMESPACE  |         NAME         | VERSION |              TIME              |                 PROFILES                  |\\n+------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+\\n| prod/prod-zone01 | helm chart    | kube-system | cilium               | 1.15.6  | 2024-09-23 12:18:37 +0200 CEST | ClusterProfile/cluster-prod-initial-setup |\\n| prod/prod-zone01 | helm chart    | monitoring  | prometheus-community | 60.2.0  | 2024-09-23 12:18:47 +0200 CEST | ClusterProfile/cluster-prod-initial-setup |\\n| prod/prod-zone01 | helm chart    | loki-stack  | loki                 | 2.10.2  | 2024-09-23 12:20:38 +0200 CEST | ClusterProfile/loki-2102-prod             |\\n| prod/prod-zone02 | helm chart    | kube-system | cilium               | 1.15.6  | 2024-09-23 12:18:47 +0200 CEST | ClusterProfile/cluster-prod-initial-setup |\\n| prod/prod-zone02 | helm chart    | monitoring  | prometheus-community | 60.2.0  | 2024-09-23 12:18:56 +0200 CEST | ClusterProfile/cluster-prod-initial-setup |\\n| prod/prod-zone02 | helm chart    | loki-stack  | loki                 | 2.10.2  | 2024-09-23 12:20:47 +0200 CEST | ClusterProfile/loki-2102-prod             |\\n+------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+\\n```\\n\\n### Validation - Managed Cluster\\n\\n```bash\\n$ kubectl get pods -n kube-system | grep -i cilium\\ncilium-operator-579c6c96c4-47wsk   1/1     Running   0          10m\\ncilium-operator-579c6c96c4-djk8s   1/1     Running   0          10m\\ncilium-mwk7x                       1/1     Running   0          10m\\ncilium-x5z5v                       1/1     Running   0          10m\\n\\n$ kubectl get ds/cilium -n kube-system -o jsonpath=\'{.spec.template.spec.containers[0].image}\'\\nquay.io/cilium/cilium:v1.15.6@sha256:6aa840986a3a9722cd967ef63248d675a87add7e1704740902d5d3162f0c0def\\n\\n$ kubectl get pods -n monitoring \\nNAME                                                       READY   STATUS    RESTARTS   AGE\\nprometheus-community-prometheus-node-exporter-78llc        1/1     Running   0          11m\\nprometheus-community-prometheus-node-exporter-xhcf9        1/1     Running   0          11m\\nprometheus-community-kube-operator-5746cfbb9-4p45x         1/1     Running   0          11m\\nprometheus-community-kube-state-metrics-7fc779fc58-z4ktb   1/1     Running   0          11m\\nalertmanager-prometheus-community-kube-alertmanager-0      2/2     Running   0          11m\\nprometheus-prometheus-community-kube-prometheus-0          2/2     Running   0          11m\\nprometheus-community-grafana-7494f9df89-pjqdd              3/3     Running   0          11m\\n\\n$ kubectl get pods -n loki-stack\\nNAME                  READY   STATUS    RESTARTS   AGE\\nloki-promtail-ttdgx   1/1     Running   0          10m\\nloki-promtail-jq5td   1/1     Running   0          10m\\nloki-0                1/1     Running   0          10m\\n```\\n\\nWe installed Cilium, Grafana, Prometheus and Loki in our production clusters. Awesome! \ud83c\udf89 Now, we will continue with the **update** of **Cilium** on a subnet of clusters only.\\n\\n## Step 3: Update Cilium tier:zone02 Cluster\\n\\nAs mentioned, we would like to use the Sveltos `tier` feature to update `Cilium` only on the clusters with the label set to `tier:zone02`. The matching cluster in our example will be the `prod-zone01` cluster.\\n\\n**Issue**: Because we have a ClusterProfile that installed Cilium to the cluster `prod-zone01`, how can we instruct Sveltos to update this cluster and only the Cilium deployment?\\n\\nTo achieve this, we will create a new ClusterProfile with the user of `tier`. We will instruct Sveltos to take the new ClusterProfile with a lower `tier` value set and update Cilium CNI only on the matching clusters.\\n\\n:::note\\nThe default `tier` value for every ClusterProfile/Profile is set to 100. If you set this to a lower value, Sveltos will take the lower value as a higher priority deployment.\\n:::\\n\\n### ClusterProfile - Update Cilium\\n\\n```yaml\\n---\\napiVersion: config.projectsveltos.io/v1beta1\\nkind: ClusterProfile\\nmetadata:\\n  name: cilium-1161\\nspec:\\n  tier: 50\\n  clusterSelector:\\n    matchLabels:\\n      tier: zone02\\n  continueOnConflict: true\\n  helmCharts:\\n  - chartName: cilium/cilium\\n    chartVersion: 1.16.1\\n    helmChartAction: Install\\n    releaseName: cilium\\n    releaseNamespace: kube-system\\n    repositoryName: cilium\\n    repositoryURL: https://helm.cilium.io/\\n```\\n\\n### Validation - Management Cluster\\n\\n\\n```bash\\n$ ./sveltosctl show addons --cluster=prod-zone02\\n+------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+\\n|     CLUSTER      | RESOURCE TYPE |  NAMESPACE  |         NAME         | VERSION |              TIME              |                 PROFILES                  |\\n+------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+\\n| prod/prod-zone02 | helm chart    | monitoring  | prometheus-community | 60.2.0  | 2024-09-23 12:18:56 +0200 CEST | ClusterProfile/cluster-prod-initial-setup |\\n| prod/prod-zone02 | helm chart    | loki-stack  | loki                 | 2.10.2  | 2024-09-23 12:20:47 +0200 CEST | ClusterProfile/loki-2102-prod             |\\n| prod/prod-zone02 | helm chart    | kube-system | cilium               | 1.16.1  | 2024-09-23 12:32:27 +0200 CEST | ClusterProfile/cilium-1161                |\\n+------------------+---------------+-------------+----------------------+---------+--------------------------------+-------------------------------------------+\\n```\\n\\n### Validation - Managed Cluster\\n\\n```bash\\n$ kubectl get pods -n kube-system | grep -i cilium                                                  \\ncilium-operator-66dcfc4678-76j6m   1/1     Running   0          2m44s\\ncilium-operator-66dcfc4678-6wvbr   1/1     Running   0          2m44s\\ncilium-xnt7k                       1/1     Running   0          2m44s\\ncilium-envoy-rj8pj                 1/1     Running   0          2m45s\\ncilium-8fbzc                       1/1     Running   0          2m44s\\ncilium-envoy-hkhmc                 1/1     Running   0          2m45s\\n\\n$ kubectl get ds/cilium -n kube-system -o jsonpath=\'{.spec.template.spec.containers[0].image}\'\\nquay.io/cilium/cilium:v1.16.1@sha256:0b4a3ab41a4760d86b7fc945b8783747ba27f29dac30dd434d94f2c9e3679f39\\n```\\n\\n## Sveltos Tiers Benefits\\n\\nSveltos tiers allow seamless **targeting** \ud83c\udfaf and **updating** \ud83d\udd04 of different Kubernetes applications and add-ons in a subset of clusters. Now, we have a way to perform updates without headaches \ud83d\ude0c and be confident and full of control \ud83d\udee0\ufe0f using the GitOps approach.\\n\\n## Conclusions\\n\\nIn a few minutes \u23f3, with minimal configuration effort and following the GitOps approach, we updated Cilium CNI in a subset of clusters painless! \ud83c\udf89 Find more about the Sveltos tiers [here](https://projectsveltos.github.io/sveltos/deployment_order/tiers/).\\n\\n## \u2709\ufe0f Contact\\n\\nWe are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to [join us](https://app.slack.com/client/T0471SNT5CZ/C06UZCXQLGP) us.\\n\\n## \ud83d\udc4f Support this\xa0project\\n\\nEvery contribution counts! If you enjoyed this article, check out the Projectsveltos [GitHub repo](https://github.com/projectsveltos). You can [star \ud83c\udf1f the project](https://github.com/projectsveltos) if you find it helpful.\\n\\nThe GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.\\n\\nThanks for reading!"},{"id":"ossummit-europe-2024","metadata":{"permalink":"/personal-blog/blog/ossummit-europe-2024","source":"@site/blog/2024-09-22-ossummit-europe-2024/ossummit-europe-2024.md","title":"OSSummit Europe 2024","description":"Introduction","date":"2024-09-22T00:00:00.000Z","tags":[{"inline":false,"label":"Conference","permalink":"/personal-blog/blog/tags/conference","description":"General Conference Tag"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":3.685,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"ossummit-europe-2024","title":"OSSummit Europe 2024","authors":["egrosdou01"],"date":"2024-09-22T00:00:00.000Z","tags":["conference","open-source","kubernetes","gitops","devops","2024"]},"unlisted":false,"prevItem":{"title":"Sveltos Tiers: Efficient Day-2 Operations and Targeted Updates","permalink":"/personal-blog/blog/sveltos-introduction-to-tiers"},"nextItem":{"title":"Civo Navigate Berlin 2024","permalink":"/personal-blog/blog/civo-navigate-berlin-2024"}},"content":"## Introduction\\n\\n[Sveltos](https://github.com/projectsveltos) is on tour! Another non-technical post describing my experience at the [OSSummit Europe 2024](https://events.linuxfoundation.org/open-source-summit-europe/). Apart from outlining my experience, the post will include useful resources on open-source projects I learned during the event.\\n\\nAbout Sveltos, [Gianluca Mardente](https://www.linkedin.com/in/gianlucamardente/) and I had the chance to talk at the conference and present Sveltos and how it is used to deploy and manage different Kubernetes applications and add-ons in a Multi-Cloud setup.\\n\\nIn the sections below, I will outline my highlights of the conference and what I have learned, while later on, I will describe what we presented about Sveltos and where to locate the required resources.\\n\\n![title image reading \\"OSSummit Europe 2024\\"](ossummit_europe_2024.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\n## OSSummit - Day 1\\n\\nAs with any other conference, the day began with the registration, badge pick-up and familiarisation with the location. Of course, coffee was widely available in almost every corner of the conference!\\n\\n![title image reading \\"Coffee Dance\\"](https://1075koolfm.com/cobPub/uploads/2024/07/giphy.gif)\\n\\nFor a quick look at the Day 1 keynotes, find below my highlights alongside the resources for further reading.\\n\\n- [Linux Foundation, CNCF, Unified Patents - partnership expansion](https://www.linuxfoundation.org/press/linux-foundation-and-cncf-expand-partnership-with-unified-patents-to-defend-open-source-software-from-non-practicing-entities)\\n- [Linux Foundation, Valkey 8.0](https://www.prnewswire.com/news-releases/announcing-valkey-8-0--302248447.html)\\n- [Developer Relations Foundation Announcement](https://github.com/DevRel-Foundation)\\n- [Open Source Innovation in Artificial Intelligence and Data](https://lfaidata.foundation/)\\n- [The Linux Foundation Europe](https://linuxfoundation.eu/)\\n\\nFrom a technical side, there was an announcement about the formation of the [OpenSearch Software Foundation](https://www.linuxfoundation.org/press/linux-foundation-announces-opensearch-software-foundation-to-foster-open-collaboration-in-search-and-analytics). AWS decided to move OpenSearch to the Linux Foundation. OpenSearch is a community-driven, Apache 2.0-licensed open-source search and analytics suite that makes it easy to ingest, search, visualise, and analyse data.\\n\\n### OpenSearch - Resources\\n\\n- [GitHub](https://github.com/opensearch-project)\\n- [OpenSearch Playground](https://playground.opensearch.org/app/home)\\n\\nApart from that, I was introduced to [cdk8s](https://cdk8s.io/) which is an open-source software development framework for defining Kubernetes applications. It synthesises applications into standard Kubernetes manifests files and supports a wide range of available programming languages like Typescript, Javascript, Go, Python, and Java.\\n\\n### cdk8s - Resources\\n\\n- [GitHub](https://github.com/cdk8s-team/cdk8s)\\n- [cdk8s Documentation](https://cdk8s.io/docs/latest/)\\n\\n## OSSummit - Day 2\\n\\nWhy not start the day with a 5k run? Around 20ish people gathered together and enjoyed a morning run near the Danube River! The Day 2 keynotes involved topics around secure supply chain consumption with existing frameworks, how to be compliant in a cloud-native landscape and ways to perform Policy as Code.\\n\\n### Frameworks\\n\\n- [Secure Supply Chain Consumption Framework (S2C2F)](https://github.com/ossf/s2c2f)\\n- [SLSA](https://slsa.dev/)\\n\\n### Container Image Patch Tools\\n\\n- [COPA](https://github.com/project-copacetic/copacetic)\\n\\nCOPA is an open-source tool written in Go that allows DevSecOps engineers to directly patch container images given vulnerability scanning results from popular tools like [Trivy](https://trivy.dev/).\\n\\n### Supply Chain Control Plane\\n\\n- [Chainloop](https://github.com/chainloop-dev/chainloop)\\n\\nChainloop is an open-source project I found out while crowling at the booth. Chainloop provides a single source of truth for **metadata** and **artifacts**, plus a declarative attestation process. We can declaratively state the pieces of evidence and artifact expectations for different CI/CD pipelines.\\n\\n### IoT\\n\\nIf you are interested in IoT, I had the chance to learn more about [Zephyr](https://github.com/zephyrproject-rtos/zephyr/tree/main) while attending interactive sessions about IoT development.\\n\\n### Open-source and Research\\n\\nIf you have an interest in open-source Research and Enablement check out [TODO](https://todogroup.org/) and [LERO/CURIOOS](https://lero.ie/) initialives alongside the [opensource.net](https://opensource.net) website for relevant topics.\\n\\n## OSSummit - Day 3\\n\\nDay 3 started with a group talk on Kernel development and what development looks like with Rust. I found interesting the talk from Paolo De Rosa who talked about the European Digital Identity (EUDI) Wallet and the fact that they want to achieve this with the use of open-source while getting help from the community.\\n\\n- [European Digital Identity (EUDI) Wallet](https://digital-strategy.ec.europa.eu/en/policies/eudi-wallet-implementation)\\n\\n## OSSummit - Sveltos\\n\\nFor the conference, we decided to demonstrate how Sveltos can be used to deploy and manage the Container Network Interface (CNI) lifecycle on a fleet of clusters with one manifest file while enabling [Cilium Hubble](https://docs.cilium.io/en/stable/overview/intro/) for Network observability. In the second part of the presentation, we demonstrated how to create another set of manifest files to deploy [Kyverno](https://kyverno.io/) and specific cluster policies down the clusters based on their scope.\\n\\n### Diagram\\n\\n![title image reading \\"OSSummit Europe 2024 - Sveltos Diagram\\"](ossummit_europe_diagram.jpg)\\n\\n### Git Repository\\n\\nThe Git repository with the manifest files and the execution instructions is located [here](https://github.com/egrosdou01/OSSummit_2024).\\n\\n## Conclusions\\n\\nI had a great fun at the conference! Not only had the chance to present alongside Gianluca, but I also met and interacted with cloud-native enthusiasts.\\n\\nTill August 2025!\\n\\n![title image reading \\"OSSummit Europe 2025\\"](ossummit_europe_2025.jpg)\\n\\nIt\'s a wrap for this post! \ud83c\udf89 Thanks for reading! Stay tuned for more exciting updates!"},{"id":"civo-navigate-berlin-2024","metadata":{"permalink":"/personal-blog/blog/civo-navigate-berlin-2024","source":"@site/blog/2024-09-12-civo-navigate-berlin-2024/civo-navigate-berlin-2024.md","title":"Civo Navigate Berlin 2024","description":"Introduction","date":"2024-09-12T00:00:00.000Z","tags":[{"inline":false,"label":"Conference","permalink":"/personal-blog/blog/tags/conference","description":"General Conference Tag"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":2.84,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"civo-navigate-berlin-2024","title":"Civo Navigate Berlin 2024","authors":["egrosdou01"],"date":"2024-09-12T00:00:00.000Z","tags":["conference","open-source","kubernetes","gitops","devops","2024"]},"unlisted":false,"prevItem":{"title":"OSSummit Europe 2024","permalink":"/personal-blog/blog/ossummit-europe-2024"},"nextItem":{"title":"OpenTofu: RKE2 Cluster with Cilium on Azure","permalink":"/personal-blog/blog/opentofu-rke2-cilium-azure"}},"content":"## Introduction\\n\\nToday\'s post will not be as technical as previous ones however, I wanted to share my experience at the [Civo Navigate](https://www.civo.com/navigate/europe) in Berlin. I had the chance to talk at the conference, present [Sveltos](https://github.com/projectsveltos) and how it can be used to painlessly deploy different Kubernetes applications and monitoring capabilities on a fleet of clusters.\\n\\nApart from that, I attended many different sessions covering relevant topics (Cloud native, Security, Thought Leadership, AI) and meet fellow enthusiasts.\\n\\nIn the sections below, I will outline some of my highlights and afterwards provide an introduction to the Sveltos presentation alongside the useful resources.\\n\\n![title image reading \\"Civo Navigate Berlin 2024\\"](civo_navigate_berlin_2024.png)\\n\x3c!--truncate--\x3e\\n\\n## Civo Navigate - Day 1\\n\\nThe day started quite chill by registering at the event, receiving a badge and becoming familiar with the location. Breakfast and coffee were provided which makes everything much easier.\\n\\nFully energized, the conference started with the keynotes and a positive outline of cloud computing, innovation and sustainability, which I can personally relate to it. More details about the Civo partnership with Deep Green can be found [here](https://www.civo.com/blog/greener-cloud-computing-deep-green). Afterwards, many empowering sessions took place, but some could not attend everything. I chose to attend those around developers empowerment and motivation. How developers can work best in diverse environments and how we can allow them to perform best by providing training, noise (notification) minimization, unnecessary meetings and freedom to think and decide on innovative solutions.\\n\\nFrom a technical side, I enjoyed a session about testing in modern CI/CD pipelines which gave me a different view of Kubernetes testing. This particular session was an introduction to [testkube](https://github.com/kubeshop/testkube). Not sure how much you can get from the free version in comparison to an enterprise one. Furthermore, I appreciated the informative session about the [Event Driven Ansible](https://www.ansible.com/blog/event-driven-ansible-is-here/) to reduce automation reaction time.\\n\\nAfter a long day and full of new ideas to try out in the lab, why not chill in the gaming area?\\n\\n![title image reading \\"Civo Navigate Berlin 2024\\"](civo_navigate_gaming_room.jpg)\\n\\n## Civo Navigate - Day 2\\n\\nThe day started with empowering keynotes from Kelsey Hightower about deep tech, Kubernetes, AI, and how the future might look like. The day continued with mostly technical sessions about open-source tools for Identity Management in Cloud Native stacks, platform engineering and [KUTTL](https://kuttl.dev/) for End-to-End (E2E) testing.\\n\\nSome of the mentioned IAM tools are listed below.\\n\\n- [Aaia](https://github.com/rams3sh/Aaia)\\n- [iamalive](https://github.com/iann0036/iamlive)\\n- [GCP Permissions](https://gcp.permissions.cloud/)\\n\\n:::tip\\nOnce the session recordings are available, a link will be included.\\n:::\\n\\n## Civo Navigate - Sveltos\\n\\nFor the conference, we decided to demonstrate how Sveltos can be used to deploy and take over the control of the Container Network Interface (CNI) lifecycle with one manifest file while enabling Cilium Hubble for Network observability. In the second part of the presentation, we demonstrated how to create another set of manifest files to integrate Grafana, Prometheus and Loki for container logs output.\\n\\n### Diagram\\n\\n![title image reading \\"Civo Navigate Berlin 2024\\"](civo_navigate_diagram.jpg)\\n\\n### Git Repository\\n\\nThe Git repository with the manifest files and the execution instructions are located [here](https://github.com/egrosdou01/civo_navigate_2024/tree/main).\\n\\n## Conclusions\\n\\nAll in all, it was an awesome experience to have the chance to attend and speak at the conference. If you plan to attend any upcoming Civo Navigate conferences, check out the [link](https://www.civo.com/navigate). I am pretty confident you will have an enjoyable experience!\\n\\nIt\'s a wrap for this post! \ud83c\udf89 Thanks for reading! Stay tuned for more exciting updates!"},{"id":"opentofu-rke2-cilium-azure","metadata":{"permalink":"/personal-blog/blog/opentofu-rke2-cilium-azure","source":"@site/blog/2024-08-21-opentofu-rke2-cilium-azure/opentofu-rke2-cilium-azure.md","title":"OpenTofu: RKE2 Cluster with Cilium on Azure","description":"Introduction","date":"2024-08-21T00:00:00.000Z","tags":[{"inline":false,"label":"OpenTofu","permalink":"/personal-blog/blog/tags/opentofu","description":"OpenTofu is a fork of Terraform"},{"inline":false,"label":"Cilium","permalink":"/personal-blog/blog/tags/cilium","description":"eBPF-based Networking, Security, and Observability for Kubernetes"},{"inline":false,"label":"RKE2","permalink":"/personal-blog/blog/tags/rke2","description":"Rancher Kubernetes Engine 2 (RKE2)"},{"inline":false,"label":"Azure","permalink":"/personal-blog/blog/tags/azure","description":"Azure Cloud"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":10.665,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"opentofu-rke2-cilium-azure","title":"OpenTofu: RKE2 Cluster with Cilium on Azure","authors":["egrosdou01"],"date":"2024-08-21T00:00:00.000Z","tags":["opentofu","cilium","rke2","azure","open-source","kubernetes","gitops","devops","2024"]},"unlisted":false,"prevItem":{"title":"Civo Navigate Berlin 2024","permalink":"/personal-blog/blog/civo-navigate-berlin-2024"},"nextItem":{"title":"Sveltos Templating: Cilium Cluster Mesh in One Run","permalink":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh"}},"content":"## Introduction\\n\\nIn a [previous post](../2024-07-26-rancher-rke2-azure/rancher-rke2-cilium-azure.md), we covered how to create an [RKE2](https://docs.rke2.io/) cluster on [Azure Cloud](https://azure.microsoft.com/en-us/get-started) using the [cloud-free credits](https://azure.microsoft.com/en-us/free#all-free-services) from the **Rancher UI**. As this is a convenient approach to get started with Rancher, in today\'s post, we will demonstrate how to use [OpenTofu](https://opentofu.org/) to automate the deployment.\\n\\n`OpenTofu` is a fork of [Terraform](https://www.terraform.io/). It is an open-source project, community-driven, and managed by the Linux Foundation. If you want to get familiar with what `OpenTofu` is and how to get started, check out the link [here](https://opentofu.org/docs/intro/core-workflow/).\\n\\nAdditionally, we will demonstrate how easy it is to customise the [Cilium](https://docs.cilium.io/en/stable/) configuration and enable [kube-vip](https://kube-vip.io/) for LoadBalancer services from the HCL (HashiCorp Configuration Language) definition.\\n\\n![title image reading \\"OpenTofu Rancher RKE2 Cluster on Azure\\"](openTofu_rancher_intro_image.jpg)\\n\x3c!--truncate--\x3e\\n\\n## Lab Setup\\n\\n```bash\\n+-----------------------------+------------------+----------------------+\\n|        Cluster Name         |       Type       |       Version        |\\n+-----------------------------+------------------+----------------------+\\n|          Rancher            |   k3s cluster    |    v1.28.7+k3s1      |\\n| Downstream RKE2 cluster     |       RKE2       |  v1.28.11+rke2r1     |\\n+-----------------------------+------------------+----------------------+\\n\\n+-------------------+----------+\\n|    Deployment     | Version  |\\n+-------------------+----------+\\n|      Cilium       | 1.15.500 |\\n|      OpenTofu     | v1.8.1   |\\n+-------------------+----------+\\n\\n```\\n\\n## Prerequisites\\n\\n### Rancher Server\\n\\nWe do not concentrate on installing `Rancher`. If you are unsure how to install Rancher, take a look at the official documentation [here](https://ranchermanager.docs.rancher.com/getting-started/quick-start-guides) or go through the guide I created a couple of weeks back found [here](https://medium.com/@eleni.grosdouli/rancher-on-eks-with-nginx-ingress-and-lets-encrypt-4f041fc1adae). \\n\\n### Azure Free Credits\\n\\nFor this demonstration, we will use the Azure [free credits](https://azure.microsoft.com/en-us/free) offering. The approach taken allows readers to understand how to set up the Azure cloud environment to perform RKE2 deployments with Rancher without spending money outside the free-credits offering.\\n\\nEnsure the below are satisfied.\\n\\n1. Helm CLI installed (Optional Step)\\n1. kubectl installed\\n\\n### Install OpenTofu\\n\\nThere is a wide variety of options provided to install `OpenTofu`. To follow along, check out the [link](https://opentofu.org/docs/intro/install/) and install `OpenTofu`.\\n\\n#### Validation\\n\\n```bash\\n$ tofu version\\nOpenTofu v1.8.1\\non darwin_arm64\\n```\\n\\n## Step 0: Pre-work\\n\\n### Step 0.1: Familiarise with OpenTofu Registry\\n\\nAs with the Terraform registry, the `OpenTofu` registry is a centralised service for **distributing** and **managing** providers/modules. Users can **share**, **discover**, and **consume** reusable infrastructure modules and providers.\\n\\nA list of the available providers/modules is located [here](https://github.com/opentofu/registry/).\\n\\nThe `rancher2` provider is supported by OpenTofu. The details can be found [here](https://github.com/opentofu/registry/tree/main/providers/r/rancher).\\n\\n\\n### Step 0.2: Familiarise with Rancher2 Provider\\n\\nBefore we even begin with the actual coding, it is a nice opportunity to familiarise with the [Rancher2 provider](https://search.opentofu.org/provider/opentofu/rancher2/v4.1.0).\\n\\n    ![title image reading \\"Rancher2 Terraform Provider\\"](rancher2_tf_provider.png)\\n\\n:::tip\\nCheck out the example sections of the resources available and the supported Cloud providers.\\n:::\\n\\n:::warning\\nBe mindful this is an alpha preview of the OpenTofu Registry UI. If you encounter any issues, report them [here](https://github.com/opentofu/registry-ui/issues).\\n:::\\n\\n### Step 0.3: Choose Integrated Development Environment (IDE)\\n\\nAs with any other project, we will use [Git](https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F) to store our code in a central location and [Visual Studio Code](https://code.visualstudio.com/) to perform the coding. Choose your favourite source control system and IDE, and dive into the next sections! \ud83d\ude80\\n\\n## GitHub Repo\\n\\nThe showcase repository is available [here](https://github.com/egrosdou01/opentofu-rke2-cilium-azure/tree/main).\\n\\n## Outline Project Structure\\n\\nLike with any Terraform project, we will create several `.tf` files to store the Infrastructure as Code (IaC) definitions. For best practices, have a look at the [link](https://spacelift.io/blog/opentofu-tutorial#opentofu-best-practices).\\n\\nIn your favourite IDE, create a new project and create the below file structure.\\n\\n### File structure\\n\\n- `main.tf`: Contains the resource blocks that define the resources to be created in the Azure cloud\\n- `variables.tf`: Contains the variable declaration used in the resource blocks\\n- `providers.tf`: Contains the required providers used in the resource blocks\\n- `data.tf`: Contains several data retrieved from the outside and used it through the resource creation\\n- `output.tf`: Contains the output that needs to be generated on successful completion of the OpenTofu plan/apply\\n- `*.tfvars`: Contains the default values of the specified variables\\n\\n## providers.tf\\n\\nThe `providers.tf` file holds the required providers that will be used for the creation of the relevant resources. OpenTofu configurations must declare which providers they require so that OpenTofu can install and use them.\\n\\n```hcl\\nterraform {\\n  required_version = \\"~> 1.8.1\\"\\n\\n  required_providers {\\n    rancher2 = {\\n      source  = \\"opentofu/rancher2\\"\\n      version = \\"4.1.0\\"\\n    }\\n    local = {\\n      source  = \\"opentofu/local\\"\\n      version = \\"2.5.1\\"\\n    }\\n    http = {\\n      source  = \\"opentofu/http\\"\\n      version = \\"3.4.4\\"\\n    }\\n  }\\n}\\n\\nprovider \\"rancher2\\" {\\n  api_url   = var.rancher2_api_url\\n  token_key = var.rancher2_token_key\\n}\\n```\\n\\n:::tip\\nIt is a good practice to avoid specifying sensitive data in the `variables.tf` file. The `providers.tf` file expects the `rancher2_api_url` and `rancher2_token_key` variables. Following the best practices, we can have a file that exports the required variable name and value. From a terminal window, we set the [source](https://www.digitalocean.com/community/tutorials/how-to-read-and-set-environmental-and-shell-variables-on-linux) pointing to the file before performing any IaC actions.\\n:::\\n\\n## data.tf\\n\\nThe `data.tf` file holds the code to download relevant information about the kube-vip installation. The information will be used later on in the `main.tf` file while defining the RKE2 cluster configuration.\\n\\n```hcl\\n# Download the kube-vip required RBAC manifest\\ndata \\"http\\" \\"kube_vip_rbac\\" {\\n  url = \\"https://kube-vip.io/manifests/rbac.yaml\\"\\n}\\n\\n# Download a specific kube-vip version\\ndata \\"http\\" \\"kube_vip_version\\" {\\n  method = \\"GET\\"\\n  url    = \\"https://api.github.com/repos/kube-vip/kube-vip/releases#v0.8.2\\"\\n}\\n\\n# Download the kube-vip-cloud-provider required manifest\\ndata \\"http\\" \\"kube_vip_cloud_provider\\" {\\n  url = \\"https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml\\"\\n}\\n\\n...\\n\\n```\\n\\n## output.tf\\n\\nIn the file, we can specify anything we want based on the use case at hand. For this demonstration, we keep it simple. We would display only the RKE2 `cluster-name` and `cluster-id`.\\n\\n```hcl\\n# Display the RKE2 Cluster Name\\noutput \\"rke2_cluster_name\\" {\\n  value = rancher2_cluster_v2.rke2.name\\n}\\n\\n# Display the RKE2 Cluster ID\\noutput \\"rancher_cluster_id\\" {\\n  value = data.rancher2_project.system.cluster_id\\n}\\n\\n...\\n\\n```\\n\\n## main.tf\\n\\nThe file contains the logic for creating virtual machines and installing RKE2 on top. We will break the `main.tf` file into smaller pieces and try to go through them in more detail.\\n\\n### Define the Azure Cloud Credentials\\n\\nIt is a requirement to have valid Azure cloud credentials before proceeding with the RKE2 installation. If you are unsure how to get the below variable details, have a look at my previous post [here](../2024-07-26-rancher-rke2-azure/rancher-rke2-cilium-azure.md#set-up-rancher-cloud-credentials).\\n\\n```hcl\\n# Create the Azure Cloud Credentials in Rancher\\nresource \\"rancher2_cloud_credential\\" \\"azure_creds\\" {\\n  name = \\"Azure Credentials\\"\\n  azure_credential_config {\\n    client_id       = var.azure_env.az_client_id\\n    client_secret   = var.azure_env.az_client_secret\\n    subscription_id = var.azure_env.az_subscription_id\\n  }\\n}\\n```\\n\\n### Define the Machine Configuration\\n\\nThe below resource will create the required virtual machines for the RKE2 cluster. Here, we define two types of nodes, the controller and the worker node. They could have the same or different hardware specifications based on the use case scenario that needs to be covered.\\n\\n```hcl\\n# Create the different nodes for RKE2 (controller and worker node)\\nresource \\"rancher2_machine_config_v2\\" \\"nodes\\" {\\n  for_each      = var.node\\n  generate_name = each.value.name\\n\\n  azure_config {\\n    disk_size            = each.value.agent_disk\\n    image                = each.value.image\\n    location             = each.value.location\\n    managed_disks        = true\\n    open_port            = each.value.open_port\\n    private_address_only = false\\n    resource_group       = each.value.resource_group\\n    storage_type         = each.value.storage_type\\n    size                 = each.value.agent_type\\n  }\\n}\\n```\\n\\n### Define the RKE2 Condifugration\\n\\n```hcl\\n# RKE2 configuration\\nresource \\"rancher2_cluster_v2\\" \\"rke2\\" {\\n  annotations           = var.rancher_env.cluster_annotations\\n  kubernetes_version    = var.rancher_env.rke2_version\\n  labels                = var.rancher_env.cluster_labels\\n  enable_network_policy = var.rancher_env.network_policy # Option to enable or disable Project Network Isolation.\\n  name                  = var.rancher_env.cluster_id\\n\\n  rke_config {\\n    # You can create a Terraform template and polulate the values of the file based on the variables defined below\\n    additional_manifest = templatefile(\\"${path.module}/files/kube-vip-daemonset.tfmpl\\",\\n      {\\n        int_name                = var.kube_vip.int_name\\n        kube_vip_rbac           = data.http.kube_vip_rbac.response_body\\n        kube_vip_version        = jsondecode(data.http.kube_vip_version.response_body)[0][\\"tag_name\\"]\\n        kube_vip_address        = var.kube_vip.kube_vip_address\\n        kube_vip_pool           = var.kube_vip.kube_vip_pool\\n        kube_vip_cloud_provider = data.http.kube_vip_cloud_provider.response_body\\n    })\\n\\n    # Define the Helm chart values for the Cilium installation\\n    chart_values = <<-EOF\\n      # Have a look at https://github.com/cilium/cilium/blob/main/install/kubernetes/cilium/values.yaml to include additional custom values\\n      rke2-cilium:\\n        k8sServiceHost: 127.0.0.1\\n        k8sServicePort: 6443\\n        kubeProxyReplacement: true # Enable Cilium with Kube-Proxy replacement on\\n      EOF\\n\\n    # Define the Rancher global settings for the whole cluster\\n    machine_global_config = <<EOF\\n      cni: \\"cilium\\" \\n      cluster-cidr: ${var.rke_cluster_cidr}\\n      service-cidr: ${var.rke_service_cidr}\\n      disable-kube-proxy: true\\n      EOF\\n\\n    # Sepcify the role of each node based on the name of the node\\n    dynamic \\"machine_pools\\" {\\n      for_each = var.node\\n      content {\\n        cloud_credential_secret_name = rancher2_cloud_credential.azure_creds.id\\n        control_plane_role           = machine_pools.key == \\"controller\\" ? true : false\\n        etcd_role                    = machine_pools.key == \\"controller\\" ? true : false\\n        name                         = machine_pools.value.name\\n        quantity                     = machine_pools.value.quantity\\n        worker_role                  = machine_pools.key != \\"controller\\" ? true : false\\n\\n        machine_config {\\n          kind = rancher2_machine_config_v2.nodes[machine_pools.key].kind\\n          name = rancher2_machine_config_v2.nodes[machine_pools.key].name\\n        }\\n      }\\n    }\\n\\n    machine_selector_config {\\n      config = null\\n    }\\n\\n  }\\n\\n...\\n\\n```\\n\\n## variables.tf\\n\\nOutline how the variables used in the `main.tf` file should look like. If required, perform additional validations to the code.\\n\\n```hcl\\nvariable \\"azure_env\\" {\\n  description = \\"Azure required details\\"\\n  type = object({\\n    az_client_id       = string\\n    az_client_secret   = string\\n    az_subscription_id = string\\n  })\\n}\\n\\nvariable \\"kube_vip\\" {\\n  description = \\"kube-vip basic settings\\"\\n  type = object({\\n    int_name         = string\\n    kube_vip_address = string\\n    kube_vip_pool    = string\\n  })\\n}\\n\\nvariable \\"node\\" {\\n  description = \\"Two RKE2 nodes to be configured\\"\\n  type = object({\\n    controller = object({\\n      name           = string\\n      agent_disk     = optional(number)\\n      image          = optional(string)\\n      location       = optional(string)\\n      open_port      = optional(list(string))\\n      resource_group = optional(string)\\n      storage_type   = optional(string)\\n      agent_type     = optional(string)\\n      quantity       = number\\n    })\\n    worker = object({\\n      name           = string\\n      agent_disk     = optional(number)\\n      image          = optional(string)\\n      location       = optional(string)\\n      open_port      = optional(list(string))\\n      resource_group = optional(string)\\n      storage_type   = optional(string)\\n      agent_type     = optional(string)\\n      quantity       = number\\n    })\\n  })\\n}\\n\\nvariable \\"rancher2_api_url\\" {\\n  description = \\"URL to Rancher Server API\\"\\n  type        = string\\n}\\n\\nvariable \\"rancher2_token_key\\" {\\n  description = \\"Rancher API Token key\\"\\n  type        = string\\n}\\n\\n...\\n\\n```\\n### terraform.tfvars\\n\\nThe file holds the input for the resource creation. Depending on how the `variables.tf` file looks like, we should set a similar structure to define the variables initialisation.\\n\\n```hcl\\nkube_vip = {\\n  int_name         = \\"eth0\\"\\n  kube_vip_address = \\"x.x.x.x\\"\\n  kube_vip_pool    = \\"x.x.x.x-x.x.x.x\\"\\n}\\n\\nnode = {\\n  controller = { name = \\"controller\\", quantity = 1, agent_disk = 30, image = \\"canonical:UbuntuServer:18.04-LTS:latest\\", location = \\"westus\\", resource_group = \\"rancher-rg\\", storage_type = \\"Standard_LRS\\", agent_type = \\"Standard_D2_v2\\" },\\n  worker  = { name = \\"worker\\", quantity = 1, agent_disk = 30, image = \\"canonical:UbuntuServer:18.04-LTS:latest\\", location = \\"westus\\", resource_group = \\"rancher-rg\\", storage_type = \\"Standard_LRS\\", agent_type = \\"Standard_D2_v2\\" }\\n}\\n\\nrancher_env = {\\n  cluster_annotations = { \\"rke2\\" = \\"demo\\" }\\n  cluster_labels      = { \\"rke2\\" = \\"azure-demo\\" }\\n  rke2_version     = \\"v1.28.11+rke2r1\\"\\n  cluster_id       = \\"eleni-azure-01\\"\\n  network_policy   = \\"false\\"\\n}\\n\\nrke_cluster_cidr   = \\"10.42.0.0/16\\"\\nrke_service_cidr   = \\"10.43.0.0/16\\"\\n```\\n\\n:::note\\nThe kube-vip interface name defined in the file represents the network interface from the virtual machines created in the Azure Cloud environment.\\n:::\\n\\n:::tip\\nThe node definition will allow you to create an RKE2 cluster based on the free-credits subscription. If the above are changed, the deployment might fail due to subscription limitations.\\n:::\\n\\n## Execution\\n\\nTo plan and apply the resources, use the below commands.\\n\\n```bash\\n\\n$ tofu init\\n\\n$ tofu plan\\n\\n$ tofu apply\\n\\n```\\n\\n:::note\\nWhen performing the `tofu init` command, I received the below warning.\\n\\n```bash\\n- Installing opentofu/rancher2 v4.1.0...\\n- Installed opentofu/rancher2 v4.1.0. Signature validation was skipped due to the registry not containing GPG keys for this provider\\n```\\n\\nI raised a `GitHub` [issue](https://github.com/rancher/terraform-provider-rancher2/issues/1385) with the Terraform Rancher2 Provider.\\n:::\\n\\n:::tip\\nCheck out the `.terraform/providers/registry.opentofu.org` directory with the providers sourced from the OpenTofu registry.\\n:::\\n\\nThe above will first create the Azure Cloud Credentials in the Rancher instance, then continue with the RKE2 cluster creation. The `tofu apply` command might take up to 10 min. Just wait for it to complete.\\n\\n## Validation\\n\\nIf the `tofu apply` command completes successfully, we should have a cluster with two nodes. One `controller` and one `worker` node in the `westus` region.\\n\\n![title image reading \\"Rancher2 Terraform Provider\\"](rke2_opentofu_azure.png)\\n\\n```bash\\n$ kubectk get nodes\\nNAME                                       STATUS   ROLES                       AGE     VERSION\\neleni-azure-01-controller-49abc099-ftvnv   Ready    control-plane,etcd,master   11m     v1.28.11+rke2r1\\neleni-azure-01-worker-87b90346-swd64       Ready    worker                      7m59s   v1.28.11+rke2r1\\n\\n$ kubectk get pods -n kube-system\\nNAME                                                                READY   STATUS      RESTARTS   AGE\\ncilium-5rfh4                                                        1/1     Running     0          11m\\ncilium-operator-6bd79b68b5-ch979                                    1/1     Running     0          11m\\ncilium-vmt9d                                                        1/1     Running     0          8m8s\\ncloud-controller-manager-eleni-azure-01-controller-49abc099-ftvnv   1/1     Running     0          11m\\netcd-eleni-azure-01-controller-49abc099-ftvnv                       1/1     Running     0          11m\\nhelm-install-rke2-cilium-kqmkc                                      0/1     Completed   0          11m\\nhelm-install-rke2-coredns-m5f8f                                     0/1     Completed   0          11m\\nhelm-install-rke2-ingress-nginx-vzdps                               0/1     Completed   0          11m\\nhelm-install-rke2-metrics-server-5t4sj                              0/1     Completed   0          11m\\nhelm-install-rke2-snapshot-controller-crd-jvdtd                     0/1     Completed   0          11m\\nhelm-install-rke2-snapshot-controller-zpkhv                         0/1     Completed   0          11m\\nhelm-install-rke2-snapshot-validation-webhook-6qlpx                 0/1     Completed   0          11m\\nkube-apiserver-eleni-azure-01-controller-49abc099-ftvnv             1/1     Running     0          11m\\nkube-controller-manager-eleni-azure-01-controller-49abc099-ftvnv    1/1     Running     0          11m\\nkube-scheduler-eleni-azure-01-controller-49abc099-ftvnv             1/1     Running     0          11m\\nkube-vip-5vlxw                                                      1/1     Running     0          11m\\nkube-vip-cloud-provider-85fd9b9cf7-n24fd                            1/1     Running     0          11m\\nrke2-coredns-rke2-coredns-84b9cb946c-5wch4                          1/1     Running     0          11m\\nrke2-coredns-rke2-coredns-84b9cb946c-zfkm5                          1/1     Running     0          8m5s\\nrke2-coredns-rke2-coredns-autoscaler-b49765765-4gkwf                1/1     Running     0          11m\\nrke2-ingress-nginx-controller-hljpx                                 1/1     Running     0          6m15s\\nrke2-metrics-server-655477f655-v2j6g                                1/1     Running     0          6m38s\\nrke2-snapshot-controller-59cc9cd8f4-66942                           1/1     Running     0          6m39s\\nrke2-snapshot-validation-webhook-54c5989b65-zqxgz                   1/1     Running     0          6m38s\\n```\\n\\n## Delete Resources\\n\\nIt is very easy to delete the resources created, simply perform the `tofu destroy` and confirm the action. The deletion of the resources will take up to 2 minutes.\\n\\n## \u2709\ufe0f Contact\\n\\nIf you have any questions, feel free to get in touch! You can use the `Discussions` option found [here](https://github.com/egrosdou01/personal-blog/discussions) or reach out to me on any of the social media platforms provided. \ud83d\ude0a\\n\\nWe look forward to hearing from you!\\n\\n## Conclusions\\n\\nThis is it! Automate the creation of RKE2 clusters in Azure with OpenTofu! \ud83c\udf89\\n\\nIt\'s a wrap for this post! \ud83c\udf89 Thanks for reading! Stay tuned for more exciting updates!"},{"id":"sveltos-templating-cilium-cluster-mesh","metadata":{"permalink":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh","source":"@site/blog/2024-08-10-sveltos-dynamic-templating/sveltos_dynamic_templating.md","title":"Sveltos Templating: Cilium Cluster Mesh in One Run","description":"Introduction","date":"2024-08-10T00:00:00.000Z","tags":[{"inline":false,"label":"Sveltos","permalink":"/personal-blog/blog/tags/sveltos","description":"Open source project Sveltos"},{"inline":false,"label":"Cilium","permalink":"/personal-blog/blog/tags/cilium","description":"eBPF-based Networking, Security, and Observability for Kubernetes"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":7.57,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"sveltos-templating-cilium-cluster-mesh","title":"Sveltos Templating: Cilium Cluster Mesh in One Run","authors":["egrosdou01"],"date":"2024-08-10T00:00:00.000Z","image":"./Sveltos_Templating_Cilium.jpg","tags":["sveltos","cilium","open-source","kubernetes","gitops","devops","2024"]},"unlisted":false,"prevItem":{"title":"OpenTofu: RKE2 Cluster with Cilium on Azure","permalink":"/personal-blog/blog/opentofu-rke2-cilium-azure"},"nextItem":{"title":"Rancher RKE2 Cluster on Azure","permalink":"/personal-blog/blog/rancher-rke2-cilium-azure"}},"content":"## Introduction\\n\\nHave you ever wondered how to dynamically instantiate Kubernetes resources before deploying them to a cluster? What if I tell you there is an easy way to do it? [Sveltos](https://github.com/projectsveltos) lets you define add-ons and applications using [templates](https://projectsveltos.github.io/sveltos/template/intro_template/). Before deploying any resource down the **managed** clusters, Sveltos instantiates the templates using information gathered from the **management** cluster.\\n\\nIn a [previous post](../2024-07-18-rke2-cilium/cilium-cluster-mesh-rke2.md), we outlined a step-by-step approach to forming a [Cilium cluster mesh](https://docs.cilium.io/en/v1.15/network/clustermesh/clustermesh/) between two clusters. In today\'s post, we will demonstrate how the Sveltos templating is used to deploy a Cilium cluster mesh dynamically in **one go**.\\n\\n![title image reading \\"Sveltos Templating Cilium\\"](Sveltos_Templating_Cilium.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Lab Setup\\n\\n```bash\\n+-----------------+-------------------+--------------------------+\\n|   Cluster Name  |        Type       |         Version          |\\n+-----------------+-------------------+--------------------------+\\n|   mgmt          | Mgmt Cluster      |      v1.28.9+rke2r1      |\\n|  mesh01         | Managed Cluster   |      v1.29.2+k3s1        |\\n|  mesh02         | Managed Cluster   |      v1.29.2+k3s1        |\\n+-----------------+-------------------+--------------------------+\\n\\n+-------------+----------+\\n|  Deployment | Version  |\\n+-------------+----------+\\n|    Cilium   | v1.15.6  |\\n|  sveltosctl | v0.32.0  |\\n+-------------+----------+\\n```\\n\\n## Prerequisites\\n\\nTo follow along, ensure the below are satisfied.\\n\\n1. A management cluster with Sveltos installed\\n1. kubectl installed\\n1. sveltosctl installed\\n\\n:::tip\\nIf you are unaware of how to install Sveltos in a Kubernetes cluster, follow the instructions mentioned [here](https://projectsveltos.github.io/sveltos/getting_started/install/install/).\\n:::\\n\\n## Step 1: Register Clusters with\xa0Sveltos\\n\\nFor this demonstration the [Civo Kubernetes](https://www.civo.com/kubernetes) cluster offering was used. Once the clusters are ready, it is time to proceed with the Sveltos cluster registration. To do that, we will utilise `sveltosctl` and generate a new kubeconfig file.\\n\\n```bash\\n$ sveltosctl register cluster --namespace=<namespace> --cluster=<cluster name> \\\\\\n    --kubeconfig=<path to Sveltos file with Kubeconfig> \\\\\\n    --labels=key=value\\n```\\n\\n### Example - mesh01 registration\\n\\n```bash\\n$ sveltosctl register cluster --namespace=civo --cluster=mesh01 \\\\\\n    --kubeconfig=/home/test/mesh01.yaml \\\\\\n    --labels=cilium=zone01\\n```\\n\\nWe will register the clusters with Sveltos on the mentioned **namespace**, **name**, and will attach the cluster **labels** `cilium=zone01` and `cilium=zone02` respectively.\\n\\n:::note\\nIf the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.\\n:::\\n\\n### Validation\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster>\\n\\n$ kubectl get sveltoscluster -A --show-labels\\nNAMESPACE   NAME     READY   VERSION          LABELS\\ncivo        mesh01   true    v1.29.2+k3s1     cilium=zone01\\ncivo        mesh02   true    v1.29.2+k3s1     cilium=zone02\\nmgmt        mgmt     true    v1.28.9+rke2r1   sveltos-agent=present\\n```\\n\\n## Step 2: Deploy Cilium Cluster Mesh ConfigMap\\n\\nBefore we even start working with the Sveltos templating, we will deploy two `ConfigMap` resources that include the Cilium Cluster Mesh configuration.\\n\\n### Example - mesh01 ConfigMap\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-config-mesh01\\n  namespace: civo\\ndata:\\n  id: \\"1\\"\\n  k8sServiceHost: \\"74.220.x.x\\"\\n  k8sServicePort: \\"6443\\"\\n  nodePort: \\"32379\\"\\n  crt: \\"Define the base64 ca.crt\\"\\n  key: \\"Define the base64 ca.key\\"\\n  clusterPoolIPv4PodCIDRList: \\"10.244.0.0/16\\"\\n  peermeshname: \\"mesh02\\"\\n  peermeship: \\"192.168.x.x\\"\\n  peermeshport: \\"32380\\"\\n```\\n\\nFrom the YAML definition, it is clear the resources we would like to deploy are in a key-value format. The `ConfigMap` for both clusters contains the required information to form a [Cilium cluster mesh](https://cilium.io/use-cases/cluster-mesh/) seamlessly.\\n\\n### Deploy ConfigMap Management Cluster\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster> \\n\\n$ kubectl apply -f config_mesh01.yaml,config_mesh02.yaml\\n\\n$ kubectl get cm -n civo\\nNAME                   DATA   AGE\\ncilium-config-mesh01   10     9s\\ncilium-config-mesh02   10     5s\\nkube-root-ca.crt       1      23m\\n```\\n\\n:::note\\nAs the Kubernetes managed clusters are registered in the `civo` namespace, the `ConfigMaps` are defined there.\\n:::\\n\\n## Step 3: Deploy Sveltos ClusterProfile\\n\\nWe will create a Sveltos [ClusterProfile](https://projectsveltos.github.io/sveltos/features/set/#referencing-clustersetset-in-a-clusterprofileprofile) to install **Cilium** as a **CNI** alongside the **Hubble UI** and **Cilium cluster mesh** by instantiating the configuration from the information located in the `ConfigMap` created in previous step.\\n\\n### Example - mesh01 ClusterProfile \\n\\n```yaml\\napiVersion: config.projectsveltos.io/v1beta1\\nkind: ClusterProfile\\nmetadata:\\n  name: cilium-1156-zone01\\nspec:\\n  clusterSelector:\\n    matchLabels:\\n      cilium: zone01\\n  templateResourceRefs:\\n  - resource:\\n      apiVersion: v1\\n      kind: ConfigMap\\n      name: cilium-config-{{ .Cluster.metadata.name }}\\n    identifier: CiliumConfig\\n  helmCharts:\\n  - chartName: cilium/cilium\\n    chartVersion: 1.15.6\\n    helmChartAction: Install\\n    releaseName: cilium\\n    releaseNamespace: kube-system\\n    repositoryName: cilium\\n    repositoryURL: https://helm.cilium.io/\\n    values: |\\n      tls:\\n        ca:\\n          cert: {{ (getResource \\"CiliumConfig\\").data.crt }}\\n          key: {{ (getResource \\"CiliumConfig\\").data.key }}\\n      cluster:\\n        id: {{ (getResource \\"CiliumConfig\\").data.id }}\\n        name: {{ .Cluster.metadata.name }}\\n      clustermesh:\\n        apiserver:\\n          replicas: 2\\n          service:\\n            type: NodePort\\n            nodePort: {{ (getResource \\"CiliumConfig\\").data.nodePort }}\\n          tls:\\n            authMode: cluster\\n            server:\\n              extraDnsNames:\\n                - \\"{{ .Cluster.metadata.name }}.mesh.cilium.io\\"\\n        config:\\n          clusters:\\n          - address: \\"\\"\\n            ips:\\n            - {{ (getResource \\"CiliumConfig\\").data.peermeship }} # The Node IP of the available nodes or a resolvable hostname\\n            name: {{ (getResource \\"CiliumConfig\\").data.peermeshname }}\\n            port: {{ (getResource \\"CiliumConfig\\").data.peermeshport }}\\n          enabled: true\\n          domain: \\"mesh.cilium.io\\"\\n        useAPIServer: true # This is required for the Cluster Mesh setup\\n      kubeProxyReplacement: true\\n      k8sServiceHost: {{ (getResource \\"CiliumConfig\\").data.k8sServiceHost }}\\n      k8sServicePort: {{ (getResource \\"CiliumConfig\\").data.k8sServicePort }}\\n      hubble:\\n        enabled: true\\n        peerService:\\n          clusterDomain: cluster.local\\n        relay:\\n          enabled: true\\n        tls:\\n          auto:\\n            certValidityDuration: 1095\\n            enabled: true\\n            method: helm\\n        ui:\\n          enabled: true\\n      nodeinit:\\n        enabled: true\\n      ipam:\\n        mode: cluster-pool\\n        operator:\\n          clusterPoolIPv4MaskSize: \\"24\\"\\n          clusterPoolIPv4PodCIDRList:\\n            - {{ (getResource \\"CiliumConfig\\").data.clusterPoolIPv4PodCIDRList }}\\n      nodePort:\\n        enabled: true\\n      debug:\\n        enabled: true\\n```\\n\\nThe `ClusterProfile` will get deployed to the **managed** clusters with the label set to `cilium:zone01`. Once a cluster is found, we instruct Sveltos to use the `templateResourceRefs` capability and use the details found in the `ConfgiMap` with the name set to `cilium-config-{{ .Cluster.metadata.name }}`. This will match the `cilium-config-mesh01` and `cilium-config-mesh02` ConfigMap. Then, we instruct Sveltos to install the Cilium Helm chart v1.15.6.\\n\\nFor the Cilium Helm chart instantiation, we go through the `ConfigMap` and populate the `values` based on the `key` definition.\\n\\n:::tip\\nWe deploy a Cilium cluster mesh with a `common TLS` certificate and a `NodePort` service. For the production environment, it is recommended to deploy a `LoadBalancer` setup. The above template can be used by updating the `clustermesh.service.type=LoadBalancer` and setting the standard service `Port`.\\n:::\\n\\n### Deploy ClusterProfile Management Cluster\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster> \\n\\n$ kubectl apply -f clusterprofile_mesh01.yaml,clusterprofile_mesh02.yaml\\n```\\n\\n## Step 4: Validate Results\\n\\n### Validation - sveltosctl\\n\\n```bash\\n$ ./sveltosctl show addons                                     \\n+-------------+---------------+-------------+--------+---------+--------------------------------+-----------------------------------+\\n|   CLUSTER   | RESOURCE TYPE |  NAMESPACE  |  NAME  | VERSION |              TIME              |             PROFILES              |\\n+-------------+---------------+-------------+--------+---------+--------------------------------+-----------------------------------+\\n| civo/mesh01 | helm chart    | kube-system | cilium | 1.15.6  | 2024-08-10 22:38:33 +0200 CEST | ClusterProfile/cilium-1156-zone01 |\\n| civo/mesh02 | helm chart    | kube-system | cilium | 1.15.6  | 2024-08-10 22:38:55 +0200 CEST | ClusterProfile/cilium-1156-zone02 |\\n+-------------+---------------+-------------+--------+---------+--------------------------------+-----------------------------------+\\n```\\n\\n### Validation - mesh01\\n\\n```bash\\n$ kubectl exec -it -n kube-system cilium-888jc -c cilium-agent -- cilium-dbg troubleshoot clustermesh mesh02\\nFound 1 remote cluster configurations\\nTroubleshooting filtered subset of clusters: mesh02\\n\\nRemote cluster \\"mesh02\\":\\n\ud83d\udcc4 Configuration path: /var/lib/cilium/clustermesh/mesh02\\n\\n\ud83d\udd0c Endpoints:\\n   - https://mesh02.mesh.cilium.io:32380\\n     \u2705 Hostname resolved to: 192.168.x.x\\n     \u2705 TCP connection successfully established to 192.168.x.x:32380\\n     \u2705 TLS connection successfully established to 192.168.x.x:32380\\n     \u2139\ufe0f  Negotiated TLS version: TLS 1.3, ciphersuite TLS_AES_128_GCM_SHA256\\n     \u2139\ufe0f  Etcd server version: 3.5.14\\n\\n\ud83d\udd11 Digital certificates:\\n   \u2705 TLS Root CA certificates:\\n      - Serial number:       2c:b4:43:9c:fb:82:62:4f:55:0f:eb:5e:a4:fe:af:5e:14:95:18:74\\n        Subject:             CN=Cilium LAB CA\\n        Issuer:              CN=Cilium LAB CA\\n        Validity:\\n          Not before:  2024-04-25 14:04:31 +0000 UTC\\n          Not after:   2034-04-23 14:04:31 +0000 UTC\\n   \u2705 TLS client certificates:\\n      - Serial number:       13:8f:38:4e:e9:08:bb:81:93:20:ff:30:33:ed:fb:13\\n        Subject:             CN=remote-mesh01\\n        Issuer:              CN=Cilium LAB CA\\n        Validity:\\n          Not before:  2024-08-10 20:53:31 +0000 UTC\\n          Not after:   2027-08-10 20:53:31 +0000 UTC\\n\\n\u2699\ufe0f Etcd client:\\n   \u2705 Etcd connection successfully established\\n   \u2139\ufe0f  Etcd cluster ID: 820847063f6cabce\\n\\n```\\n\\n### Validation - mesh02\\n\\n```bash\\n$ kubectl exec -it -n kube-system cilium-94ddz -c cilium-agent -- cilium-dbg troubleshoot clustermesh mesh01\\nFound 1 remote cluster configurations\\nTroubleshooting filtered subset of clusters: mesh01\\n\\nRemote cluster \\"mesh01\\":\\n\ud83d\udcc4 Configuration path: /var/lib/cilium/clustermesh/mesh01\\n\\n\ud83d\udd0c Endpoints:\\n   - https://mesh01.mesh.cilium.io:32379\\n     \u2705 Hostname resolved to: 192.168.x.x\\n     \u2705 TCP connection successfully established to 192.168.x.x:32379\\n     \u2705 TLS connection successfully established to 192.168.x.x:32379\\n     \u2139\ufe0f  Negotiated TLS version: TLS 1.3, ciphersuite TLS_AES_128_GCM_SHA256\\n     \u2139\ufe0f  Etcd server version: 3.5.14\\n\\n\ud83d\udd11 Digital certificates:\\n   \u2705 TLS Root CA certificates:\\n      - Serial number:       2c:b4:43:9c:fb:82:62:4f:55:0f:eb:5e:a4:fe:af:5e:14:95:18:74\\n        Subject:             CN=Cilium LAB CA\\n        Issuer:              CN=Cilium LAB CA\\n        Validity:\\n          Not before:  2024-04-25 14:04:31 +0000 UTC\\n          Not after:   2034-04-23 14:04:31 +0000 UTC\\n   \u2705 TLS client certificates:\\n      - Serial number:       94:02:e1:4a:b8:74:4c:d7:62:af:c1:d8:19:a8:3b:8f\\n        Subject:             CN=remote-mesh02\\n        Issuer:              CN=Cilium LAB CA\\n        Validity:\\n          Not before:  2024-08-10 20:59:07 +0000 UTC\\n          Not after:   2027-08-10 20:59:07 +0000 UTC\\n\\n\u2699\ufe0f Etcd client:\\n   \u2705 Etcd connection successfully established\\n   \u2139\ufe0f  Etcd cluster ID: 21f7360bef94b707\\n```\\n\\nCilium cluster mesh is deployed in **one run** without the need for an additional Infrastructure as Code (IaC) tool or additional rendering!\\n\\n![title image reading \\"Michael Scott Dance - Reference: Phillip Hamilton - https://knowyourmeme.com/memes/michael-scott-with-speaker-everybody-dance-now\\"](https://i.kym-cdn.com/entries/icons/original/000/045/269/ebdn.jpg)\\n\\n## Sveltos Templating Benefits\\n\\nSveltos templating enables users to utilise the **same add-on configuration** across different clusters while allowing variations like different add-on configuration values.\\n\\nSveltos lets users define add-ons and applications in a **reusable** way. We can deploy the definitions across **multiple** clusters with **minor adjustments**. The approach saves **time**, **effort**, and **headaches**, especially in large-scale environments.\\n\\n## Conclusions\\n\\nIn a couple of minutes and with a minimal configuration effort we formed a cluster mesh between two Kubernetes clusters. This is the power of Sveltos templating.\\n\\nIf you want to explore a step-by-step guide on how to setup a Cilium cluster mesh, have a look at my previous blog post [here](../2024-07-18-rke2-cilium/cilium-cluster-mesh-rke2.md).\\n\\n## \u2709\ufe0f Contact\\n\\nWe are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to [join us](https://app.slack.com/client/T0471SNT5CZ/C06UZCXQLGP) us.\\n\\n## \ud83d\udc4f Support this\xa0project\\n\\nEvery contribution counts! If you enjoyed this article, check out the Projectsveltos [GitHub repo](https://github.com/projectsveltos). You can [star \ud83c\udf1f the project](https://github.com/projectsveltos) if you find it helpful.\\n\\nThe GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.\\n\\nThanks for reading!"},{"id":"rancher-rke2-cilium-azure","metadata":{"permalink":"/personal-blog/blog/rancher-rke2-cilium-azure","source":"@site/blog/2024-07-26-rancher-rke2-azure/rancher-rke2-cilium-azure.md","title":"Rancher RKE2 Cluster on Azure","description":"Introduction","date":"2024-07-26T00:00:00.000Z","tags":[{"inline":false,"label":"Rancher","permalink":"/personal-blog/blog/tags/rancher","description":"Rancher"},{"inline":false,"label":"Cilium","permalink":"/personal-blog/blog/tags/cilium","description":"eBPF-based Networking, Security, and Observability for Kubernetes"},{"inline":false,"label":"RKE2","permalink":"/personal-blog/blog/tags/rke2","description":"Rancher Kubernetes Engine 2 (RKE2)"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"Azure","permalink":"/personal-blog/blog/tags/azure","description":"Azure Cloud"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":8.145,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"rancher-rke2-cilium-azure","title":"Rancher RKE2 Cluster on Azure","authors":["egrosdou01"],"date":"2024-07-26T00:00:00.000Z","tags":["rancher","cilium","rke2","open-source","kubernetes","gitops","devops","azure","2024"],"image":"./Rancher_RKE2_Cilium_Azure.jpg"},"unlisted":false,"prevItem":{"title":"Sveltos Templating: Cilium Cluster Mesh in One Run","permalink":"/personal-blog/blog/sveltos-templating-cilium-cluster-mesh"},"nextItem":{"title":"Cilium Cluster Mesh on RKE2","permalink":"/personal-blog/blog/cilium-cluster-mesh-rke2"}},"content":"## Introduction\\n\\nFor the last couple of days, I have been working on a new use case installing [RKE2](https://docs.rke2.io/) clusters powered with [Cilium](https://docs.cilium.io/en/v1.15/) on [Azure Cloud](https://azure.microsoft.com/en-us/get-started). The requirement at hand was to use a [Rancher](https://ranchermanager.docs.rancher.com/v2.8) instance and from there start deploying RKE2 clusters. After going through the official Rancher documentation, I have noticed that the instructions provided to pre-configure [Azure Cloud](https://ranchermanager.docs.rancher.com/v2.8/how-to-guides/new-user-guides/launch-kubernetes-with-rancher/use-new-nodes-in-an-infra-provider/create-an-azure-cluster) are outdated.\\n\\nIn today\'s blog post, we will cover all the required steps taken to configure the [Azure cloud-free credits](https://azure.microsoft.com/en-us/free#all-free-services) to deploy RKE2 clusters with Cilium in that environment. Additionally, we will cover any limitations that come with the `free credit` concept.\\n\\n![title image reading \\"Rancher RKE2 Cluster on Azure\\"](Rancher_RKE2_Cilium_Azure.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Lab Setup\\n\\n```bash\\n+-----------------------------+------------------+----------------------+\\n|        Cluster Name         |       Type       |       Version        |\\n+-----------------------------+------------------+----------------------+\\n|          Rancher            |   k3s cluster    |    v1.28.7+k3s1      |\\n| Downstream RKE2 cluster     |       RKE2       |  v1.28.11+rke2r1     |\\n+-----------------------------+------------------+----------------------+\\n\\n+-------------------+----------+\\n|    Deployment     | Version  |\\n+-------------------+----------+\\n|      Cilium       | 1.15.500 |\\n+-------------------+----------+\\n\\n```\\n\\n## Prerequisites\\n\\n### Rancher Server\\n\\nWe do not concentrate on installing Rancher. If you are not sure how to install Rancher, have a look at the official documentation [here](https://ranchermanager.docs.rancher.com/getting-started/quick-start-guides) or go through the guide I created a couple of weeks back [here](https://medium.com/@eleni.grosdouli/rancher-on-eks-with-nginx-ingress-and-lets-encrypt-4f041fc1adae). \\n\\n### Azure Free Credits\\n\\nFor this demonstration, we will use the Azure [free credits](https://azure.microsoft.com/en-us/free) offering. The approach taken is more than enough to give readers a free and good understanding of how to set up the Azure cloud environment to perform RKE2 deployments with Rancher.\\n\\nEnsure the below are satisfied.\\n\\n1. Helm CLI installed (Optional Step)\\n1. kubectl installed\\n\\n## Set up Azure Cloud Environment\\n\\nIn this section, we will provide readers with all the needed guidance on setting up their environment for the RKE2 deployment\\n\\n### Retrieve the Tenant ID\\n\\nThe `Tenant ID` identifies which Azure Active Directory (AD) instance the application sits under. To retrieve the Tenant ID, follow the steps below.\\n\\n1. Login to Azure [portal](https://portal.azure.com/)\\n1. Navigate to **Home**\\n1. Search for `Microsoft Entra ID`\\n  ![title image reading \\"Microsoft Entra ID\\"](microsoft_entra_id.png)\\n1. Navigate to **Manage > Properties**\\n1. Grab the `Tenant ID` once the new windows appear\\n\\n### Create a Rancher App Registration\\n\\nAzure App Registration is the process of registering an application with Azure AD.\\n\\n1. Access the Azure [portal](https://portal.azure.com/) \\n1. Navigate to **Home > App registrations > + New Registration**\\n1. Choose the below details:\\n    - `Name`: What is the name your application will have\\n    - `Supported account types`: In my case, I chose the first one, \\"Accounts in this organizational directory only (Default Directory only - Single tenant)\\"\\n    - `Redirect URI (optional)`: set `Web` and leave the `Sign-on URL` empty or add your own URI\\n1. Click the **Register** button to create the application\\n\\n    ![title image reading \\"App Registration\\"](app_registration.png)\\n\\n### Create an Azure Client Secret for App Registration\\n\\n1. Access the Azure [portal](https://portal.azure.com/)  \\n1. Navigate to **Home > App registrations > App name**\\n1. Navigate to **Manage > Certificates & secrets**\\n  ![title image reading \\"Client Secret\\"](client_secret.png)\\n1. Click on the **+ New client secret**\\n1. Provide a `description` and an `expiry date`\\n1. Click **+ Add**\\n1. Copy the `Value` and proceed with the configuraition\\n\\n### Create App Registration Permissions\\n\\n1. Access the Azure [portal](https://portal.azure.com/) \\n1. Navigate to **Home > Subscriptions > Your subscription name > Access Control (IAM)**\\n    ![title image reading \\"IAM\\"](iam.png)\\n1. Click on the **+ Add > Add role assigment**\\n    ![title image reading \\"IAM\\"](add_role_assigment.png)\\n1. Open the `Privileged administrator roles` tab\\n1. For Role, select `Contributor`\\n1. Click on **Next**\\n1. Members and **+ Select members** and then choose or type for `Rancher`. If your application name is something else, provide the application name created in a previous step \\n1. **Review + assing**\\n1. Proceed with the creation\\n\\n## Azure Free Account Limitations\\n\\nFind below some of the limitations spotted with the `free-credit` subscription.\\n\\n1. You cannot have more than **one** resource pools\\n1. You cannot have more than **3** Public IP addresses\\n1. You cannot create a resource pool with a `VM Size` **greater than** `Standard_D2_v2`\\n\\n## Set up Rancher Cloud Credentials\\n\\nOnce we have the Azure environment ready, we can move on with Rancher. The first thing we have to do is to create `Azure Cloud Credentials`. The cloud credentials will be used to provision clusters or can be used in other node templates.\\n\\n1. Login to Rancher\\n1. Navigate to **Home > Cluster management ![title image reading \\"Cluster Management\\"](cluster_management.png) > Cloud Credentials > Create > Choose Azure**\\n1. Provide a `name`, `tenant ID` (**Home > Subscriptions**), `clientID` (**Home > App registrations > Rancher > copy the Application (client) ID**), `client secret` (**Home > App registrations > App name > manage > certificates & secrets**)\\n1. Click the **\\"Create\\"** button and ensure no error appears on the screen\\n\\n\\n## Create an RKE2 cluster with Cilium\\n\\nIt is time to use the Azure cloud credentials to create an RKE2 cluster on Azure with Cilium.\\n\\n1. Login to the **Rancher UI**\\n1. Navigiate to **Home > Cluster management ![title image reading \\"Cluster Management\\"](cluster_management.png) > Create > ensure RKE2 is selected > Choose Azure**\\n1. The `Cloud Credentials` field will get populated automatically. Fill out the details below.\\n    - `Cluster Name`: Set a cluster name\\n    - `Cluster Description`: Set a cluster description\\n    - Set the `Machine Pools` details. This reflects the nodes we will have on the cluster and their specified role (controller or worker nodes).\\n      - `Pool Name`: Left the default\\n      - `Machine Count`: Set to 2 due to limitations\\n      - `Location`: Choose your favourable location\\n        ![title image reading \\"Azure RKE2 Cluster Page 01\\"](azure_rke2_page01.png)\\n    - Continue with the **Cluster Configuration > Basics**\\n      - `Kubernetes Version`: Define the preferred Kubernetes version\\n      - `Container Network`: Choose **Cilium**\\n        ![title image reading \\"Azure RKE2 Cluster Page 02\\"](azure_rke2_page02.png)\\n        :::tip\\n        We can leave the rest of the configuration as default. However, if we want to enable Cilium with `kube-proxy` replacement, we can update the cluster by editing the YAML configuration instead. This can be done by clicking the `Edit as YAML` button at the bottom right-hand side.\\n        :::\\n    - Continue with the **Cluster Configuration > Advanced**\\n      - `Additional Controller Manager Args`: Set `--configure-cloud-routes=false`\\n        ![title image reading \\"Azure RKE2 Cluster Page 03\\"](azure_rke2_page03.png)\\n1. Click **\\"Save\\"**\\n\\n:::note\\nThe cluster creation might take up to 20 minutes. Be patient as a couple of resources and Azure items are created for this cluster.\\n\\nAfter 20 minutes:\\n  ![title image reading \\"Azure RKE2 Cluster Page Success\\"](azure_rke2_success.png)\\n:::\\n\\n\\n## RKE2 Cluster Validation\\n\\nNow that the RKE2 cluster is up and running, let\'s perform some validation steps to ensure the cluster is working as expected.\\n\\n```bash\\n$ kubectl get nodes -o wide\\nNAME                          STATUS   ROLES                              AGE   VERSION           INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\\ntest03-pool1-cff6be83-jfnrw   Ready    control-plane,etcd,master,worker   50m   v1.28.11+rke2r1   192.168.0.5   <none>        Ubuntu 18.04.6 LTS   5.4.0-1109-azure   containerd://1.7.17-k3s1\\ntest03-pool1-cff6be83-wml94   Ready    control-plane,etcd,master,worker   56m   v1.28.11+rke2r1   192.168.0.4   <none>        Ubuntu 18.04.6 LTS   5.4.0-1109-azure   containerd://1.7.17-k3s1\\n\\n$ kubectl get pods -n kube-system\\nNAME                                                   READY   STATUS      RESTARTS      AGE\\ncilium-4wzld                                           1/1     Running     0             49m\\ncilium-7st9q                                           1/1     Running     0             55m\\ncilium-operator-695b4bfc8b-rrdpw                       1/1     Running     3 (46m ago)   55m\\ncilium-operator-695b4bfc8b-wvswc                       1/1     Running     5 (44m ago)   55m\\ncloud-controller-manager-test03-pool1-cff6be83-jfnrw   1/1     Running     4 (43m ago)   49m\\ncloud-controller-manager-test03-pool1-cff6be83-wml94   1/1     Running     5 (44m ago)   55m\\netcd-test03-pool1-cff6be83-jfnrw                       1/1     Running     0             49m\\netcd-test03-pool1-cff6be83-wml94                       1/1     Running     0             55m\\nhelm-install-rke2-cilium-4g9m6                         0/1     Completed   0             56m\\nhelm-install-rke2-coredns-4fcl8                        0/1     Completed   0             56m\\nhelm-install-rke2-ingress-nginx-sllhl                  0/1     Completed   0             56m\\nhelm-install-rke2-metrics-server-djld4                 0/1     Completed   0             56m\\nhelm-install-rke2-snapshot-controller-6vktm            0/1     Completed   0             56m\\nhelm-install-rke2-snapshot-controller-crd-fmr6n        0/1     Completed   0             56m\\nhelm-install-rke2-snapshot-validation-webhook-tj4b8    0/1     Completed   0             56m\\nkube-apiserver-test03-pool1-cff6be83-jfnrw             1/1     Running     1 (43m ago)   49m\\nkube-apiserver-test03-pool1-cff6be83-wml94             1/1     Running     1             43m\\nkube-controller-manager-test03-pool1-cff6be83-jfnrw    1/1     Running     3 (46m ago)   49m\\nkube-controller-manager-test03-pool1-cff6be83-wml94    1/1     Running     1 (44m ago)   46m\\nkube-proxy-test03-pool1-cff6be83-jfnrw                 1/1     Running     0             49m\\nkube-proxy-test03-pool1-cff6be83-wml94                 1/1     Running     0             55m\\nkube-scheduler-test03-pool1-cff6be83-jfnrw             1/1     Running     3 (46m ago)   49m\\nkube-scheduler-test03-pool1-cff6be83-wml94             1/1     Running     1 (44m ago)   46m\\nrke2-coredns-rke2-coredns-84b9cb946c-bd8p6             1/1     Running     0             49m\\nrke2-coredns-rke2-coredns-84b9cb946c-npgl2             1/1     Running     0             55m\\nrke2-coredns-rke2-coredns-autoscaler-b49765765-lp244   1/1     Running     0             55m\\nrke2-ingress-nginx-controller-qk5mc                    1/1     Running     0             53m\\nrke2-ingress-nginx-controller-wbdf8                    1/1     Running     0             48m\\nrke2-metrics-server-655477f655-j9vzw                   1/1     Running     0             54m\\nrke2-snapshot-controller-59cc9cd8f4-8fhwb              1/1     Running     6 (44m ago)   54m\\nrke2-snapshot-validation-webhook-54c5989b65-9vb9w      1/1     Running     0             54m\\n\\n$ kubectl get pods,svc -n cattle-system\\nNAME                                             READY   STATUS      RESTARTS      AGE\\npod/cattle-cluster-agent-59df97fc7f-44q74        1/1     Running     2 (43m ago)   46m\\npod/cattle-cluster-agent-59df97fc7f-vbvbb        1/1     Running     0             45m\\npod/helm-operation-4clvk                         0/2     Completed   0             50m\\npod/helm-operation-89qsx                         0/2     Completed   0             51m\\npod/rancher-webhook-d677765b4-6vrkh              1/1     Running     1 (44m ago)   44m\\npod/system-upgrade-controller-6f86d6d4df-jvd9k   1/1     Running     0             51m\\n\\nNAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\\nservice/cattle-cluster-agent   ClusterIP   10.43.82.34    <none>        80/TCP,443/TCP   56m\\nservice/rancher-webhook        ClusterIP   10.43.80.125   <none>        443/TCP          51m\\n```\\n\\n```bash\\n$ kubectl exec -it ds/cilium -n kube-system -- cilium status\\nDefaulted container \\"cilium-agent\\" out of: cilium-agent, install-portmap-cni-plugin (init), config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init)\\nKVStore:                 Ok   Disabled\\nKubernetes:              Ok   1.28 (v1.28.11+rke2r1) [linux/amd64]\\nKubernetes APIs:         [\\"EndpointSliceOrEndpoint\\", \\"cilium/v2::CiliumClusterwideNetworkPolicy\\", \\"cilium/v2::CiliumEndpoint\\", \\"cilium/v2::CiliumNetworkPolicy\\", \\"cilium/v2::CiliumNode\\", \\"cilium/v2alpha1::CiliumCIDRGroup\\", \\"core/v1::Namespace\\", \\"core/v1::Pods\\", \\"core/v1::Service\\", \\"networking.k8s.io/v1::NetworkPolicy\\"]\\nKubeProxyReplacement:    False   [eth0   192.168.0.5 fe80::6245:bdff:fe94:cd06]\\nHost firewall:           Disabled\\nSRv6:                    Disabled\\nCNI Chaining:            portmap\\nCNI Config file:         successfully wrote CNI configuration file to /host/etc/cni/net.d/05-cilium.conflist\\nCilium:                  Ok   1.15.5 (v1.15.5-8c7e442c)\\nNodeMonitor:             Disabled\\nCilium health daemon:    Ok   \\nIPAM:                    IPv4: 9/254 allocated from 10.42.1.0/24, \\nIPv4 BIG TCP:            Disabled\\nIPv6 BIG TCP:            Disabled\\nBandwidthManager:        Disabled\\nHost Routing:            Legacy\\nMasquerading:            IPTables [IPv4: Enabled, IPv6: Disabled]\\nController Status:       52/52 healthy\\nProxy Status:            OK, ip 10.42.1.172, 0 redirects active on ports 10000-20000, Envoy: embedded\\nGlobal Identity Range:   min 256, max 65535\\nHubble:                  Disabled\\nEncryption:              Disabled        \\nCluster health:          2/2 reachable   (2024-07-26T09:19:17Z)\\nModules Health:          Stopped(0) Degraded(0) OK(11)\\n\\n```\\n\\n## \u2709\ufe0f Contact\\n\\nIf you have any questions, feel free to get in touch! You can use the `Discussions` option found [here](https://github.com/egrosdou01/personal-blog/discussions) or reach out to me on any of the social media platforms provided. \ud83d\ude0a\\n\\nWe look forward to hearing from you!\\n\\n## Conclusions\\n\\nThis is it! We performed an RKE2 cluster with Cilium using the Rancher UI in just a few clicks. \ud83c\udf89 In an upcoming blog post, we will demonstrate how to perform the same with either [Terraform](https://www.terraform.io/) or [OpenTofu](https://opentofu.org/)!\\n\\nIt\'s a wrap for this post! \ud83c\udf89 Thanks for reading! Stay tuned for more exciting updates!"},{"id":"cilium-cluster-mesh-rke2","metadata":{"permalink":"/personal-blog/blog/cilium-cluster-mesh-rke2","source":"@site/blog/2024-07-18-rke2-cilium/cilium-cluster-mesh-rke2.md","title":"Cilium Cluster Mesh on RKE2","description":"Introduction","date":"2024-07-18T00:00:00.000Z","tags":[{"inline":false,"label":"Cilium","permalink":"/personal-blog/blog/tags/cilium","description":"eBPF-based Networking, Security, and Observability for Kubernetes"},{"inline":false,"label":"RKE2","permalink":"/personal-blog/blog/tags/rke2","description":"Rancher Kubernetes Engine 2 (RKE2)"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":10.415,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"cilium-cluster-mesh-rke2","title":"Cilium Cluster Mesh on RKE2","authors":["egrosdou01"],"date":"2024-07-18T00:00:00.000Z","tags":["cilium","rke2","open-source","kubernetes","gitops","devops","2024"]},"unlisted":false,"prevItem":{"title":"Rancher RKE2 Cluster on Azure","permalink":"/personal-blog/blog/rancher-rke2-cilium-azure"},"nextItem":{"title":"Cilium on EKS with Sveltos","permalink":"/personal-blog/blog/cilium-eks-sveltos"}},"content":"## Introduction\\n\\nAfter spending some time working with the on-prem [RKE2](https://docs.rke2.io/) lab setup, I came to notice a couple of issues while forming in an automated fashion the [Cilium cluster mesh](https://docs.cilium.io/en/stable/network/clustermesh/clustermesh/) between on-prem clusters.\\n\\nIn today\'s post, we will go through the step-by-step process of forming a **Cilium Cluster Mesh** and explain any issues that might have arisen by following the **GitOps** approach. The cilium CLI will not be required. The deployment will be performed primarily via `Helm` and `kubectl`.\\n\x3c!--truncate--\x3e\\n\\nAdditionally, we will use the [shared CA](https://docs.cilium.io/en/v1.15/network/clustermesh/clustermesh/#shared-certificate-authority) (Certificate Authority) approach as this is a convenient way to form a cluster mesh in an automated fashion and also the best practise for the Hubble Relay setup. The approach will enable **mTLS** across clusters.\\n\\n## Lab Setup\\n\\n```bash\\n+-----------------+----------------------+----------------------+\\n|   Cluster Name  |        Type          |       Version        |\\n+-----------------+----------------------+----------------------+\\n|   mesh01        | RKE2 managed cluster | RKE2 v1.27.14+rke2r1 |\\n|   mesh02        | RKE2 managed cluster | RKE2 v1.27.14+rke2r1 |\\n+-----------------+----------------------+----------------------+\\n\\n+-------------------+----------+\\n|    Deployment     | Version  |\\n+-------------------+----------+\\n| Rancher2 Provider |  4.2.0   |\\n|     Cilium        | 1.15.500 |\\n+-------------------+----------+\\n```\\n\\n## Prerequisites\\n\\n### Infrastructure\\n\\nFor this demonstration, we assume readers have at least two RKE2 clusters up and running. In our case, to create an RKE2 cluster on-prem we used the [Rancher2](https://registry.terraform.io/providers/rancher/rancher2/latest/docs) Terraform provider. The provider allows users to create different resources across different platforms alongside defining information for the RKE2 deployment like IP Address handling, and CNI (Container Network Interface) custom configuration.\\n\\n### Cilium Cluster Mesh\\n\\n- The **Cluster Name** and the **Cluster ID** must be unique.\\n- The **Pods** and the **Services CIDR** ranges must be unique across all the Kubernetes Clusters. The pods need to communicate over a unique IP address. See the IP address schema table above.\\n- Node CIDRs must be unique. The Nodes to have IP connectivity.\\n- The Cilium pods must connect to the `ClusterMesh API Server` service exposed on every Kubernetes cluster.\\n\\n### Resources\\n\\nEnsure the below are satisfied.\\n\\n1. Helm CLI installed\\n1. kubectl installed\\n\\n## Step 0: RKE2 Terraform Provider\\n\\nThe below snippet is an example configuration on how to deploy an RKE2 cluster via the **Rancher2** Provider.\\n\\n```terraform\\n  # RKE2 configuration\\n  resource \\"rancher2_cluster_v2\\" \\"rke2\\" {\\n    # Define basic cluster details like labels and annotations\\n    annotations           = var.rancher_env.cluster_annotations\\n    kubernetes_version    = var.rancher_env.rke2_version\\n    labels                = var.rancher_env.cluster_labels\\n    enable_network_policy = var.rancher_env.network_policy # Option to enable or disable Project Network Isolation.\\n    name                  = var.rancher_env.cluster_id\\n      \\n      # Define the Cilium Configuration for the cluster\\n      chart_values = <<-EOF\\n        rke2-cilium:\\n          k8sServiceHost: 127.0.0.1\\n          k8sServicePort: 6443\\n          kubeProxyReplacement: true # Prepare the deployment for kube-proxy replacement\\n          operator:\\n            replicas: 1\\n          hubble: # Enable Hubble for observability \\n            enabled: true\\n            peerService:\\n              clusterDomain: cluster.local\\n            relay:\\n              enabled: true\\n            tls:\\n              auto:\\n                certValidityDuration: 1095\\n                enabled: true\\n                method: helm\\n            ui:\\n              enabled: true\\n        EOF\\n      \\n      # Apply machine global settings for the clusters\\n      machine_global_config = <<EOF\\n        cni: \\"cilium\\" # Enable Cilium CNI for every cluster\\n        cluster-cidr: ${var.rke_cluster_cidr}\\n        service-cidr: ${var.rke_service_cidr}\\n        disable-kube-proxy: true # Disable kube-proxy\\n        etcd-expose-metrics: false # Do not expose the etcd metrics\\n        EOF\\n      \\n      # Start building the controller and workder nodes dynamically\\n      dynamic \\"machine_pools\\" {\\n        for_each = var.node\\n        content {\\n          cloud_credential_secret_name = data.rancher2_cloud_credential.auth.id\\n          control_plane_role           = machine_pools.key == \\"ctl_plane\\" ? true : false\\n          etcd_role                    = machine_pools.key == \\"ctl_plane\\" ? true : false\\n          name                         = machine_pools.value.name\\n          quantity                     = machine_pools.value.quantity\\n          worker_role                  = machine_pools.key != \\"ctl_plane\\" ? true : false\\n\\n          machine_config {\\n            kind = rancher2_machine_config_v2.nodes[machine_pools.key].kind\\n            name = replace(rancher2_machine_config_v2.nodes[machine_pools.key].name, \\"_\\", \\"-\\")\\n          }\\n        }\\n      }\\n      machine_selector_config {\\n        config = null\\n      }\\n    }\\n  }\\n\\n```\\nAs the focus here is more on the **Cilium Cluster Mesh** setup, we will not go into much detail about the Terraform RKE2 deployment. If there is demand for an in-depth blog post about Terraform RKE2 deployments, feel free to get in touch.\\n\\n## Step 1: Export kubeconfig\\nEither from the Terraform execution plan or via the Rancher UI, collect the kubeconfig of the RKE2 clusters. Alternatively, we can `SSH` into one of the RKE2 master nodes and collect the `kubeconfig` found in the directory `/etc/rancher/rke2/rke2.yaml`.\\n\\n```bash\\n$ export KUBECONFIG=<directory of kubeconfig>\\n$ kubectl nodes\\n```\\n\\n## Step 2: Helm list and values export\\nRKE2 comes with its own Cilium CNI Helm chart. That means RKE2 clusters will have an RKE2 Cilium Helm chart deployment in the `kube-system` namespace.\\n\\n### Validate\\n\\n```bash\\n$ export KUBECONFIG=<directory of kubeconfig>\\n$ helm list -n kube-system\\n\\nNAME                            \\tNAMESPACE  \\tREVISION\\tUPDATED                                \\tSTATUS  \\tCHART                                       \\tAPP VERSION\\nrke2-cilium                     \\tkube-system\\t1       \\t2024-07-13 09:32:09.981662 +0200 CEST  \\tdeployed\\trke2-cilium-1.15.500                        \\t1.15.5     \\nrke2-coredns                    \\tkube-system\\t1       \\t2024-07-13 07:05:49.846980773 +0000 UTC\\tdeployed\\trke2-coredns-1.29.002                       \\t1.11.1     \\nrke2-ingress-nginx              \\tkube-system\\t1       \\t2024-07-13 07:06:24.63272854 +0000 UTC \\tdeployed\\trke2-ingress-nginx-4.8.200                  \\t1.9.3      \\nrke2-metrics-server             \\tkube-system\\t1       \\t2024-07-13 07:06:24.86243331 +0000 UTC \\tdeployed\\trke2-metrics-server-2.11.100-build2023051513\\t0.6.3      \\nrke2-snapshot-controller        \\tkube-system\\t1       \\t2024-07-13 07:06:26.764326178 +0000 UTC\\tdeployed\\trke2-snapshot-controller-1.7.202            \\tv6.2.1     \\nrke2-snapshot-controller-crd    \\tkube-system\\t1       \\t2024-07-13 07:06:24.217899546 +0000 UTC\\tdeployed\\trke2-snapshot-controller-crd-1.7.202        \\tv6.2.1     \\nrke2-snapshot-validation-webhook\\tkube-system\\t1       \\t2024-07-13 07:06:24.544748567 +0000 UTC\\tdeployed\\trke2-snapshot-validation-webhook-1.7.302    \\tv6.2.2 \\n```\\n\\n### Collect rke2-cilium Helm Values\\n\\n**mesh01**\\n```bash\\n$ helm get values rke2-cilium -n kube-system -o yaml > values_mesh01.yaml\\n```\\n\\n**mesh02**\\n```bash\\n$ helm get values rke2-cilium -n kube-system -o yaml > values_mesh02.yaml\\n```\\n\\n**Example values_mesh01.yaml**\\n\\n```yaml\\nglobal:\\n  cattle:\\n    clusterId: c-m-8ffz659l\\n  clusterCIDR: 10.244.0.0/16\\n  clusterCIDRv4: 10.244.0.0/16\\n  clusterDNS: 10.96.0.10\\n  clusterDomain: cluster.local\\n  rke2DataDir: /var/lib/rancher/rke2\\n  serviceCIDR: 10.96.0.0/18\\nhubble:\\n  enabled: true\\n  peerService:\\n    clusterDomain: cluster.local\\n  relay:\\n    enabled: true\\n  tls:\\n    auto:\\n      certValidityDuration: 1095\\n      enabled: true\\n      method: helm\\n  ui:\\n    enabled: true\\nk8sServiceHost: 127.0.0.1\\nk8sServicePort: 6443\\nkubeProxyReplacement: true\\noperator:\\n  replicas: 1\\n```\\n\\n:::note\\nThe configuration comes from the `machine_global_config` and `chart_values` sections defined in the Terraform code found in [Step 0](#step-0-rke2-terraform-provider).\\n:::\\n\\n## Step 3: Cilium Cluster Mesh Helm Values\\n\\nTo set up the Cilium cluster mesh, we need to include the `rke2-charts` repo and later on, update the Helm values with the required cluster mesh settings. For this demonstration, we will use the `NodePort` deployment. For production environments, a `LoadBalancer` deployment is recommended as we do not have to rely on Node availability.\\n\\n### Add rke2-charts Repo\\n\\nThe action should be performed in both clusters.\\n\\n```bash\\n$ helm repo add rke2-charts https://rke2-charts.rancher.io/\\n$ helm repo update\\n```\\n\\n### Update mesh01 Helm Values\\n\\nOn the same level as global, add the below configuration.\\n\\n```yaml\\ntls:\\n  ca:\\n    cert: \\"\\" # Base64 encoded shared CA crt\\n    key: \\"\\" # Base64 encoded shared CA key\\ncluster:\\n  name: mesh01 # The unique name of the cluster\\n  id: 1 # The unique ID of the cluster used for the cluster mesh formation\\nclustermesh:\\n  apiserver:\\n    replicas: 2\\n    service:\\n      type: NodePort # Set the Clustermesh API service to be of type NodePort. Not recommended for Production environments\\n      nodePort: 32379 # Define the listening port for the Clustermesh API service\\n    tls:\\n      authMode: cluster\\n      server:\\n        extraDnsNames:\\n          - \\"mesh01.mesh.cilium.io\\" # Define the extra DNS\\n  config:\\n    clusters:\\n    - address: \\"\\"\\n      ips:\\n      - <Node IP> # The Node IP of the mesh02 cluster\\n      name: mesh02\\n      port: 32380 # The NodePort defined on mesh02 for the Clustermesh API service\\n    enabled: true\\n    domain: \\"mesh.cilium.io\\" # Define the default domain for the mesh\\n  useAPIServer: true # Enable the Clustermesh API deployment\\n```\\n\\n### Update mesh02 Helm Values\\n\\nOn the same level as global, add the below configuration.\\n\\n```yaml\\ntls:\\n  ca:\\n    cert: \\"\\" # Base64 encoded shared CA crt\\n    key: \\"\\" # Base64 encoded shared CA key\\ncluster:\\n  name: mesh02 # The unique name of the cluster\\n  id: 2 # The unique ID of the cluster used for the cluster mesh formation\\nclustermesh:\\n  apiserver:\\n    replicas: 2\\n    service:\\n      type: NodePort # Set the Clustermesh API service to be of type NodePort. Not recommended for production environments\\n      nodePort: 32380 # Define the listening port for the Clustermesh API service\\n    tls:\\n      authMode: cluster\\n      server:\\n        extraDnsNames:\\n          - \\"mesh02.mesh.cilium.io\\" # Define the extra DNS\\n  config:\\n    clusters:\\n    - address: \\"\\"\\n      ips:\\n      - <Node IP> # The Node IP of the mesg01 cluster\\n      name: mesh01 # Define the name of the cluster\\n      port: 32379 # The NodePort defined on mesh02 for the Clustermesh API service\\n    enabled: true\\n    domain: \\"mesh.cilium.io\\" # Define the default domain for the mesh\\n  useAPIServer: true # Enable the Clustermesh API deployment\\n```\\n\\n### Update mesh01/mesh02 Helm deployment\\n\\nTo ensure the updated Helm values are applied, we will use the HELM CLI to update the `rke2-cilium` deployment.\\n\\n```bash\\n$ helm upgrade rke2-cilium rke2-charts/rke2-cilium --version 1.15.500 --namespace kube-system -f values_mesh01.yaml\\n\\n$ helm list -n kube-system\\n```\\n\\nPerform the commands for the `mesh02` cluster.\\n\\n:::tip\\nThe `helm upgrade` command will create a new revision of the `rke2-cilium` application and show if the update was successful or not. Additionally, the cilium daemonset will get restarted and the Clustermesh API deployment will get created. Execute the commands below to double-check the update action.\\n\\n```bash\\n$ kubectl rollout status daemonset cilium -n kube-system\\n\\n$ kubectl get pods,svc -n kube-system | grep -i clustermesh\\n```\\n:::\\n\\n\\n## Step 4: Validate Cilium Cluster Mesh\\n\\nAs we do not use the Cilium CLI, to ensure the Cilium cluster mesh works as expected, we will exec into the cilium daemonset and check the required details.\\n\\n```bash\\n$ kubectl get ds -n kube-system | grep -i cilium\\ncilium                          4         4         4       4            4           kubernetes.io/os=linux   7d6h\\n```\\n\\n### On mesh01 and mesh02\\n\\n```bash\\n$ kubectl exec -it ds/cilium -n kube-system -- cilium status | grep -i clustermesh\\n\\nDefaulted container \\"cilium-agent\\" out of: cilium-agent, install-portmap-cni-plugin (init), config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init)\\nClusterMesh:             1/1 clusters ready, 11 global-services\\n```\\n\\nOn both sides, the `ClusterMesh` should point to `1/1 clusters ready`.\\n\\n```bash\\n$ kubectl exec -it ds/cilium -n kube-system -- cilium-health status               \\nDefaulted container \\"cilium-agent\\" out of: cilium-agent, install-portmap-cni-plugin (init), config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init)\\nProbe time:   2024-07-20T13:58:47Z\\nNodes:\\n  mesh01/mesh01-controller-3d16581b-7q5bj (localhost):\\n    Host connectivity to x.x.x.x:\\n      ICMP to stack:   OK, RTT=693.829\xb5s\\n      HTTP to agent:   OK, RTT=118.583\xb5s\\n    Endpoint connectivity to 10.244.1.71:\\n      ICMP to stack:   OK, RTT=688.411\xb5s\\n      HTTP to agent:   OK, RTT=251.927\xb5s\\n  mesh01/mesh01-controller-3d16581b-v58rq:\\n    Host connectivity to x.x.x.x:\\n      ICMP to stack:   OK, RTT=671.007\xb5s\\n      HTTP to agent:   OK, RTT=237.395\xb5s\\n    Endpoint connectivity to 10.244.0.75:\\n      ICMP to stack:   OK, RTT=702.976\xb5s\\n      HTTP to agent:   OK, RTT=342.115\xb5s\\n  mesh01/mesh01-worker-7ced0c6c-lz9sp:\\n    Host connectivity to x.x.x.x:\\n      ICMP to stack:   OK, RTT=819.21\xb5s\\n      HTTP to agent:   OK, RTT=397.398\xb5s\\n    Endpoint connectivity to 10.244.3.215:\\n      ICMP to stack:   OK, RTT=821.223\xb5s\\n      HTTP to agent:   OK, RTT=465.965\xb5s\\n  mesh01/mesh01-worker-7ced0c6c-w294x:\\n    Host connectivity to x.x.x.x:\\n      ICMP to stack:   OK, RTT=738.787\xb5s\\n      HTTP to agent:   OK, RTT=335.803\xb5s\\n    Endpoint connectivity to 10.244.2.36:\\n      ICMP to stack:   OK, RTT=693.326\xb5s\\n      HTTP to agent:   OK, RTT=426.571\xb5s\\n  mesh02/mesh02-controller-52d8e160-b27rn:\\n    Host connectivity to x.x.x.x:\\n      ICMP to stack:   OK, RTT=683.278\xb5s\\n      HTTP to agent:   OK, RTT=335.076\xb5s\\n    Endpoint connectivity to 10.245.0.106:\\n      ICMP to stack:   OK, RTT=818.386\xb5s\\n      HTTP to agent:   OK, RTT=387.314\xb5s\\n  mesh02/mesh02-controller-52d8e160-q4rvf:\\n    Host connectivity to x.x.x.x:\\n      ICMP to stack:   OK, RTT=683.097\xb5s\\n      HTTP to agent:   OK, RTT=301.448\xb5s\\n    Endpoint connectivity to 10.245.1.75:\\n      ICMP to stack:   OK, RTT=748.101\xb5s\\n      HTTP to agent:   OK, RTT=510.124\xb5s\\n  mesh02/mesh02-worker-a1c14ae0-5l759:\\n    Host connectivity to x.x.x.x:\\n      ICMP to stack:   OK, RTT=631.954\xb5s\\n      HTTP to agent:   OK, RTT=266.391\xb5s\\n    Endpoint connectivity to 10.245.3.232:\\n      ICMP to stack:   OK, RTT=751.853\xb5s\\n      HTTP to agent:   OK, RTT=433.049\xb5s\\n  mesh02/mesh02-worker-a1c14ae0-c7tcb:\\n    Host connectivity to x.x.x.x:\\n      ICMP to stack:   OK, RTT=671.823\xb5s\\n      HTTP to agent:   OK, RTT=365.949\xb5s\\n    Endpoint connectivity to 10.245.2.69:\\n      ICMP to stack:   OK, RTT=690.894\xb5s\\n      HTTP to agent:   OK, RTT=466.73\xb5s\\n```\\n\\n:::note\\nWith the cilium-health status command, you should be able to see all the nodes from both clusters. Check the ICMP and HTTP status. Should be \\"OK\\".\\n\\nAlso, it might take a couple of minutes till the cilium-health status is available.\\n\\nIf the time-out persists, have a look at the firewall rules and whether traffic between the clusters is allowed.\\n:::\\n\\n:::warning\\nThe NodePort IP addresses set for the cluster mesh need to be the IP addresses of the worker node instead of the master node. If they are the master node, the Cilium Cluster Mesh will not get deployed and we will get the below error.\\n```\\nremote-etcd-cluster01                                                             4m25s ago      4s ago       22      failed to detect whether the cluster configuration is required: etcdserver: permission denied \\n```\\n:::\\n\\n## Step 5: Hubble UI\\n\\nTo work with the Hubble UI we can use the `kubectl port-forward` of the Hubble UI service or update the existing `rke2-cilium` deployment on one of the nodes and expose the Hubble UI as a `NodePort` service. Just include the below in the `values_mesh01.yaml` or the `values_mesh02.yaml` file.\\n\\n```yaml\\n  ui:\\n    enabled: true\\n    service:\\n      type: NodePort\\n```\\n\\nFor more information about the RKE2 Cilium Helm Chart values, have a look [here](https://artifacthub.io/packages/helm/rke2-charts/rke2-cilium/1.15.500).\\n\\n## \u2709\ufe0f Contact\\n\\nIf you have any questions, feel free to get in touch! You can use the `Discussions` option found [here](https://github.com/egrosdou01/personal-blog/discussions) or reach out to me on any of the social media platforms provided. \ud83d\ude0a\\n\\nWe look forward to hearing from you!\\n\\n## Conclusions\\n\\nThis is it! We performed a Cilium cluster mesh between two on-prem RKE2 clusters in just a few steps! \ud83c\udf89\\n\\nIt\'s a wrap for this post! \ud83c\udf89 Thanks for reading! Stay tuned for more exciting updates!"},{"id":"cilium-eks-sveltos","metadata":{"permalink":"/personal-blog/blog/cilium-eks-sveltos","source":"@site/blog/2024-07-15-welcome/cilium-eks-sveltos.md","title":"Cilium on EKS with Sveltos","description":"Introduction","date":"2024-07-15T00:00:00.000Z","tags":[{"inline":false,"label":"Sveltos","permalink":"/personal-blog/blog/tags/sveltos","description":"Open source project Sveltos"},{"inline":false,"label":"Cilium","permalink":"/personal-blog/blog/tags/cilium","description":"eBPF-based Networking, Security, and Observability for Kubernetes"},{"inline":false,"label":"Open Source","permalink":"/personal-blog/blog/tags/open-source","description":"Open source software"},{"inline":false,"label":"Kubernetes","permalink":"/personal-blog/blog/tags/kubernetes","description":"Container orchestration platform for automating application deployment, scaling, and management"},{"inline":false,"label":"GitOps","permalink":"/personal-blog/blog/tags/gitops","description":"Operational framework that uses Git as a single source of truth for declarative infrastructure and applications"},{"inline":false,"label":"DevOps","permalink":"/personal-blog/blog/tags/devops","description":"Set of practices that combines software development and IT operations"},{"inline":false,"label":"2024","permalink":"/personal-blog/blog/tags/2024","description":"The year the post went online"}],"readingTime":8.29,"hasTruncateMarker":true,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"cilium-eks-sveltos","title":"Cilium on EKS with Sveltos","authors":["egrosdou01"],"date":"2024-07-15T00:00:00.000Z","image":"./cilium_sveltos_eks.jpg","tags":["sveltos","cilium","open-source","kubernetes","gitops","devops","2024"]},"unlisted":false,"prevItem":{"title":"Cilium Cluster Mesh on RKE2","permalink":"/personal-blog/blog/cilium-cluster-mesh-rke2"},"nextItem":{"title":"Welcome","permalink":"/personal-blog/blog/welcome"}},"content":"## Introduction\\n\\nIn today\'s blog post, we will demonstrate an easy way of deploying and controlling [Cilium](https://docs.cilium.io/en/v1.14/) on an [EKS](https://aws.amazon.com/eks/) cluster with [Sveltos](https://github.com/projectsveltos). \\n\\nAs the majority of the documentation out there provides a step-by-step installation directly with the Helm chart commands, we decided to demonstrate a different approach, the GitOps approach, with the use of [Sveltos ClusterProfile](https://projectsveltos.github.io/sveltos/addons/addons/) CRD (Custom Resource Definition).\\n\\n![title image reading \\"Cilium on EKS with Sveltos Diagram\\"](cilium_sveltos_eks.jpg)\\n\\n\x3c!--truncate--\x3e\\n\\nWe will utilise the Terraform [AWS EKS module](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest) to create an EKS cluster. Once the cluster is up and running, we will register it with Sveltos. Then, we will update the [`aws-core` daemonset](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html) to support  ENI mode and remove the `kube-proxy` Kubernetes resources as Cilium will take over.\\n\\n## Lab Setup\\n\\n```bash\\n+-----------------+-------------------+--------------------------+\\n|   Cluster Name  |        Type       |         Version          |\\n+-----------------+-------------------+--------------------------+\\n|   mgmt          | Management Cluster| RKE2 v1.28.9+rke2r1      |\\n| eks-test01      | Managed Cluster   | EKS v1.28.10-eks-49c6de4 |\\n+-----------------+-------------------+--------------------------+\\n\\n+-------------+----------+\\n|  Deployment | Version  |\\n+-------------+----------+\\n|    Cilium   | v1.14.8  |\\n|  sveltosctl | v0.27.0  |\\n+-------------+----------+\\n```\\n\\n## Prerequisites\\n\\nTo follow along with the blog post, ensure the below are satisfied.\\n\\n1. AWS Service Account\\n1. AWS CLI installed\\n1. Terraform installed\\n1. kubectl installed\\n1. sveltosctl installed\\n\\n## Step 1: Create EKS Cluster with Terraform\\n\\nThe easiest way to spin up an EKS cluster is by following the recommended training and resources from the Hashicorp website. Find the training material and the Git repository further below.\\n\\n- Training: https://developer.hashicorp.com/terraform/tutorials/kubernetes/eks\\n\\n- GitHub Repository: https://github.com/hashicorp/learn-terraform-provision-eks-cluster\\n\\nTo execute the Terraform plan, a valid `AWS Service Account` should be available with the right permissions to create the required resources. For more information about the AWS Service Accounts, have a look [here](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html).\\n\\nTo get the cluster `kubeconfig` and start interacting with the cluster, the **AWS CLI** is used. Modify and execute the command below.\\n\\n```bash\\n$ aws eks update-kubeconfig --region <the region the cluster created> --name <the name of the cluster>\\n```\\n\\n:::tip\\nThe command will save the kubeconfig in the default directory `~/.kube/config`. If the file should be stored elsewhere, pass the argument `--kubeconfig` and specify the output directory. For more details, check out the [link](https://docs.aws.amazon.com/cli/latest/reference/eks/update-kubeconfig.html).\\n:::\\n\\n## Step 2: Register Cluster with\xa0Sveltos\\n\\nOnce we have access to the cluster, it is time to proceed with the Sveltos cluster registration. As this is a cloud Kubernetes cluster, we need to ensure Sveltos has the **right set of permissions** to perform the Kubernetes deployments and add-ons. To do that, we will utilise `sveltosctl` and generate a new kubeconfig file.\\n\\nDownload the `sveltosctl` binary [here](https://github.com/projectsveltos/sveltosctl/releases).\\n\\n### Generate Sveltos kubeconfig\\n\\n:::tip\\nEnsure you are authenticated with the AWS service account before performing the command below.\\n:::\\n\\n```bash\\n$ export KUBECONFIG=<directory of the EKS kubeconfig file>\\n\\n$ sveltosctl generate kubeconfig --create --expirationSeconds=86400 > eks_kubeconfig.yaml\\n```\\n\\nThe `sveltosctl` redirects the output of the command into a file named `eks_kubeconfig.yaml`. The file will be used by Sveltos for the managed cluster registration.\\n\\n### Register EKS\xa0Cluster - Management Cluster\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster>\\n\\n$ sveltosctl register cluster --namespace=<namespace> --cluster=<cluster name> \\\\\\n    --kubeconfig=<path to Sveltos file with Kubeconfig> \\\\\\n    --labels=env=test\\n```\\n\\nThe command above will register the EKS cluster with Sveltos on the mentioned **namespace**, and **name** and will attach the cluster **label** `env=test` defined.\\n\\n:::note\\nIf the namespace does not exist in the management cluster, the command will fail with the namespace not found error. Ensure the defined namespace exists in the cluster before registration.\\n:::\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster> \\n\\n$ kubectl get sveltosclusters -A --show-labels\\nNAMESPACE        NAME         READY   VERSION                LABELS\\nmgmt             mgmt         true    v1.28.9+rke2r1         sveltos-agent=present\\ntest             eks-test01   true    v1.28.10-eks-49c6de4   env=test,sveltos-agent=present\\n```\\n\\n## Step 3: Update the EKS\xa0cluster\\n\\nAs we would like to use Cilium with the Kube Proxy replacement and the [ENI](https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/networking-networkmode-awsvpc.html) mode enabled, we need to perform additional actions. As the `kube-proxy` daemonset is already installed, we have to remove all related resources and update the `aws-node` daemonset to support the ENI mode.\\n\\n### Validation\\n\\n```bash\\n$ kubectl get pods,ds -n kube-system\\nNAME                                      READY   STATUS    RESTARTS   AGE\\npod/aws-node-4x8sq                        2/2     Running   0          16m\\npod/aws-node-vjtlx                        2/2     Running   0          16m\\npod/aws-node-xp7vl                        2/2     Running   0          16m\\npod/coredns-648485486-t5sxm               1/1     Running   0          20m\\npod/coredns-648485486-tv4h5               1/1     Running   0          20m\\npod/ebs-csi-controller-5df9db689f-8hmdm   6/6     Running   0          15m\\npod/ebs-csi-controller-5df9db689f-qmxhs   6/6     Running   0          15m\\npod/ebs-csi-node-2rspx                    3/3     Running   0          15m\\npod/ebs-csi-node-gvtfj                    3/3     Running   0          15m\\npod/ebs-csi-node-t96ch                    3/3     Running   0          15m\\npod/kube-proxy-4jxlt                      1/1     Running   0          16m\\npod/kube-proxy-hgx9h                      1/1     Running   0          16m\\npod/kube-proxy-l877x                      1/1     Running   0          16m\\n\\nNAME                                  DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR              AGE\\ndaemonset.apps/aws-node               3         3         3       3            3           <none>                     20m\\ndaemonset.apps/ebs-csi-node           3         3         3       3            3           kubernetes.io/os=linux     16m\\ndaemonset.apps/ebs-csi-node-windows   0         0         0       0            0           kubernetes.io/os=windows   16m\\ndaemonset.apps/kube-proxy             3         3         3       3            3           <none>                     20m\\n```\\n\\n### Delete kube-proxy Resources\\n\\n```bash\\n$ export KUBECONFIG=<directory of the EKS kubeconfig file>\\n\\n$ kubectl delete ds kube-proxy -n kube-system # Remove the kube-proxy daemonset\\n\\n$ kubectl delete cm kube-proxy -n kube-system # Remove the kube-proxy ConfigMap\\n```\\n\\n### Update aws-node Resources\\n\\n```bash\\n$ kubectl patch daemonset aws-node --type=\'strategic\' -p=\'{\\"spec\\":{\\"template\\":{\\"spec\\":{\\"nodeSelector\\":{\\"io.cilium/aws-node-enabled\\":\\"true\\"}}}}}\' -n kube-system # This is required based on the Cilium documentation to enable the ENI mode\\n```\\n\\n```bash\\n$ kubectl get pods,ds -n kube-system\\nNAME                                      READY   STATUS    RESTARTS   AGE\\npod/coredns-648485486-t5sxm               1/1     Running   0          22m\\npod/coredns-648485486-tv4h5               1/1     Running   0          22m\\npod/ebs-csi-controller-5df9db689f-8hmdm   6/6     Running   0          17m\\npod/ebs-csi-controller-5df9db689f-qmxhs   6/6     Running   0          17m\\npod/ebs-csi-node-2rspx                    3/3     Running   0          17m\\npod/ebs-csi-node-gvtfj                    3/3     Running   0          17m\\npod/ebs-csi-node-t96ch                    3/3     Running   0          17m\\n\\nNAME                                  DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE\\ndaemonset.apps/aws-node               0         0         0       0            0           io.cilium/aws-node-enabled=true   22m\\ndaemonset.apps/ebs-csi-node           3         3         3       3            3           kubernetes.io/os=linux            17m\\ndaemonset.apps/ebs-csi-node-windows   0         0         0       0            0           kubernetes.io/os=windows          17m\\n```\\n\\n:::tip\\nThe aws-node daemonset scaled down to 0 replicas.\\n:::\\n\\n### Step 4: Create Sveltos ClusterProfile\\n\\nIt is time to create a **Sveltos ClusterProfile** and deploy **Cilium** to the EKS cluster with the label set to `env=test`. Following the Cilium [documentation](https://docs.cilium.io/en/v1.14/installation/k8s-install-helm/), we will enable the required Helm values for the `kube-proxy `replacement and the ENI mode.\\n\\n```yaml\\n---\\napiVersion: config.projectsveltos.io/v1alpha1\\nkind: ClusterProfile\\nmetadata:\\n  name: cilium-1148\\nspec:\\n  clusterSelector: env=test # Deploy Cilium v1.14.8 to any cluster with the cluster label set to env=test\\n  helmCharts:\\n  - chartName: cilium/cilium\\n    chartVersion: 1.14.8\\n    helmChartAction: Install\\n    releaseName: cilium\\n    releaseNamespace: kube-system\\n    repositoryName: cilium\\n    repositoryURL: https://helm.cilium.io/\\n    values: |\\n      eni:\\n        enabled: true\\n      ipam:\\n        mode: eni\\n      egressMasqueradeInterfaces: eth0\\n      routingMode: native\\n      kubeProxyReplacement: true\\n      k8sServiceHost: <The Server API FQDN or IP Address> # The information can be exctracted from the kubeconfig file or the AWS UI\\n      k8sServicePort: <The Server API listening port> # The information can be extracted from the kubeconfig file or the AWS UI\\n      nodePort:\\n        enabled: true\\n      debug:\\n        enabled: true\\n```\\n\\nThe ClusterProfile will deploy Cilium CNI to any cluster with the cluster label set to `env=test`. It will then deploy the Cilium Helm chart in the `kube-system` namespace alongside the kube-proxy replacement and the ENI mode. Hubble is also enabled.\\n\\n## Step 5: Deploy Cilium and\xa0Validate\\n\\nTo see and evaluate the results, the Sveltos ClusterProfile will be deployed to the management cluster.\\n\\n```bash\\n$ export KUBECONFIG=<Sveltos managament cluster>\\n\\n$ kubectl apply -f \\"clusterprofile_cilium1148.yaml\\"\\n```\\n\\n### Validation\\n\\n```bash\\n$ ./sveltosctl show addons\\n+-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+\\n|     CLUSTER     | RESOURCE TYPE |  NAMESPACE  |  NAME  | VERSION |             TIME              |          PROFILES          |\\n+-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+\\n| test/eks-test01 | helm chart    | kube-system | cilium | 1.14.8  | 2024-06-18 14:39:26 +0000 UTC | ClusterProfile/cilium-1148 |\\n+-----------------+---------------+-------------+--------+---------+-------------------------------+----------------------------+\\n```\\n\\n```bash\\n$ export KUBECONFIG=<directory of the EKS kubeconfig file>\\n\\n$ kubectl get pods -n kube-system | grep -i cilium\\ncilium-2vg4c                          1/1     Running             0          54s\\ncilium-operator-594f4858f6-km2wh      1/1     Running             0          54s\\ncilium-operator-594f4858f6-xx2q6      1/1     Running             0          55s\\ncilium-qrwwf                          1/1     Running             0          55s\\ncilium-s55v5                          1/1     Running             0          54s\\n```\\n\\n```bash\\n$ kubectl exec -it cilium-2vg4c -n kube-system -- cilium status\\nDefaulted container \\"cilium-agent\\" out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init)\\nKVStore:                 Ok   Disabled\\nKubernetes:              Ok   1.28+ (v1.28.10-eks-49c6de4) [linux/amd64]\\nKubernetes APIs:         [\\"EndpointSliceOrEndpoint\\", \\"cilium/v2::CiliumClusterwideNetworkPolicy\\", \\"cilium/v2::CiliumEndpoint\\", \\"cilium/v2::CiliumNetworkPolicy\\", \\"cilium/v2::CiliumNode\\", \\"cilium/v2alpha1::CiliumCIDRGroup\\", \\"core/v1::Namespace\\", \\"core/v1::Pods\\", \\"core/v1::Service\\", \\"networking.k8s.io/v1::NetworkPolicy\\"]\\nKubeProxyReplacement:    True   [eth0 10.0.1.150 (Direct Routing), eth1 10.0.1.37]\\n```\\n\\n### Deploy Nginx Application\\n\\n```bash\\n$ kubectl apply -f \\"nginx.yaml\\"\\n\\n$ kubectl get pods,svc\\nNAME                            READY   STATUS    RESTARTS   AGE\\npod/my-nginx-684dd4dcd4-gl9rm   1/1     Running   0          18s\\npod/my-nginx-684dd4dcd4-nk9mm   1/1     Running   0          18s\\n\\nNAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\\nservice/kubernetes   ClusterIP   172.20.0.1      <none>        443/TCP        33m\\nservice/my-nginx     NodePort    172.20.80.220   <none>        80:32449/TCP   3s\\n```\\n\\n### Cilium Validation\\n\\n```bash\\n$ kubectl -n kube-system exec ds/cilium -- cilium service list\\nDefaulted container \\"cilium-agent\\" out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init)\\nID   Frontend             Service Type   Backend                         \\n1    172.20.0.1:443       ClusterIP      1 => 10.0.1.15:443 (active)     \\n                                         2 => 10.0.2.226:443 (active)    \\n2    172.20.208.197:443   ClusterIP      1 => 10.0.1.150:4244 (active)   \\n3    172.20.22.66:80      ClusterIP      1 => 10.0.3.36:4245 (active)    \\n4    172.20.141.67:80     ClusterIP      1 => 10.0.2.229:8081 (active)   \\n5    172.20.0.10:53       ClusterIP      1 => 10.0.1.144:53 (active)     \\n                                         2 => 10.0.3.123:53 (active)     \\n6    172.20.80.220:80     ClusterIP      1 => 10.0.1.216:80 (active)     \\n                                         2 => 10.0.3.39:80 (active)      \\n7    10.0.1.150:32449     NodePort       1 => 10.0.1.216:80 (active)     \\n                                         2 => 10.0.3.39:80 (active)      \\n8    10.0.1.37:32449      NodePort       1 => 10.0.1.216:80 (active)     \\n                                         2 => 10.0.3.39:80 (active)      \\n9    0.0.0.0:32449        NodePort       1 => 10.0.1.216:80 (active)     \\n                                         2 => 10.0.3.39:80 (active)\\n```\\n\\nFrom the output above, we can observe that Cilium eBPF kube-proxy replacement created the NodePort service for Nginx.\\n\\nAs the blog post is not intended to outline in depth how the kube-proxy replacement works, check out the [link](https://docs.cilium.io/en/v1.14/network/kubernetes/kubeproxy-free/) for further tests.\\n\\n## Conclusions\\n\\nWe demonstrated an easy way of deploying Cilium CNI to an EKS cluster with the Sveltos ClusterProfile. The complete lifecycle of the CNI is now controlled by Sveltos and without external dependencies.\\n\\nTake advantage of the [Sveltos Templating](https://projectsveltos.github.io/sveltos/template/intro_template/) and the [Sveltos Event Framework](https://projectsveltos.github.io/sveltos/events/addon_event_deployment/) capabilities to make every Kubernetes deployment and add-on easier!\\n\\n## Contact\\n\\nWe are here to help! Whether you have questions, or issues or need assistance, our Slack channel is the perfect place for you. Click here to [join us](https://app.slack.com/client/T0471SNT5CZ/C06UZCXQLGP) us.\\n\\n## \ud83d\udc4f Support this\xa0project\\n\\nEvery contribution counts! If you enjoyed this article, check out the Projectsveltos [GitHub repo](https://github.com/projectsveltos). You can [star \ud83c\udf1f the project](https://github.com/projectsveltos) if you find it helpful.\\n\\nThe GitHub repo is a great resource for getting started with the project. It contains the code, documentation, and many more examples.\\n\\nThanks for reading!"},{"id":"welcome","metadata":{"permalink":"/personal-blog/blog/welcome","source":"@site/blog/2024-07-15-welcome/index.md","title":"Welcome","description":"\ud83c\udf1f Welcome!","date":"2024-07-15T00:00:00.000Z","tags":[{"inline":false,"label":"Hello","permalink":"/personal-blog/blog/tags/hello","description":"Hello tag description"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"Eleni Grosdouli","title":"DevOps Consulting Engineer at Cisco Systems","url":"https://github.com/egrosdou01","imageURL":"https://github.com/egrosdou01.png","key":"egrosdou01"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["egrosdou01"],"tags":["hello"]},"unlisted":false,"prevItem":{"title":"Cilium on EKS with Sveltos","permalink":"/personal-blog/blog/cilium-eks-sveltos"}},"content":"## \ud83c\udf1f Welcome!\\n\\nHello and welcome to my blog! \ud83d\ude80\\n\\nHere, you will discover everything you need to know about open source tools, DevOps, and GitOps practices.\\n\\nDive in and let\'s explore together! \ud83d\udca1\ud83d\udd27"}]}}')}}]);